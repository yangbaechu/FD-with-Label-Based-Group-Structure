{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T13:05:40.428552Z",
     "iopub.status.busy": "2023-08-04T13:05:40.428361Z",
     "iopub.status.idle": "2023-08-04T13:05:42.031676Z",
     "shell.execute_reply": "2023-08-04T13:05:42.030635Z",
     "shell.execute_reply.started": "2023-08-04T13:05:40.428529Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pytz\n",
    "import torch.nn as nn\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "\n",
    "from helper import ExperimentLogger, display_train_stats\n",
    "from fl_devices import Server, Client\n",
    "from data_utils import generate_server_idcs, CustomSubset, split_noniid, split_contain_2class, split_2class_plus_alpha, split_contain_3class_unbalanced\n",
    "from torchvision.models import mobilenet_v3_large\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial import distance\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T13:05:42.033144Z",
     "iopub.status.busy": "2023-08-04T13:05:42.032833Z",
     "iopub.status.idle": "2023-08-04T13:05:42.037407Z",
     "shell.execute_reply": "2023-08-04T13:05:42.036497Z",
     "shell.execute_reply.started": "2023-08-04T13:05:42.033122Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "LOCAL_EPOCHS = 25\n",
    "N_CLIENTS = 10\n",
    "NUMBER_OF_CLUSTER = 3\n",
    "ALPHA = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T13:05:42.039026Z",
     "iopub.status.busy": "2023-08-04T13:05:42.038800Z",
     "iopub.status.idle": "2023-08-04T13:05:42.918526Z",
     "shell.execute_reply": "2023-08-04T13:05:42.916257Z",
     "shell.execute_reply.started": "2023-08-04T13:05:42.039006Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = datasets.CIFAR10(root=\"CIFAR10/\", download=False)\n",
    "idcs = np.random.permutation(len(data))\n",
    "\n",
    "def visualize_clusters(label_predicted, clusters):\n",
    "    # Reduce the dimension of the data\n",
    "    pca = PCA(n_components=2)\n",
    "    label_predicted_pca = pca.fit_transform(label_predicted)\n",
    "\n",
    "    # Define colors for the clusters\n",
    "    colors = ['blue', 'red', 'green', 'purple', 'orange', 'yellow', 'black']\n",
    "\n",
    "    # Plot the clusters\n",
    "    plt.figure(figsize=(10,7))\n",
    "    \n",
    "    for i, cluster in enumerate(clusters):\n",
    "        plt.scatter(label_predicted_pca[cluster, 0], label_predicted_pca[cluster, 1], c=colors[i % len(colors)], label=f'Cluster {i+1}')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def test_acc(server, clients, client_accs, cluster_accs, global_accs, client_distribution=[0.5, 0.3, 0.2]):\n",
    "    \n",
    "    # Get individual client accuracies\n",
    "    acc_clients = [client.evaluate() for client in clients]\n",
    "\n",
    "    # Compute the average accuracy for each client\n",
    "    client_acc = round(sum(acc_clients) / len(acc_clients), 3) if len(acc_clients) > 0 else 0\n",
    "    client_accs.append(client_acc)\n",
    "\n",
    "    # Compute cluster accuracies for this iteration\n",
    "    cluster_accs_iteration = []\n",
    "    start_idx = 0\n",
    "    for distribution in client_distribution:\n",
    "        end_idx = start_idx + int(distribution * len(clients))\n",
    "        cluster_acc = round(sum(acc_clients[start_idx:end_idx]) / (end_idx - start_idx), 3)\n",
    "        cluster_accs_iteration.append(cluster_acc)\n",
    "        start_idx = end_idx\n",
    "    cluster_accs.append(cluster_accs_iteration)\n",
    "    \n",
    "    accuracies = [server.evaluate_distil(client.model) for client in clients]\n",
    "    global_acc = round(np.mean(accuracies), 3)\n",
    "    global_accs.append(global_acc)\n",
    "    \n",
    "    return client_accs, cluster_accs, global_accs\n",
    "\n",
    "def cluster(server, clients, number_of_cluster):\n",
    "    label_predicted = pd.DataFrame()\n",
    "    # label_acc = pd.DataFrame()\n",
    "    for i, client in enumerate(clients):\n",
    "        pred = server.check_cluster(client.model)\n",
    "        # print(f'pred: {pred}')\n",
    "        label_predicted = pd.concat([label_predicted, pd.DataFrame(pred, index=[i])])\n",
    "        # label_acc = pd.concat([label_acc, pd.DataFrame(acc, index=[i])])\n",
    "    label_predicted.reset_index(drop=True, inplace=True)\n",
    "    label_predicted.fillna(0, inplace=True)\n",
    "    \n",
    "    print(f'predicted label')\n",
    "    print(label_predicted)\n",
    "\n",
    "    cluster_idcs = server.cluster_clients_GMM(label_predicted, number_of_cluster)\n",
    "    return label_predicted, cluster_idcs\n",
    "\n",
    "\n",
    "def get_global_logits(client_logits):\n",
    "    avg_logits = torch.mean(torch.stack(client_logits), dim=0)\n",
    "    return avg_logits\n",
    "\n",
    "def get_cluster_logits(client_logits, cluster_idcs):\n",
    "    cluster_logits = []\n",
    "    for i, cluster in enumerate(cluster_idcs):\n",
    "        cluster_client_logits = [client_logits[i] for i in cluster]\n",
    "        avg_cluster_logits = torch.mean(torch.stack(cluster_client_logits), dim=0)\n",
    "        cluster_logits.append(avg_cluster_logits)\n",
    "    return cluster_logits\n",
    "\n",
    "\n",
    "def get_cluster_averages(df, cluster_idcs):\n",
    "    cluster_averages = {}\n",
    "\n",
    "    for i, cluster in enumerate(cluster_idcs):\n",
    "        cluster_data = df.loc[cluster, :]\n",
    "        cluster_average = cluster_data.mean()\n",
    "        cluster_averages[i] = cluster_average\n",
    "\n",
    "    cluster_averages_df = pd.DataFrame(cluster_averages).T\n",
    "\n",
    "    # calculate percentage for each value in a row\n",
    "    cluster_percentages_df = cluster_averages_df.div(cluster_averages_df.sum(axis=1), axis=0).multiply(100)\n",
    "\n",
    "    # apply threshold and rounding\n",
    "    cluster_percentages_df = cluster_percentages_df.where(cluster_percentages_df >= 5, 0).round()\n",
    "\n",
    "    return cluster_percentages_df / 10\n",
    "\n",
    "\n",
    "def get_cluster_weights(cluster_logits, cluster_weight_per_class):\n",
    "    # Step 1: Find the class with the maximum value in each logit in cluster_logits\n",
    "    # Assuming the dimension for classes is the second one\n",
    "    max_classes = [torch.argmax(logit, dim=1) for logit in cluster_logits]\n",
    "    \n",
    "    print(max_classes)\n",
    "    cluster_weights = []\n",
    "    #c luster별로 \n",
    "    for i, iogits in enumerate(cluster_logits):\n",
    "        weights = []\n",
    "        for j, logit in enumerate(logits):\n",
    "            \n",
    "            # print(f'max_classes: {max_classes[i][j]}')\n",
    "            # print(\"Shape of cluster_weight_per_class[{}]: \".format(i), cluster_weight_per_class[i].shape)\n",
    "            weights.append(max_classes[i][j])\n",
    "        print(weights)\n",
    "        cluster_weights.append(weights)\n",
    "data = datasets.CIFAR10(root=\"CIFAR10/\", download=False)\n",
    "idcs = np.random.permutation(len(data))\n",
    "\n",
    "def visualize_clusters(label_predicted, clusters, real_cluster_distribution):\n",
    "    # Reduce the dimension of the data\n",
    "    pca = PCA(n_components=2)\n",
    "    label_predicted_pca = pca.fit_transform(label_predicted)\n",
    "\n",
    "    # Define colors for the clusters\n",
    "    colors = ['blue', 'red', 'green', 'purple', 'orange', 'yellow', 'black']\n",
    "\n",
    "    # Plot the clusters\n",
    "    plt.figure(figsize=(10,7))\n",
    "    \n",
    "    for i, cluster in enumerate(clusters):\n",
    "        plt.scatter(label_predicted_pca[cluster, 0], label_predicted_pca[cluster, 1], c=colors[i % len(colors)], label=f'Cluster {i+1}')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Print the real cluster distribution\n",
    "    print(\"Real cluster distribution:\", real_cluster_distribution)\n",
    "\n",
    "    # Calculate and print each client's cluster identity based on the real_cluster_distribution\n",
    "    n_clients = len(label_predicted)\n",
    "    cumulative_distribution = [0] + [sum(real_cluster_distribution[:i+1]) for i in range(len(real_cluster_distribution))]\n",
    "    client_cluster_id_real = [next((i for i, val in enumerate(cumulative_distribution) if val > client_idx / n_clients), -1) - 1 for client_idx in range(n_clients)]\n",
    "\n",
    "    print(\"Real cluster identity for each client:\", client_cluster_id_real)\n",
    "\n",
    "    # Print each client's cluster identity based on the clusters argument\n",
    "    client_cluster_id_predicted = [next((i for i, cluster in enumerate(clusters) if client_idx in cluster), -1) for client_idx in range(n_clients)]\n",
    "    print(\"Predicted cluster identity for each client:\", client_cluster_id_predicted)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_acc(server, clients, client_accs, cluster_accs, global_accs, client_distribution=[0.5, 0.3, 0.2]):\n",
    "    \n",
    "    # Get individual client accuracies\n",
    "    acc_clients = [client.evaluate() for client in clients]\n",
    "\n",
    "    # Compute the average accuracy for each client\n",
    "    client_acc = round(sum(acc_clients) / len(acc_clients), 3) if len(acc_clients) > 0 else 0\n",
    "    client_accs.append(client_acc)\n",
    "\n",
    "    # Compute cluster accuracies for this iteration\n",
    "    cluster_accs_iteration = []\n",
    "    start_idx = 0\n",
    "    for distribution in client_distribution:\n",
    "        end_idx = start_idx + int(distribution * len(clients))\n",
    "        cluster_acc = round(sum(acc_clients[start_idx:end_idx]) / (end_idx - start_idx), 3)\n",
    "        cluster_accs_iteration.append(cluster_acc)\n",
    "        start_idx = end_idx\n",
    "    cluster_accs.append(cluster_accs_iteration)\n",
    "    \n",
    "    accuracies = [server.evaluate_distil(client.model) for client in clients]\n",
    "    global_acc = round(np.mean(accuracies), 3)\n",
    "    global_accs.append(global_acc)\n",
    "    \n",
    "    return client_accs, cluster_accs, global_accs\n",
    "\n",
    "# def cluster(server, clients, number_of_cluster):\n",
    "#     label_predicted = pd.DataFrame()\n",
    "#     # label_acc = pd.DataFrame()\n",
    "#     for i, client in enumerate(clients):\n",
    "#         pred = server.check_cluster(client.model)\n",
    "#         # print(f'pred: {pred}')\n",
    "#         label_predicted = pd.concat([label_predicted, pd.DataFrame(pred, index=[i])])\n",
    "#         # label_acc = pd.concat([label_acc, pd.DataFrame(acc, index=[i])])\n",
    "#     label_predicted.reset_index(drop=True, inplace=True)\n",
    "#     label_predicted.fillna(0, inplace=True)\n",
    "    \n",
    "#     if random.random() < 1/10:\n",
    "#         print(label_predicted[:8])\n",
    "\n",
    "#     cluster_idcs = server.cluster_clients_GMM(label_predicted, number_of_cluster)\n",
    "#     return label_predicted, cluster_idcs\n",
    "\n",
    "\n",
    "def get_global_logits(client_logits):\n",
    "    avg_logits = torch.mean(torch.stack(client_logits), dim=0)\n",
    "    return avg_logits\n",
    "\n",
    "def get_cluster_logits(client_logits, cluster_idcs):\n",
    "    cluster_logits = []\n",
    "    for i, cluster in enumerate(cluster_idcs):\n",
    "        cluster_client_logits = [client_logits[i] for i in cluster]\n",
    "        avg_cluster_logits = torch.mean(torch.stack(cluster_client_logits), dim=0)\n",
    "        cluster_logits.append(avg_cluster_logits)\n",
    "    return cluster_logits\n",
    "\n",
    "\n",
    "def get_cluster_averages(df, cluster_idcs):\n",
    "    cluster_averages = {}\n",
    "\n",
    "    for i, cluster in enumerate(cluster_idcs):\n",
    "        cluster_data = df.loc[cluster, :]\n",
    "        cluster_average = cluster_data.mean()\n",
    "        cluster_averages[i] = cluster_average\n",
    "\n",
    "    cluster_averages_df = pd.DataFrame(cluster_averages).T\n",
    "\n",
    "    # calculate percentage for each value in a row\n",
    "    cluster_percentages_df = cluster_averages_df.div(cluster_averages_df.sum(axis=1), axis=0).multiply(100)\n",
    "\n",
    "    # apply threshold and rounding\n",
    "    cluster_percentages_df = cluster_percentages_df.where(cluster_percentages_df >= 5, 0).round()\n",
    "\n",
    "    return cluster_percentages_df / 10\n",
    "\n",
    "\n",
    "def get_cluster_weights(cluster_logits, cluster_weight_per_class):\n",
    "    # Step 1: Find the class with the maximum value in each logit in cluster_logits\n",
    "    # Assuming the dimension for classes is the second one\n",
    "    max_classes = [torch.argmax(logit, dim=1) for logit in cluster_logits]\n",
    "    \n",
    "    print(max_classes)\n",
    "    cluster_weights = []\n",
    "    #c luster별로 \n",
    "    for i, iogits in enumerate(cluster_logits):\n",
    "        weights = []\n",
    "        for j, logit in enumerate(logits):\n",
    "            \n",
    "            # print(f'max_classes: {max_classes[i][j]}')\n",
    "            # print(\"Shape of cluster_weight_per_class[{}]: \".format(i), cluster_weight_per_class[i].shape)\n",
    "            weights.append(max_classes[i][j])\n",
    "        print(weights)\n",
    "        cluster_weights.append(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T13:05:42.921408Z",
     "iopub.status.busy": "2023-08-04T13:05:42.921130Z",
     "iopub.status.idle": "2023-08-04T13:05:42.931886Z",
     "shell.execute_reply": "2023-08-04T13:05:42.930722Z",
     "shell.execute_reply.started": "2023-08-04T13:05:42.921378Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def global_distill_experiments(total_client_data, distill_data, alpha, number_of_cluster, cluster_distribution):\n",
    "    print(f'number_of_cluster: {number_of_cluster}')\n",
    "    data_per_class=int(distill_data//10)\n",
    "    train_idcs, test_idcs = idcs[:total_client_data], idcs[total_client_data:(total_client_data + int(distill_data * 2))]\n",
    "    train_labels = data.targets\n",
    "    test_labels = data.targets\n",
    "    \n",
    "    server_idcs = generate_server_idcs(test_idcs, test_labels, int(distill_data//10))\n",
    "\n",
    "    # client_idcs = split_noniid(train_idcs, train_labels, alpha=alpha, n_clients=N_CLIENTS)\n",
    "    client_idcs = split_contain_3class_unbalanced(train_idcs, train_labels, N_CLIENTS, cluster_distribution)\n",
    "    \n",
    "    print('client idcs generated!')\n",
    "    client_data = [CustomSubset(data, idcs) for idcs in client_idcs]\n",
    "    test_data = CustomSubset(data, server_idcs, transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "    for i, client_datum in enumerate(client_data):\n",
    "        client_datum.subset_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    server = Server(mobilenet_v3_large, lambda x : torch.optim.Adam(x, weight_decay=0.1),test_data)\n",
    "\n",
    "    clients = [Client(mobilenet_v3_large, lambda x: torch.optim.Adam(x, weight_decay=0.1), dat, i) \n",
    "           for i, dat in enumerate(client_data) if len(dat) > 20]\n",
    "    \n",
    "    print(f'client count: {len(clients)}')\n",
    "\n",
    "    client_accs = []\n",
    "    cluster_accs = []\n",
    "    global_accs = []\n",
    "    client_logits = []\n",
    "    \n",
    "    # 1.Local training\n",
    "    for epoch in range(LOCAL_EPOCHS):\n",
    "        for i, client in enumerate(clients):\n",
    "            client.compute_weight_update(epochs=1)\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            client_accs, cluster_accs, global_accs = test_acc(server, clients, client_accs, cluster_accs, global_accs, cluster_distribution)\n",
    "            print(f'client_acc: {client_accs[-1]}, cluster_acc: {cluster_distribution}: {cluster_accs[-1]},  global_acc: {global_accs[-1]}')\n",
    "      \n",
    "    # 2. get global loigt\n",
    "    for i, client in enumerate(clients):\n",
    "        if i == 0:\n",
    "            distill_data = server.get_clients_logit(client.model,data_per_class=data_per_class)\n",
    "            client_logits.append(distill_data[2])\n",
    "        else:\n",
    "            client_logits.append(server.get_clients_logit(client.model,data_per_class=data_per_class)[2])\n",
    "    global_logits = get_global_logits(client_logits)\n",
    "    \n",
    "    # 3.Distillation\n",
    "    for i, client in enumerate(clients):\n",
    "        if i % 10 == 0:\n",
    "            print(f'client {i} distill')\n",
    "        \n",
    "        client.distill((distill_data[0], global_logits))\n",
    "    \n",
    "    client_accs, cluster_accs, global_accs = test_acc(server, clients, client_accs, cluster_accs, global_accs, cluster_distribution)\n",
    "    \n",
    "    print(f'total_client_data: {total_client_data}, data_per_class: {data_per_class}, cluster_distribution: {cluster_distribution}')\n",
    "    print(f'first acc: {client_accs[0]}, {cluster_accs[0]}, {global_accs[0]}')\n",
    "    print(f'acc before distill: {client_accs[-2]}, {cluster_accs[-2]}, {global_accs[-2]}')\n",
    "    print(f'last acc: {client_accs[-1]}, {cluster_accs[-1]}, {global_accs[-1]}')\n",
    "    return client_accs, cluster_accs, global_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T13:05:42.934975Z",
     "iopub.status.busy": "2023-08-04T13:05:42.934353Z",
     "iopub.status.idle": "2023-08-04T13:05:42.943919Z",
     "shell.execute_reply": "2023-08-04T13:05:42.942892Z",
     "shell.execute_reply.started": "2023-08-04T13:05:42.934956Z"
    }
   },
   "outputs": [],
   "source": [
    "def cluster_distill_experiments(total_client_data, distill_data, alpha, number_of_cluster, cluster_distribution):\n",
    "    print(f'number_of_cluster: {number_of_cluster}')\n",
    "    data_per_class=int(distill_data//10)\n",
    "    train_idcs, test_idcs = idcs[:total_client_data], idcs[total_client_data:(total_client_data + int(distill_data * 2))]\n",
    "    train_labels = data.targets\n",
    "    test_labels = data.targets\n",
    "    \n",
    "    server_idcs = generate_server_idcs(test_idcs, test_labels, int(distill_data//10))\n",
    "\n",
    "    #client_idcs = split_noniid(train_idcs, train_labels, alpha=alpha, n_clients=N_CLIENTS)\n",
    "    client_idcs = split_contain_3class_unbalanced(train_idcs, train_labels, N_CLIENTS, cluster_distribution)\n",
    "    \n",
    "    print('client idcs generated!')\n",
    "    client_data = [CustomSubset(data, idcs) for idcs in client_idcs]\n",
    "    print(f'server data length: {len(server_idcs)}')\n",
    "    test_data = CustomSubset(data, server_idcs, transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "    for i, client_datum in enumerate(client_data):\n",
    "        client_datum.subset_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    server = Server(mobilenet_v3_large, lambda x : torch.optim.Adam(x, weight_decay=0.1),test_data)\n",
    "\n",
    "    clients = [Client(mobilenet_v3_large, lambda x: torch.optim.Adam(x, weight_decay=0.1), dat, i) \n",
    "           for i, dat in enumerate(client_data) if len(dat) > 20]\n",
    "    \n",
    "    print(f'client count: {len(clients)}')\n",
    "\n",
    "    client_accs = []\n",
    "    cluster_accs = []\n",
    "    global_accs = []\n",
    "    client_logits = []\n",
    "    \n",
    "    # 1.Local training\n",
    "    for epoch in range(1, LOCAL_EPOCHS + 1):\n",
    "        for i, client in enumerate(clients):\n",
    "            client.compute_weight_update(epochs=1)\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            client_accs, cluster_accs, global_accs = test_acc(server, clients, client_accs, cluster_accs, global_accs, cluster_distribution)\n",
    "            print(f'client_acc: {client_accs[-1]}, cluster_acc: {cluster_distribution}: {cluster_accs[-1]},  global_acc: {global_accs[-1]}')\n",
    "\n",
    "    # 2.Clustering\n",
    "    label_predicted, cluster_idcs = cluster(server, clients, number_of_cluster)\n",
    "    cluster_weight_per_class = get_cluster_averages(label_predicted.sort_index(axis=1), cluster_idcs)\n",
    "    # print(cluster_weight_per_class)\n",
    "    visualize_clusters(label_predicted, cluster_idcs, cluster_distribution)\n",
    "        \n",
    "    # 3.Get cluster, global loigt\n",
    "    for i, client in enumerate(clients):\n",
    "        if i == 0:\n",
    "            distill_data = server.get_clients_logit(client.model,data_per_class=data_per_class)\n",
    "            client_logits.append(distill_data[2])\n",
    "        else:\n",
    "            client_logits.append(server.get_clients_logit(client.model,data_per_class=data_per_class)[2])\n",
    "    global_logits = get_global_logits(client_logits)\n",
    "    \n",
    "    cluster_logits = get_cluster_logits(client_logits, cluster_idcs)\n",
    "    # cluster_weights = get_cluster_weights(cluster_logits, cluster_weight_per_class)\n",
    "    \n",
    "    # 4.Distillation\n",
    "    for i, client in enumerate(clients):\n",
    "        if i % 10 == 0:\n",
    "            print(f'client {i} distill')\n",
    "        \n",
    "             # Find the corresponding cluster for the client\n",
    "        cluster_idx = next(j for j, cluster in enumerate(cluster_idcs) if i in cluster)\n",
    "\n",
    "        # Extract the corresponding cluster logits\n",
    "        my_cluster_logit = cluster_logits[cluster_idx]\n",
    "        \n",
    "        client.distill((distill_data[0], my_cluster_logit))\n",
    "    \n",
    "    client_accs, cluster_accs, global_accs = test_acc(server, clients, client_accs, cluster_accs, global_accs, cluster_distribution)\n",
    "    \n",
    "    print(f'total_client_data: {total_client_data}, data_per_class: {data_per_class}, cluster_distribution: {cluster_distribution}')\n",
    "    print(f'first acc: {client_accs[0]}, {cluster_accs[0]}, {global_accs[0]}')\n",
    "    print(f'acc before distill: {client_accs[-2]}, {cluster_accs[-2]}, {global_accs[-2]}')\n",
    "    print(f'last acc: {client_accs[-1]}, {cluster_accs[-1]}, {global_accs[-1]}')\n",
    "    return client_accs, cluster_accs, global_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T13:05:42.945412Z",
     "iopub.status.busy": "2023-08-04T13:05:42.944977Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_cluster: 3\n",
      "client idcs generated!\n",
      "server data length: 4000\n",
      "Train Label Distribution for client 0: Counter({1: 560, 2: 557, 0: 553})\n",
      "Evaluation Label Distribution for client 0: Counter({1: 241, 2: 239, 0: 237})\n",
      "Train Label Distribution for client 1: Counter({1: 560, 2: 557, 0: 553})\n",
      "Evaluation Label Distribution for client 1: Counter({1: 240, 2: 239, 0: 237})\n",
      "Train Label Distribution for client 2: Counter({1: 560, 2: 557, 0: 553})\n",
      "Evaluation Label Distribution for client 2: Counter({1: 240, 2: 239, 0: 237})\n",
      "Train Label Distribution for client 3: Counter({1: 560, 2: 557, 0: 552})\n",
      "Evaluation Label Distribution for client 3: Counter({1: 240, 2: 239, 0: 237})\n",
      "Train Label Distribution for client 4: Counter({1: 560, 2: 557, 0: 552})\n",
      "Evaluation Label Distribution for client 4: Counter({1: 240, 2: 239, 0: 237})\n",
      "Train Label Distribution for client 5: Counter({4: 700, 3: 700, 5: 700})\n",
      "Evaluation Label Distribution for client 5: Counter({4: 300, 5: 300, 3: 300})\n",
      "Train Label Distribution for client 6: Counter({3: 700, 5: 700, 4: 700})\n",
      "Evaluation Label Distribution for client 6: Counter({3: 300, 4: 300, 5: 300})\n",
      "Train Label Distribution for client 7: Counter({5: 700, 3: 700, 4: 700})\n",
      "Evaluation Label Distribution for client 7: Counter({4: 300, 3: 300, 5: 300})\n",
      "Train Label Distribution for client 8: Counter({8: 700, 7: 700, 6: 700})\n",
      "Evaluation Label Distribution for client 8: Counter({6: 300, 8: 300, 7: 300})\n",
      "Train Label Distribution for client 9: Counter({7: 700, 8: 700, 6: 700})\n",
      "Evaluation Label Distribution for client 9: Counter({7: 300, 6: 300, 8: 300})\n",
      "client count: 10\n",
      "output in train\n",
      "tensor([[ 4.1602,  2.9064,  3.8067, -5.1513, -4.0327, -4.8517, -3.3986, -4.8112,\n",
      "         -4.0850, -4.6027],\n",
      "        [ 1.5921,  1.9567,  2.3707, -4.1125, -2.4613, -2.5219, -2.8607, -2.8004,\n",
      "         -3.0512, -3.0597],\n",
      "        [ 1.6954,  1.9455,  1.7161, -3.0068, -3.9586, -3.0075, -2.7811, -3.0299,\n",
      "         -1.8705, -2.2457],\n",
      "        [ 1.8817,  1.7810,  3.0020, -2.8277, -2.6123, -1.6536, -1.9139, -2.3679,\n",
      "         -1.6361, -1.5354],\n",
      "        [ 2.0438,  2.5647,  3.4286, -3.8246, -4.1748, -3.0345, -4.1643, -3.5845,\n",
      "         -2.4833, -3.3724],\n",
      "        [ 1.4079,  1.3968,  1.4532, -2.0163, -1.1839, -1.8083, -1.1137, -1.6096,\n",
      "         -0.7376, -1.5412],\n",
      "        [ 4.1150,  4.9626,  3.6572, -5.7406, -3.0435, -4.0784, -4.2905, -5.1643,\n",
      "         -2.8885, -3.8084],\n",
      "        [ 1.8009,  3.8569,  2.1629, -4.1998, -3.9180, -3.0836, -2.4988, -2.2197,\n",
      "         -3.0188, -2.2293],\n",
      "        [ 1.6290,  1.4627,  2.0824, -2.4657, -1.6094, -2.4881, -1.7606, -2.5611,\n",
      "         -2.2663, -2.4377],\n",
      "        [ 2.9002,  3.2496,  3.7345, -3.7008, -4.3005, -2.9171, -4.1766, -3.4734,\n",
      "         -3.1678, -3.0176]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "output in train\n",
      "tensor([[ 1.1759,  1.4121,  0.8203, -1.1819, -1.0907, -1.2129, -1.3726, -0.6227,\n",
      "         -0.9849, -1.2972],\n",
      "        [ 1.1614,  2.2258,  1.9570, -2.5038, -1.5366, -2.8254, -2.0644, -1.8599,\n",
      "         -1.3646, -2.0496],\n",
      "        [ 1.0980,  1.8377,  1.2150, -1.5863, -1.0459, -1.6854, -0.8696, -1.4863,\n",
      "         -1.3613, -1.1880],\n",
      "        [ 0.9950,  0.6718,  1.0880, -1.3189, -0.9732, -1.5815, -0.9264, -1.4505,\n",
      "         -1.7495, -0.9923],\n",
      "        [ 3.3845,  3.0171,  3.1343, -4.2845, -2.6179, -4.9875, -3.3997, -5.1999,\n",
      "         -4.0943, -3.6105],\n",
      "        [ 6.1978, 10.3459,  6.9205, -9.5049, -8.0082, -8.4976, -9.1520, -7.4093,\n",
      "         -8.9964, -7.6461],\n",
      "        [ 2.0405,  2.0551,  2.4771, -2.6915, -3.0329, -4.3642, -2.6901, -3.8802,\n",
      "         -3.2908, -2.7881],\n",
      "        [ 2.3021,  2.0189,  2.5410, -2.0677, -1.7271, -2.3187, -2.6869, -2.3544,\n",
      "         -2.3635, -2.3348],\n",
      "        [ 4.5942,  2.9795,  3.1725, -5.5494, -6.1048, -5.0498, -3.6654, -4.5405,\n",
      "         -5.9949, -4.9971],\n",
      "        [ 3.2832,  3.1257,  2.7158, -1.8082, -3.7465, -3.6367, -2.6895, -2.8966,\n",
      "         -1.8593, -2.8481]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "output in train\n",
      "tensor([[-3.0956, -2.2991, -2.7727,  1.2063,  2.9168,  1.5169, -1.9239, -2.1429,\n",
      "         -2.6750, -2.8176],\n",
      "        [-5.6494, -5.1679, -4.5803,  4.3846,  2.8630,  6.3197, -4.7746, -4.9504,\n",
      "         -6.7362, -7.4366],\n",
      "        [-3.1460, -3.3097, -2.9741,  1.8459,  3.4364,  1.5660, -2.5192, -2.8740,\n",
      "         -2.2541, -3.1090],\n",
      "        [-3.1751, -2.2574, -2.3874,  1.5675,  2.1237,  2.2143, -2.8195, -2.7084,\n",
      "         -2.4064, -3.2630],\n",
      "        [-1.4019, -1.9059, -1.6677,  1.1065,  1.6992,  1.2651, -2.1323, -1.9217,\n",
      "         -1.9338, -2.2938],\n",
      "        [-2.3978, -3.0097, -2.2367,  2.1918,  2.3960,  2.0429, -2.8818, -2.8418,\n",
      "         -2.2590, -2.7616],\n",
      "        [-2.7691, -1.9306, -2.8509,  2.1069,  2.6141,  1.6318, -2.6734, -2.1771,\n",
      "         -3.3664, -3.2643],\n",
      "        [-5.0704, -4.7704, -4.8111,  2.9704,  4.1238,  1.2950, -3.5986, -4.8130,\n",
      "         -3.7043, -4.2698],\n",
      "        [-3.3504, -3.7726, -3.1176,  3.1077,  2.7966,  2.4421, -3.4686, -3.5742,\n",
      "         -3.7404, -3.0640],\n",
      "        [-5.0814, -4.7470, -4.8202,  3.2708,  3.1522,  2.5221, -3.2688, -3.8004,\n",
      "         -4.8965, -4.7403]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "output in train\n",
      "tensor([[-1.9692, -2.6477, -1.9947, -1.9716, -1.5809, -1.4854,  1.7696,  1.9487,\n",
      "          1.7691, -2.0828],\n",
      "        [-2.9604, -1.9449, -3.4544, -2.9036, -2.0111, -2.3141,  2.0474,  1.7878,\n",
      "          2.8913, -2.6915],\n",
      "        [-2.7514, -3.2123, -2.3033, -2.2894, -1.7154, -0.9490,  1.8817,  2.3374,\n",
      "          2.0559, -2.0542],\n",
      "        [-3.9227, -3.4621, -3.6719, -3.2540, -2.9740, -2.0771,  2.4170,  3.0218,\n",
      "          4.6063, -4.6119],\n",
      "        [-3.1855, -3.7743, -3.6205, -2.5897, -2.7754, -2.4536,  3.8380,  2.4898,\n",
      "          1.3820, -3.1811],\n",
      "        [-5.4328, -4.6931, -4.1328, -3.3899, -3.3558, -3.3615,  2.7295,  3.1433,\n",
      "          3.0386, -3.7751],\n",
      "        [-1.3006, -0.8560, -1.6072, -1.9664, -2.2400, -0.8206,  1.4585,  1.1574,\n",
      "          1.2197, -1.0698],\n",
      "        [-1.2630, -2.8945, -1.8354, -2.1999, -2.4227, -1.6641,  1.9340,  1.2451,\n",
      "          2.5184, -2.4608],\n",
      "        [-8.7232, -7.0870, -6.5087, -7.1513, -5.5574, -7.6388,  4.9138, 10.0795,\n",
      "          7.0375, -9.3276],\n",
      "        [-3.3398, -2.2390, -3.5870, -3.2320, -1.5843, -2.0679,  2.4410,  2.3454,\n",
      "          1.4669, -2.3996]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "output in train\n",
      "tensor([[ 0.6553,  3.4304,  2.1822, -3.7558, -3.1016, -2.7402, -3.7690, -3.0753,\n",
      "         -3.0934, -2.8064],\n",
      "        [-0.0220,  0.2550,  0.6084, -0.5851, -0.7243, -0.6521, -0.7267, -0.8802,\n",
      "         -0.5827, -0.2625],\n",
      "        [ 1.9359,  1.2390,  1.2125, -2.6741, -2.7580, -2.6087, -2.0084, -2.4972,\n",
      "         -3.3457, -3.0023],\n",
      "        [ 0.6134,  0.7510,  2.0004, -1.5036, -1.3397, -2.4274, -2.5217, -1.5534,\n",
      "         -1.0728, -1.2788],\n",
      "        [ 1.5583,  1.7643,  1.5558, -1.6028, -0.9254, -2.1850, -1.8561, -2.5055,\n",
      "         -2.5529, -1.5083],\n",
      "        [ 0.6696,  1.0663,  0.4828, -1.7373, -0.9980, -1.2747, -1.6208, -1.6096,\n",
      "         -1.0939, -0.8405]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "output in train\n",
      "tensor([[-4.7174, -4.2495, -5.4221,  1.9576,  4.8722,  2.7797, -3.0813, -4.6296,\n",
      "         -3.2577, -4.5222],\n",
      "        [-5.0116, -6.0658, -5.5207,  3.2637,  5.2506,  2.4856, -4.5073, -4.8865,\n",
      "         -6.1946, -5.4721],\n",
      "        [-3.9988, -3.8799, -4.1863,  2.6074,  1.3505,  2.5049, -3.6975, -3.2931,\n",
      "         -3.6172, -4.0976],\n",
      "        [-2.5450, -2.6014, -2.3714,  2.1623,  0.7100,  1.5373, -2.9287, -2.8309,\n",
      "         -3.6409, -1.9613],\n",
      "        [-2.4537, -2.4946, -2.5325,  2.7610,  0.1243,  2.4923, -2.8052, -4.0159,\n",
      "         -2.6941, -4.2498],\n",
      "        [-5.9584, -7.7132, -5.2807,  4.3512,  1.5193,  5.4866, -6.1511, -6.1793,\n",
      "         -6.4284, -5.3845],\n",
      "        [-2.9638, -2.6725, -3.1958,  1.1637,  1.2173,  2.0833, -2.5395, -2.5292,\n",
      "         -2.9188, -3.1399],\n",
      "        [-6.0384, -5.4189, -5.3311,  4.2070,  2.5446,  5.7405, -6.4734, -6.8659,\n",
      "         -5.8595, -7.7874],\n",
      "        [-6.6376, -7.2580, -6.1573,  6.9261,  2.5766,  6.2118, -5.2891, -7.9447,\n",
      "         -6.0401, -7.8212],\n",
      "        [-2.0830, -1.8925, -2.1844,  1.5935,  1.5295,  1.9226, -1.5307, -2.1558,\n",
      "         -2.3596, -2.1170]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "output in train\n",
      "tensor([[ 3.0602,  1.4496,  2.0855, -3.4106, -3.1277, -3.5038, -2.9168, -2.8377,\n",
      "         -3.5727, -3.1314],\n",
      "        [ 0.6735,  3.0120,  0.8058, -3.1630, -2.8001, -2.3748, -2.2197, -2.1614,\n",
      "         -2.2142, -2.2703],\n",
      "        [ 0.5071,  3.1558,  4.5113, -3.7495, -3.4789, -3.8908, -2.2205, -3.2455,\n",
      "         -3.0943, -3.0573],\n",
      "        [ 1.2229,  0.3646,  1.9377, -1.8761, -2.0350, -2.3922, -2.4429, -1.9331,\n",
      "         -2.3128, -2.0886],\n",
      "        [ 0.9412,  1.6677,  1.5168, -2.3478, -2.0220, -1.6366, -1.5394, -1.9239,\n",
      "         -2.1464, -1.7470],\n",
      "        [ 0.3597,  1.7882,  1.9009, -2.0698, -1.8698, -2.0966, -1.8174, -2.0190,\n",
      "         -2.0118, -1.7017],\n",
      "        [ 2.9519,  0.9348,  0.0921, -2.0566, -1.6645, -1.4483, -1.3482, -1.8495,\n",
      "         -1.6639, -1.2834],\n",
      "        [ 4.4418,  0.9127,  3.1763, -3.8893, -4.0742, -3.8721, -4.2797, -4.6221,\n",
      "         -4.4391, -3.5955],\n",
      "        [ 1.0419,  0.8004,  4.6214, -3.9270, -3.6714, -3.6162, -3.2422, -3.1784,\n",
      "         -3.9975, -3.5698],\n",
      "        [ 1.8243,  1.9132,  4.2237, -3.9406, -4.0233, -3.2205, -3.7758, -3.1323,\n",
      "         -3.5556, -3.5390]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "output in train\n",
      "tensor([[-1.5952, -1.6532, -1.4278,  1.9571,  0.1999,  2.6060, -1.2433, -1.2292,\n",
      "         -1.6250, -1.4917],\n",
      "        [-2.5324, -2.5433, -2.0120,  2.1627, -0.9638,  2.0720, -2.1107, -2.4327,\n",
      "         -2.3793, -2.5719],\n",
      "        [-3.2666, -2.9408, -3.3177,  2.8274,  0.9091,  3.4346, -2.8338, -3.0778,\n",
      "         -3.1276, -3.1911],\n",
      "        [-1.9850, -1.9669, -2.1949,  1.1007,  4.0272,  0.5021, -2.1381, -2.3993,\n",
      "         -1.8535, -2.2031],\n",
      "        [-1.2070, -1.3945, -1.4157,  0.4757,  3.5886,  0.2422, -1.5584, -1.3710,\n",
      "         -1.6629, -1.2496],\n",
      "        [-1.2488, -1.5353, -1.4255,  0.6691,  2.0795,  0.7022, -1.4273, -1.8555,\n",
      "         -1.4961, -1.3711],\n",
      "        [-2.5327, -2.4920, -2.4653,  2.4368, -0.4329,  3.8265, -2.4383, -2.3347,\n",
      "         -2.6750, -2.5052],\n",
      "        [-2.9960, -3.0434, -2.9560,  0.3576,  5.2270,  0.3039, -3.3526, -3.1409,\n",
      "         -3.2185, -3.0017],\n",
      "        [-1.5396, -1.4668, -1.5056,  2.2256, -0.0982,  1.8816, -1.6183, -1.0827,\n",
      "         -1.6653, -1.5445],\n",
      "        [-2.3899, -2.4665, -2.2398,  1.4143,  3.2935,  0.6950, -1.9838, -2.3337,\n",
      "         -2.3000, -1.6550]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "output in train\n",
      "tensor([[-1.2887, -1.0604, -1.5291, -1.3755, -1.8768, -1.5486, -0.9040,  0.6192,\n",
      "          2.9410, -1.5238],\n",
      "        [-2.8537, -3.1792, -2.9111, -2.9699, -3.3405, -2.2362,  4.4344,  1.4417,\n",
      "          0.6485, -2.5494],\n",
      "        [-3.3379, -3.2402, -3.1550, -2.9390, -3.1510, -2.4353,  4.3944,  0.8783,\n",
      "          0.8375, -2.1872],\n",
      "        [-1.6524, -1.3175, -1.7781, -1.8878, -2.1704, -1.9626, -0.4126,  1.1103,\n",
      "          3.0443, -1.7734],\n",
      "        [-1.7618, -3.0356, -2.9878, -2.6637, -4.1964, -3.2032,  2.3903,  2.6531,\n",
      "          1.0861, -3.5225],\n",
      "        [-3.0513, -3.3168, -3.5051, -2.9069, -4.0847, -3.5600,  5.1635,  0.7023,\n",
      "          1.0998, -4.2928],\n",
      "        [-2.4992, -2.7186, -2.8287, -1.6355, -2.8436, -3.0071,  6.0030,  0.3435,\n",
      "         -0.8598, -1.9597],\n",
      "        [-1.4017, -1.8225, -2.1260, -2.4650, -2.2000, -1.9163,  2.1560,  1.1136,\n",
      "          1.3184, -2.1303],\n",
      "        [-0.7203, -1.4473, -1.5625, -1.3324, -1.5960, -1.5704,  0.9424, -0.2927,\n",
      "          2.5584, -1.5658],\n",
      "        [-0.6424, -1.3185, -1.5611, -1.1999, -1.0083, -1.3503,  3.5459, -0.8205,\n",
      "         -0.7068, -1.1135]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "output in train\n",
      "tensor([[-0.7424, -0.4449, -0.6403,  0.2327,  1.0676,  1.2991, -0.6059, -0.8684,\n",
      "         -0.8045, -0.5597],\n",
      "        [-1.5761, -1.4499, -1.6951,  1.3485,  2.9631,  0.6420, -1.6584, -1.7729,\n",
      "         -1.3400, -1.3276],\n",
      "        [-2.1583, -2.3721, -2.1986,  2.4572,  2.6690,  1.0243, -2.3622, -2.0876,\n",
      "         -2.0685, -2.1261],\n",
      "        [-1.7985, -1.4541, -1.7101,  2.9796, -0.2959,  1.8642, -1.7643, -1.7718,\n",
      "         -2.0427, -1.4464],\n",
      "        [-0.3826, -0.6848, -0.8844,  0.8399,  0.3536,  0.8521, -0.7308, -0.5331,\n",
      "         -0.7833, -0.6715],\n",
      "        [-1.9231, -2.0354, -1.8591,  2.8947, -1.0486,  2.9340, -2.0516, -2.0852,\n",
      "         -1.8104, -2.0319],\n",
      "        [-1.8013, -1.8165, -1.9338,  1.1690,  2.6676,  1.5980, -1.6250, -1.9562,\n",
      "         -2.0510, -1.8797],\n",
      "        [-0.5890, -0.7450, -0.6562, -0.3405,  2.2701,  0.1360, -0.7019, -0.8867,\n",
      "         -0.7581, -0.6729],\n",
      "        [-1.6221, -1.3749, -1.6388,  2.0114, -0.7427,  3.3021, -1.7299, -1.9211,\n",
      "         -1.3633, -1.4056],\n",
      "        [-1.0853, -1.4684, -1.3794, -0.7701,  4.2078,  0.1619, -1.0945, -1.3627,\n",
      "         -1.2808, -1.3289]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "output in train\n",
      "tensor([[-1.4003, -1.0698, -1.4536, -1.2121, -1.1272, -1.2829, -0.7466,  0.1791,\n",
      "          4.6856, -1.4113],\n",
      "        [-0.6779, -0.6143, -0.7589, -0.7950, -0.2963, -0.5557, -1.0164,  3.6007,\n",
      "         -0.7190, -0.5200],\n",
      "        [-0.6374, -0.4794, -0.8645, -0.7714, -0.7297, -1.0098, -1.0253, -0.5977,\n",
      "          3.2460, -0.7416],\n",
      "        [-1.8475, -1.7743, -1.6802, -1.5094, -0.8114, -1.8668, -1.0584,  4.7614,\n",
      "          0.4220, -1.5701],\n",
      "        [-2.7090, -3.8335, -2.9084, -2.7464, -3.5118, -3.3480,  6.2796,  1.9178,\n",
      "         -0.2497, -3.2522],\n",
      "        [-2.2653, -2.1224, -2.1664, -2.2908, -1.8308, -2.0201, -0.2818, -0.7384,\n",
      "          6.8258, -1.9740],\n",
      "        [-1.7333, -1.7533, -1.8222, -1.6247, -2.0533, -2.0840,  5.9935, -1.2283,\n",
      "          0.0267, -1.9333],\n",
      "        [-1.1109, -1.5203, -1.3525, -1.2657, -0.9091, -1.1029,  4.6331,  0.3872,\n",
      "         -1.8093, -1.1826],\n",
      "        [-2.1409, -2.1713, -2.0330, -1.9543, -1.9941, -1.6982, -0.2086,  4.9748,\n",
      "          0.5582, -1.8167],\n",
      "        [-1.6992, -1.9343, -1.5299, -1.6790, -1.6932, -1.3439,  4.4757,  1.2546,\n",
      "         -0.5420, -1.8083]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "output in train\n",
      "tensor([[ 3.7496,  0.0465,  1.1804, -1.9602, -1.8838, -1.8411, -1.6987, -1.4774,\n",
      "         -1.1932, -1.8863],\n",
      "        [ 0.3548,  0.5327,  2.1366, -1.0858, -1.0827, -1.0283, -1.1430, -1.1446,\n",
      "         -1.0142, -1.0373],\n",
      "        [ 0.8204, -0.6539,  1.7262, -0.8035, -0.8008, -0.7175, -0.7983, -0.9202,\n",
      "         -0.8580, -0.6443],\n",
      "        [ 0.7292,  4.3334,  2.9150, -3.0129, -2.6120, -2.7766, -2.6078, -2.7555,\n",
      "         -3.2327, -3.0308],\n",
      "        [-0.7933,  1.0241,  3.6602, -1.6200, -1.6431, -1.6178, -1.6451, -1.6747,\n",
      "         -1.6878, -1.5054],\n",
      "        [ 1.4497,  0.6475,  0.9696, -1.2092, -1.1964, -1.3541, -1.1145, -0.9902,\n",
      "         -0.9028, -1.2792],\n",
      "        [-0.2437,  4.0247, -0.1141, -1.6853, -1.6505, -1.4111, -1.7026, -2.0102,\n",
      "         -1.4008, -1.4952],\n",
      "        [ 2.9808,  0.2742,  0.4193, -1.1794, -1.4064, -1.4892, -1.3373, -1.2082,\n",
      "         -0.8857, -1.5431],\n",
      "        [ 1.6615,  1.3558,  5.6270, -3.4656, -3.4770, -3.4313, -3.4825, -3.4777,\n",
      "         -3.2158, -3.4091],\n",
      "        [ 4.3464,  0.6900,  2.5018, -2.8192, -2.7936, -2.5717, -2.6576, -2.9394,\n",
      "         -2.3974, -2.9565]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "client_acc: 0.783, cluster_acc: [0.5, 0.3, 0.2]: [0.805, 0.665, 0.907],  global_acc: 0.185\n",
      "output in train\n",
      "tensor([[ 1.5380,  2.9431,  0.0515, -1.3196, -1.4294, -1.1685, -1.2356, -1.2824,\n",
      "         -1.6329, -1.0381],\n",
      "        [ 3.2775,  0.3506,  1.4482, -2.0179, -1.8032, -1.6385, -1.7604, -1.9945,\n",
      "         -1.6533, -1.4477],\n",
      "        [ 1.1688,  0.9589, -0.1137, -0.6926, -0.5654, -0.7971, -0.6063, -0.7087,\n",
      "         -0.5714, -0.7005],\n",
      "        [ 1.2237,  8.3106,  0.4278, -4.6329, -4.5307, -4.0885, -4.7287, -4.6919,\n",
      "         -4.2601, -4.5040],\n",
      "        [ 0.3481,  1.5094,  1.4619, -1.3672, -1.2851, -1.2426, -1.2530, -1.1386,\n",
      "         -1.2227, -1.3025],\n",
      "        [ 1.2095,  0.2542,  3.9207, -1.7892, -1.8708, -1.7612, -1.5454, -1.7844,\n",
      "         -1.6459, -2.0042],\n",
      "        [ 3.7630,  0.4292, -0.6783, -1.5404, -1.4316, -1.1184, -1.2419, -1.2311,\n",
      "         -1.3908, -1.5235],\n",
      "        [ 1.3114,  1.2707,  5.7375, -2.9423, -2.8777, -2.8358, -2.8376, -2.8753,\n",
      "         -2.8394, -2.9853],\n",
      "        [ 3.7091, -0.4342,  3.5838, -2.2448, -2.0592, -1.9472, -2.1648, -2.2959,\n",
      "         -1.8089, -2.1261],\n",
      "        [ 2.8304,  1.3509,  9.4937, -4.5683, -4.9422, -4.7921, -4.7358, -4.6889,\n",
      "         -4.4494, -4.4363]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "output in train\n",
      "tensor([[-2.1093, -2.2951, -2.2027,  3.4274, -0.1735,  3.0175, -1.9692, -1.8185,\n",
      "         -2.0260, -1.9608],\n",
      "        [-0.9572, -1.1069, -0.9724, -1.1418,  4.6624, -0.8250, -0.8127, -0.8760,\n",
      "         -0.8742, -0.8163],\n",
      "        [-1.6864, -1.7645, -1.7485, -0.4913,  6.5869, -0.4966, -1.6660, -1.6480,\n",
      "         -1.6520, -1.5963],\n",
      "        [-2.0376, -2.1890, -2.1349, -0.1286, -1.7908,  6.9023, -1.6803, -1.7828,\n",
      "         -1.9403, -1.7430],\n",
      "        [-6.6626, -6.6427, -6.4570, 14.0872,  3.6692,  1.3776, -6.4351, -6.1321,\n",
      "         -6.1822, -6.3387],\n",
      "        [-1.0895, -1.2268, -1.0735, -0.5995,  3.3101,  0.6580, -0.9406, -0.9713,\n",
      "         -0.9153, -0.9005],\n",
      "        [-2.5457, -2.6447, -2.4955,  5.1569,  1.5192,  1.3927, -2.5580, -2.3157,\n",
      "         -2.2975, -2.4684],\n",
      "        [-1.4270, -1.5275, -1.4564,  2.2542,  0.4351,  1.6632, -1.3139, -1.4166,\n",
      "         -1.4792, -1.3709],\n",
      "        [-1.4420, -1.5994, -1.3890,  1.4914, -1.2167,  2.8845, -1.1028, -1.0335,\n",
      "         -1.3376, -1.2078],\n",
      "        [-2.5123, -2.6535, -2.6114,  5.2394,  0.6265,  2.6271, -2.3124, -2.4169,\n",
      "         -2.5041, -2.5502]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "output in train\n",
      "tensor([[-1.5500, -1.5434, -1.5634, -1.6547, -1.5429, -1.6209,  4.8509, -0.2107,\n",
      "         -0.8289, -1.5493],\n",
      "        [-2.0111, -2.3586, -2.3226, -2.3155, -2.3275, -2.3930, -2.4892, -1.7737,\n",
      "          9.9200, -2.1783],\n",
      "        [-1.0396, -1.3459, -1.2982, -1.2172, -1.3391, -1.3418, -1.0347,  3.9347,\n",
      "          0.5885, -1.2191],\n",
      "        [-2.5569, -2.7113, -2.7159, -2.7021, -2.6792, -2.6778,  7.0104,  0.7588,\n",
      "          0.3534, -2.5918],\n",
      "        [-2.4228, -2.7108, -2.6114, -2.5539, -2.6110, -2.6366, -2.9564, -1.4241,\n",
      "         10.6530, -2.4857],\n",
      "        [-1.1756, -1.5617, -1.5246, -1.4630, -1.4889, -1.5798, -1.5844, -0.6386,\n",
      "          5.5694, -1.4626],\n",
      "        [-1.2591, -1.5935, -1.5618, -1.5700, -1.5813, -1.5706,  5.6673, -0.4016,\n",
      "         -1.9026, -1.4320],\n",
      "        [-1.1991, -1.3981, -1.4009, -1.4013, -1.4249, -1.4449, -1.2937,  4.0787,\n",
      "          0.9513, -1.3901],\n",
      "        [-1.2352, -1.6256, -1.5878, -1.6382, -1.5463, -1.5706, -0.7548,  4.9761,\n",
      "         -0.3876, -1.5127],\n",
      "        [-1.2430, -1.5004, -1.5577, -1.5059, -1.5178, -1.5524, -1.1174,  5.4770,\n",
      "         -0.3736, -1.4557]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "output in train\n",
      "tensor([[ 5.8975,  0.4355, -0.5311, -2.7107, -2.4480, -2.4679, -2.2404, -2.7339,\n",
      "         -2.1670, -2.2692],\n",
      "        [-0.4252,  4.6662,  1.1146, -2.4869, -2.4341, -2.3561, -2.2975, -2.4639,\n",
      "         -2.3046, -2.2378],\n",
      "        [ 1.5568, -0.8906,  4.1451, -1.8657, -1.6943, -1.8201, -1.5780, -1.9049,\n",
      "         -1.5999, -1.7932],\n",
      "        [ 1.0404, -1.4061,  9.2108, -3.0459, -2.9188, -2.9700, -2.8292, -3.1004,\n",
      "         -2.9375, -2.8698],\n",
      "        [ 2.7207, -0.3383, -0.0515, -0.8781, -0.8022, -0.8373, -0.8182, -0.8815,\n",
      "         -0.7923, -0.7686],\n",
      "        [ 0.9122, -1.0217,  6.4855, -2.4073, -2.2223, -2.3358, -2.2079, -2.4142,\n",
      "         -2.1791, -2.3313],\n",
      "        [-1.2225,  5.0337, -0.9300, -1.5632, -1.4887, -1.4488, -1.2975, -1.5573,\n",
      "         -1.1407, -1.2953],\n",
      "        [ 2.2882, -0.8953,  3.8098, -1.6262, -1.5360, -1.5952, -1.5790, -1.6488,\n",
      "         -1.6387, -1.5642],\n",
      "        [ 4.6701,  2.0065,  0.6619, -2.8706, -2.9248, -2.8652, -2.5464, -3.0678,\n",
      "         -2.8195, -2.5481],\n",
      "        [ 1.8050, -0.9846,  9.4733, -3.5950, -3.5378, -3.5300, -3.1564, -3.6378,\n",
      "         -3.4853, -3.2987]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "output in train\n",
      "tensor([[-2.7547, -2.7162, -2.7542, -2.7021, -2.6644, -2.6637,  9.0988, -0.3398,\n",
      "         -1.6844, -2.7488],\n",
      "        [-0.7730, -0.7550, -0.7953, -0.8003, -0.7097, -0.6203, -0.4068,  1.9935,\n",
      "         -0.2962, -0.7902],\n",
      "        [-1.9598, -1.9444, -1.9862, -2.0107, -1.7357, -1.6725,  7.3857, -0.9912,\n",
      "         -1.7313, -1.9830],\n",
      "        [-1.2755, -1.2569, -1.3401, -1.3521, -1.1826, -1.0975, -1.0815,  4.7850,\n",
      "         -1.3548, -1.3527],\n",
      "        [-1.4912, -1.3864, -1.5481, -1.5851, -1.3177, -1.2138,  0.9598,  1.8952,\n",
      "         -0.4101, -1.5212],\n",
      "        [-0.9049, -0.8334, -0.9678, -0.9917, -0.7319, -0.6170, -0.8553,  2.0630,\n",
      "          0.1005, -0.9457],\n",
      "        [-1.2210, -1.1139, -1.2792, -1.3292, -0.9265, -0.7279,  3.4697,  0.0153,\n",
      "         -1.0412, -1.2680],\n",
      "        [-0.9922, -0.9143, -1.0024, -1.0379, -0.7889, -0.7200, -0.9271,  3.3276,\n",
      "         -0.9348, -1.0013],\n",
      "        [-1.0696, -0.9699, -1.0997, -1.1262, -0.8764, -0.7806, -0.4596,  3.0833,\n",
      "         -0.9757, -1.1156],\n",
      "        [-2.2208, -2.1887, -2.3250, -2.3449, -1.8547, -1.9030,  2.1771,  3.5703,\n",
      "         -0.8837, -2.2768]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "output in train\n",
      "tensor([[-1.2462, -1.1981, -1.2841, -1.2867, -1.0189, -1.0250, -1.4325, -0.5927,\n",
      "          4.3686, -1.2714],\n",
      "        [-1.6462, -1.6127, -1.6259, -1.6255, -1.5392, -1.5162, -0.9327,  4.8097,\n",
      "         -0.4149, -1.6444],\n",
      "        [-2.2129, -2.2262, -2.1726, -2.1353, -2.2220, -2.0996, -1.5475, -0.9623,\n",
      "          8.6030, -2.2012],\n",
      "        [-1.5911, -1.5582, -1.6165, -1.5955, -1.4635, -1.3645, -1.3515, -0.2686,\n",
      "          5.2027, -1.6219],\n",
      "        [-2.0325, -2.0212, -1.9939, -1.9636, -2.0337, -1.8535, -1.6621, -0.9195,\n",
      "          8.7067, -2.0112],\n",
      "        [-1.3959, -1.3521, -1.3922, -1.3713, -1.2056, -1.2401, -1.5390, -1.3519,\n",
      "          5.8530, -1.3902],\n",
      "        [-2.3701, -2.3490, -2.3118, -2.2630, -2.3143, -2.3059, -1.4867, -0.9406,\n",
      "          9.1386, -2.3369],\n",
      "        [-1.4820, -1.4311, -1.4748, -1.4837, -1.3393, -1.3189, -1.0347,  4.8572,\n",
      "         -1.2608, -1.4826],\n",
      "        [-1.3509, -1.3331, -1.3483, -1.3555, -1.3051, -1.2235,  5.2246, -1.0947,\n",
      "         -1.5225, -1.3690],\n",
      "        [-1.6294, -1.6094, -1.6347, -1.6427, -1.5223, -1.5030, -0.3208,  3.8632,\n",
      "         -0.0553, -1.6337]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "output in train\n",
      "tensor([[-1.2543, -1.2013, -1.2606, -1.2821, -1.0815, -1.0555,  4.4894, -1.2486,\n",
      "         -1.3519, -1.2684],\n",
      "        [-2.0725, -2.1094, -2.0332, -2.0084, -2.0458, -2.0893,  0.6470,  6.3171,\n",
      "         -1.2960, -2.0612],\n",
      "        [-2.1165, -2.1395, -2.0994, -2.0786, -2.0105, -2.0301,  7.1807, -0.6630,\n",
      "         -1.5920, -2.1069],\n",
      "        [-1.6843, -1.6481, -1.6656, -1.6435, -1.5136, -1.5347, -1.4093, -0.8100,\n",
      "          6.2826, -1.6693],\n",
      "        [-1.6725, -1.6350, -1.6479, -1.6390, -1.6379, -1.5174,  6.3268, -1.1018,\n",
      "         -1.3864, -1.6508],\n",
      "        [-2.2731, -2.2755, -2.2047, -2.1552, -2.3282, -2.3288,  8.6065, -0.0534,\n",
      "         -1.7121, -2.2133],\n",
      "        [-1.7797, -1.7457, -1.7432, -1.7291, -1.7628, -1.6874,  6.3236, -0.7687,\n",
      "         -1.6389, -1.7661],\n",
      "        [-1.8903, -1.8715, -1.8759, -1.8693, -1.8662, -1.8331,  7.3953, -1.4313,\n",
      "         -1.5431, -1.8930],\n",
      "        [-1.6377, -1.6095, -1.6179, -1.6229, -1.5823, -1.5514, -0.9804,  5.2729,\n",
      "         -0.9425, -1.6395],\n",
      "        [-1.8372, -1.8246, -1.8335, -1.8357, -1.7651, -1.7353,  0.7073,  3.1708,\n",
      "          0.3649, -1.8490]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "client_acc: 0.799, cluster_acc: [0.5, 0.3, 0.2]: [0.832, 0.677, 0.902],  global_acc: 0.089\n",
      "output in train\n",
      "tensor([[-1.1542e+00,  4.3009e+00,  7.0761e-01, -1.9719e+00, -2.0308e+00,\n",
      "         -2.0374e+00, -1.8780e+00, -1.9439e+00, -1.9454e+00, -2.0004e+00],\n",
      "        [-3.3314e-01, -1.1641e+00,  4.2682e+00, -1.4519e+00, -1.4795e+00,\n",
      "         -1.4826e+00, -1.3732e+00, -1.4301e+00, -1.4187e+00, -1.4434e+00],\n",
      "        [ 2.0359e-01,  3.0379e+00, -3.4176e-01, -1.5807e+00, -1.6246e+00,\n",
      "         -1.6073e+00, -1.5079e+00, -1.5327e+00, -1.5170e+00, -1.5954e+00],\n",
      "        [ 4.4400e+00, -1.1456e+00,  1.1689e-01, -2.0817e+00, -2.0932e+00,\n",
      "         -2.0864e+00, -1.9215e+00, -1.9653e+00, -1.9251e+00, -2.0571e+00],\n",
      "        [-2.5243e-01, -7.6218e-01,  4.0014e+00, -1.8075e+00, -1.8583e+00,\n",
      "         -1.8551e+00, -1.6254e+00, -1.7143e+00, -1.7151e+00, -1.7836e+00],\n",
      "        [ 1.4562e-03, -1.3283e+00,  3.4114e+00, -1.4282e+00, -1.4691e+00,\n",
      "         -1.4709e+00, -1.3290e+00, -1.3847e+00, -1.3656e+00, -1.4077e+00],\n",
      "        [ 2.3428e+00, -9.1145e-01,  8.5380e-02, -8.7246e-01, -8.9565e-01,\n",
      "         -8.9427e-01, -8.0628e-01, -8.5704e-01, -8.2694e-01, -8.7645e-01],\n",
      "        [-1.6419e-01, -1.0488e+00,  3.3304e+00, -1.6616e+00, -1.7232e+00,\n",
      "         -1.7151e+00, -1.4786e+00, -1.5597e+00, -1.5706e+00, -1.6224e+00],\n",
      "        [ 3.6924e-01, -1.0244e+00,  6.0800e+00, -2.7515e+00, -2.7574e+00,\n",
      "         -2.7555e+00, -2.6926e+00, -2.6939e+00, -2.7099e+00, -2.7202e+00],\n",
      "        [-2.6707e-01, -1.7298e+00,  4.6220e+00, -1.7920e+00, -1.8407e+00,\n",
      "         -1.8335e+00, -1.6497e+00, -1.7371e+00, -1.7434e+00, -1.7719e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "output in train\n",
      "tensor([[-1.6307, -1.6240, -1.5844,  1.0130,  2.4997,  0.0550, -1.5681, -1.6361,\n",
      "         -1.6242, -1.6048],\n",
      "        [-2.4223, -2.4040, -2.3830,  3.7123, -0.2080,  2.4975, -2.4444, -2.3971,\n",
      "         -2.4218, -2.3534],\n",
      "        [-2.0962, -2.0828, -2.0655,  4.1530, -0.3540,  1.5730, -2.1170, -2.0910,\n",
      "         -2.1037, -2.0484],\n",
      "        [-3.4314, -3.4115, -3.3268,  2.7682,  1.7292,  3.6227, -3.4385, -3.4161,\n",
      "         -3.4348, -3.3334],\n",
      "        [-2.1284, -2.1173, -2.0651,  1.8482,  0.2616,  2.9169, -2.1240, -2.1196,\n",
      "         -2.1346, -2.0707],\n",
      "        [-2.3453, -2.3264, -2.2559,  2.3288,  1.9264,  1.6620, -2.3186, -2.3307,\n",
      "         -2.3558, -2.2689],\n",
      "        [-1.5977, -1.5910, -1.5582, -0.1786,  5.4525, -0.6027, -1.6194, -1.5969,\n",
      "         -1.5986, -1.5319],\n",
      "        [-1.9404, -1.9340, -1.9490,  3.8790,  0.6057,  0.5910, -1.9709, -1.9349,\n",
      "         -1.9474, -1.9026],\n",
      "        [-1.1178, -1.1138, -1.1046,  0.4960, -0.0523,  1.9010, -1.0886, -1.1141,\n",
      "         -1.1212, -1.0904],\n",
      "        [-2.6015, -2.5673, -2.4603,  1.1420,  0.6994,  4.0684, -2.5835, -2.5762,\n",
      "         -2.6166, -2.5116]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "output in train\n",
      "tensor([[ 0.5661, -0.3060,  4.9563, -2.7134, -2.6459, -2.6280, -2.6776, -2.7378,\n",
      "         -2.5429, -2.7678],\n",
      "        [ 0.2630, -0.1295,  1.5029, -1.1284, -1.0744, -1.0725, -1.0826, -1.1359,\n",
      "         -1.0106, -1.1669],\n",
      "        [ 3.5303,  0.5784, -0.4757, -1.6302, -1.6106, -1.6316, -1.6260, -1.6267,\n",
      "         -1.5787, -1.6246],\n",
      "        [ 2.3150, -0.6321,  0.9194, -1.2371, -1.2075, -1.2072, -1.2145, -1.2352,\n",
      "         -1.1776, -1.2324],\n",
      "        [ 1.1024,  3.2888,  0.8146, -2.8784, -2.8356, -2.8372, -2.8742, -2.9128,\n",
      "         -2.7304, -2.9552],\n",
      "        [ 0.5290,  0.8715,  5.4080, -3.0784, -3.0646, -3.0867, -3.0276, -3.0847,\n",
      "         -2.9103, -3.0860],\n",
      "        [ 1.7695,  5.7680, -0.7079, -3.0250, -3.0494, -3.0621, -3.0700, -3.0383,\n",
      "         -2.9496, -3.0398],\n",
      "        [ 1.7576,  4.7074, -0.6646, -2.9574, -2.9517, -2.9814, -2.9858, -2.9660,\n",
      "         -2.8983, -2.9606],\n",
      "        [ 1.0197,  0.3484,  0.3455, -0.9588, -0.9421, -0.9380, -0.9399, -0.9649,\n",
      "         -0.8774, -0.9700],\n",
      "        [ 1.5351,  3.1434, -0.6533, -2.0060, -1.9918, -1.9969, -2.0209, -2.0099,\n",
      "         -1.9269, -2.0159]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "output in train\n",
      "tensor([[ 2.6570,  0.8208, -0.0212, -2.3701, -2.3913, -2.3871, -2.2925, -2.3423,\n",
      "         -2.3061, -2.3676],\n",
      "        [ 1.0634, -1.1602,  3.5536, -2.6827, -2.7223, -2.7149, -2.5557, -2.6318,\n",
      "         -2.6233, -2.6577],\n",
      "        [ 0.8577,  5.0809, -0.2792, -2.8962, -2.8922, -2.8784, -2.9209, -2.9020,\n",
      "         -2.9209, -2.9072],\n",
      "        [ 5.3011,  2.3269,  0.5313, -4.0522, -4.0397, -4.0266, -4.0903, -4.0810,\n",
      "         -4.0410, -4.0574],\n",
      "        [-0.9528,  2.6571,  1.4330, -1.6117, -1.5968, -1.6008, -1.5892, -1.6052,\n",
      "         -1.6094, -1.5978],\n",
      "        [ 5.2085,  2.0775,  1.3198, -4.6910, -4.6942, -4.6787, -4.6821, -4.6779,\n",
      "         -4.6886, -4.6888],\n",
      "        [ 3.1382,  0.1649,  0.4082, -2.2124, -2.2337, -2.2294, -2.1700, -2.2127,\n",
      "         -2.1690, -2.2218],\n",
      "        [ 1.7569,  0.9158, -0.5059, -1.0314, -1.0356, -1.0318, -1.0279, -1.0271,\n",
      "         -1.0207, -1.0353],\n",
      "        [ 1.6959, -0.7912,  2.6012, -2.7284, -2.7683, -2.7585, -2.6239, -2.6731,\n",
      "         -2.6916, -2.7160],\n",
      "        [-0.6258, -0.3714,  2.0389, -1.0731, -1.0944, -1.0976, -0.9908, -1.0397,\n",
      "         -1.0305, -1.0623]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "output in train\n",
      "tensor([[-1.5697e+00, -1.6048e+00, -1.6141e+00,  2.4037e+00,  3.2340e-01,\n",
      "          1.2150e+00, -1.6081e+00, -1.5846e+00, -1.6094e+00, -1.5988e+00],\n",
      "        [-1.1496e+00, -1.1119e+00, -1.1147e+00,  3.1355e+00, -1.3708e-03,\n",
      "          1.9098e-01, -1.1232e+00, -1.1190e+00, -1.1252e+00, -1.1364e+00],\n",
      "        [-1.9810e+00, -1.9939e+00, -1.9870e+00,  4.4454e-01,  2.0292e-01,\n",
      "          4.1225e+00, -1.9858e+00, -1.9946e+00, -1.9903e+00, -2.0087e+00],\n",
      "        [-2.2760e+00, -2.2586e+00, -2.2684e+00,  4.4959e+00,  1.3856e+00,\n",
      "          3.3804e-01, -2.2758e+00, -2.2702e+00, -2.2772e+00, -2.2692e+00],\n",
      "        [-1.8504e+00, -1.8686e+00, -1.8576e+00, -7.4965e-01,  6.3468e+00,\n",
      "         -4.1113e-02, -1.8663e+00, -1.8617e+00, -1.8768e+00, -1.8860e+00],\n",
      "        [-9.0241e-01, -9.4863e-01, -9.4590e-01, -3.5224e-02,  3.9288e-02,\n",
      "          2.0797e+00, -9.4857e-01, -9.2958e-01, -9.3813e-01, -9.3205e-01],\n",
      "        [-1.6980e+00, -1.6483e+00, -1.6509e+00,  3.7697e+00, -3.9948e-01,\n",
      "          1.5650e+00, -1.6666e+00, -1.6758e+00, -1.6690e+00, -1.6826e+00],\n",
      "        [-1.3765e+00, -1.4470e+00, -1.4456e+00,  9.3963e-01,  3.7616e-01,\n",
      "          1.7179e+00, -1.4521e+00, -1.4364e+00, -1.4430e+00, -1.4105e+00],\n",
      "        [-1.9027e+00, -1.9143e+00, -1.9064e+00,  1.6666e-01,  2.0329e-01,\n",
      "          3.9612e+00, -1.9002e+00, -1.9117e+00, -1.9047e+00, -1.9096e+00],\n",
      "        [-1.5564e+00, -1.5649e+00, -1.5627e+00,  3.0005e-01,  4.0867e+00,\n",
      "          2.5002e-01, -1.5690e+00, -1.5343e+00, -1.5561e+00, -1.5620e+00]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "output in train\n",
      "tensor([[ 0.0062,  3.8285, -1.7108, -1.9486, -1.9131, -1.8085, -1.8845, -1.8935,\n",
      "         -1.8361, -1.8910],\n",
      "        [-0.1893,  5.8332, -0.5159, -3.1436, -3.1403, -3.0027, -3.1368, -3.1156,\n",
      "         -3.0923, -3.1427],\n",
      "        [ 2.0981,  0.9317,  0.0063, -2.0204, -1.9823, -1.8828, -1.9037, -1.9300,\n",
      "         -1.7799, -1.9240],\n",
      "        [ 4.4885,  0.8459,  1.0740, -3.6559, -3.5818, -3.3831, -3.5118, -3.5548,\n",
      "         -3.3946, -3.4851],\n",
      "        [ 1.3973, -0.3560,  1.9955, -1.2729, -1.2523, -1.2305, -1.2365, -1.2368,\n",
      "         -1.1843, -1.2372],\n",
      "        [ 2.5811, -0.1634,  0.5352, -1.4466, -1.4207, -1.3655, -1.3917, -1.4050,\n",
      "         -1.3236, -1.3897],\n",
      "        [-0.5797, -0.1843,  4.7851, -1.9581, -1.9266, -1.8683, -1.8926, -1.9066,\n",
      "         -1.8114, -1.9189],\n",
      "        [-0.2853,  6.1013, -1.1217, -3.0659, -3.0443, -2.9087, -3.0162, -3.0280,\n",
      "         -2.8933, -3.0455],\n",
      "        [ 1.1682,  0.7114,  5.5023, -3.3484, -3.3019, -3.2270, -3.2861, -3.2985,\n",
      "         -3.1930, -3.2568],\n",
      "        [ 2.2091,  1.0102, -0.5032, -1.8903, -1.8577, -1.7509, -1.7910, -1.8273,\n",
      "         -1.7051, -1.8206]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "client_acc: 0.806, cluster_acc: [0.5, 0.3, 0.2]: [0.831, 0.683, 0.928],  global_acc: 0.134\n",
      "output in train\n",
      "tensor([[-1.5678, -1.5763, -1.5788,  0.4369,  3.7909,  0.8171, -1.5781, -1.5707,\n",
      "         -1.5789, -1.5726],\n",
      "        [-1.3628, -1.3399, -1.3393,  3.1829,  0.1745,  1.1576, -1.3426, -1.3502,\n",
      "         -1.3443, -1.3444],\n",
      "        [-0.9952, -0.9901, -0.9924,  1.3191, -0.1579,  2.0072, -0.9932, -0.9926,\n",
      "         -0.9922, -0.9945],\n",
      "        [-2.5037, -2.4464, -2.4470,  1.4759,  7.1186,  0.4379, -2.4521, -2.4665,\n",
      "         -2.4590, -2.4850],\n",
      "        [-1.7166, -1.7058, -1.7036,  3.6459, -0.1929,  2.2132, -1.7074, -1.7183,\n",
      "         -1.7144, -1.7177],\n",
      "        [-2.5758, -2.5532, -2.5526,  5.1447,  0.9530,  2.2457, -2.5541, -2.5646,\n",
      "         -2.5639, -2.5605],\n",
      "        [-3.4840, -3.3442, -3.3519,  1.9428,  9.3334,  1.3216, -3.3591, -3.3895,\n",
      "         -3.3703, -3.4307],\n",
      "        [-3.0610, -3.0737, -3.0710,  3.1603,  1.8944,  4.4728, -3.0766, -3.0862,\n",
      "         -3.0814, -3.0752],\n",
      "        [-2.8863, -2.7973, -2.8015,  5.5151, -0.4172,  4.4849, -2.8052, -2.8213,\n",
      "         -2.8199, -2.8428],\n",
      "        [-2.2217, -2.2256, -2.2232,  4.3198,  0.9701,  1.5635, -2.2229, -2.2138,\n",
      "         -2.2256, -2.2128]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "output in train\n",
      "tensor([[-0.0152,  2.3580,  0.8786, -1.2168, -1.2150, -1.2095, -1.2202, -1.2258,\n",
      "         -1.1946, -1.2494],\n",
      "        [ 0.0693,  7.3496, -0.3390, -2.7412, -2.7597, -2.7437, -2.7681, -2.7574,\n",
      "         -2.7719, -2.7904],\n",
      "        [ 1.1814,  0.2792,  5.7966, -2.8041, -2.8191, -2.8258, -2.7978, -2.8074,\n",
      "         -2.8107, -2.8329],\n",
      "        [ 0.5284, -0.2811,  2.5317, -1.1609, -1.1345, -1.1543, -1.1498, -1.1665,\n",
      "         -1.1312, -1.1838],\n",
      "        [ 0.0574,  2.2698,  1.4785, -1.6440, -1.6252, -1.6302, -1.6453, -1.6591,\n",
      "         -1.6027, -1.6878],\n",
      "        [ 0.5393,  3.7458,  0.0419, -1.8005, -1.7885, -1.7949, -1.8059, -1.8069,\n",
      "         -1.7497, -1.8367],\n",
      "        [ 6.6182,  0.1905,  0.1418, -2.5481, -2.5827, -2.5629, -2.5550, -2.5180,\n",
      "         -2.6642, -2.4954],\n",
      "        [ 0.1801,  0.3863,  3.9918, -2.2176, -2.1725, -2.1952, -2.1979, -2.2369,\n",
      "         -2.1376, -2.2879],\n",
      "        [-0.9059,  3.7112,  1.8748, -2.1110, -2.0824, -2.0731, -2.1153, -2.1297,\n",
      "         -2.0498, -2.1793],\n",
      "        [ 9.4136,  0.0215,  0.9442, -3.4164, -3.5157, -3.4580, -3.4501, -3.3728,\n",
      "         -3.7024, -3.2984]], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now(pytz.timezone('Asia/Seoul'))\n",
    "date_time = now.strftime(\"%m%d_%H%M\")\n",
    "\n",
    "columns = pd.MultiIndex.from_product([['client_accs', 'cluster_accs', 'global_accs'], ['before_distill', 'after_distill']],\n",
    "                                     names=['acc_type', 'distill_state'])\n",
    "\n",
    "# The tuples for which we want to run the experiment \n",
    "desired_pairs = [(40000, 4000)]\n",
    "cluster_distribution = [0.5, 0.3, 0.2]\n",
    "# Add additional index 'global_distill' and 'cluster_distill'\n",
    "experiments = ['cluster_distill', 'global_distill']\n",
    "index = pd.MultiIndex.from_product([experiments, desired_pairs], names=['experiment', 'data_pair'])\n",
    "\n",
    "# Initialize an empty DataFrame with the desired index for rows and columns\n",
    "df = pd.DataFrame(index=index, columns=columns)\n",
    "\n",
    "for exp in experiments:\n",
    "    for pair in desired_pairs:\n",
    "        client_data, distill_data = pair\n",
    "\n",
    "        if exp == 'global_distill':\n",
    "            client_accs, cluster_accs, global_accs = global_distill_experiments(client_data, distill_data, ALPHA, NUMBER_OF_CLUSTER, cluster_distribution)\n",
    "        else:\n",
    "            client_accs, cluster_accs, global_accs = cluster_distill_experiments(client_data, distill_data, ALPHA, NUMBER_OF_CLUSTER, cluster_distribution)\n",
    "\n",
    "        # Set the values in the DataFrame\n",
    "        df.loc[(exp, pair), ('client_accs', 'before_distill')] = client_accs[-2]\n",
    "        df.loc[(exp, pair), ('client_accs', 'after_distill')] = client_accs[-1]\n",
    "        df.loc[(exp, pair), ('global_accs', 'before_distill')] = global_accs[-2]\n",
    "        df.loc[(exp, pair), ('global_accs', 'after_distill')] = global_accs[-1]\n",
    "        df.loc[(exp, pair), ('cluster_accs', 'before_distill')] = cluster_accs[-2]\n",
    "        df.loc[(exp, pair), ('cluster_accs', 'after_distill')] = cluster_accs[-1]\n",
    "\n",
    "        directory = f'results/Unbalanced_cluster'\n",
    "\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        cluster_distribution_str = \"_\".join(map(str, cluster_distribution))\n",
    "        file_name = f'{directory}/client:{N_CLIENTS}_cluster:{NUMBER_OF_CLUSTER}_distribution:{cluster_distribution_str}_{date_time}.csv'\n",
    "        df = df.round(decimals=3)\n",
    "        df.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('results/global_distill/CIFAR_0720_0435.csv', index_col=[0,1], header=[0,1])\n",
    "\n",
    "# 그릴 데이터와 제목을 리스트로 저장\n",
    "heatmap_data = [('client_accs', 'change_after_distill', 'Client Accuracy change after Distillation'),\n",
    "                ('global_accs', 'change_after_distill', 'Global Accuracy change after Distillation')]\n",
    "\n",
    "# Compute change in accuracy\n",
    "df[('client_accs', 'change_after_distill')] = df[('client_accs', 'after_distill')] - df[('client_accs', 'before_distill')]\n",
    "df[('global_accs', 'change_after_distill')] = df[('global_accs', 'after_distill')] - df[('global_accs', 'before_distill')]\n",
    "\n",
    "# 전체 데이터의 최솟값, 최댓값 계산\n",
    "vmin = min(df[data1][data2].min() for data1, data2, _ in heatmap_data)\n",
    "vmax = max(df[data1][data2].max() for data1, data2, _ in heatmap_data)\n",
    "\n",
    "for data1, data2, title in heatmap_data:\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    sns.heatmap(df[(data1, data2)].unstack(), annot=True, cmap='coolwarm', center=0, vmin=-0.1, vmax=0.2)\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Clustering 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def do_clustering_experiments(total_client_data=total_client_data, data_per_class=data_per_class, ALPHA=ALPHA):\n",
    "    train_idcs, test_idcs = idcs[:int(total_client_data*10)], idcs[int(total_client_data*10):]\n",
    "    train_labels = data.train_labels.numpy()\n",
    "    test_labels = data.train_labels.numpy()[int(total_client_data*10):]\n",
    "\n",
    "    client_idcs = split_noniid(train_idcs, train_labels, alpha=ALPHA, n_clients=N_CLIENTS)#, data_per_class=int(total_client_data/10))\n",
    "    # server_idcs = generate_server_idcs(test_idcs, test_labels, int(total_client_data*10))\n",
    "\n",
    "    client_data = [CustomSubset(data, idcs) for idcs in client_idcs]\n",
    "    test_data = CustomSubset(data, test_idcs, transforms.Compose([transforms.ToTensor()]))\n",
    "    \n",
    "    for i, client_datum in enumerate(client_data):\n",
    "        client_datum.subset_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    server = Server(resnet18, lambda x : torch.optim.SGD(x, lr=0.1, momentum=0.9),test_data)\n",
    "\n",
    "    \n",
    "    distillation_data_file = f'distillation_data_{data_per_class}_per_class.pth'\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if not os.path.exists(distillation_data_file):\n",
    "        # The file does not exist, generate and save the distillation data\n",
    "        distillation_data = server.make_distillation_data(data_per_class=data_per_class)\n",
    "        torch.save(distillation_data, distillation_data_file)\n",
    "\n",
    "    # Load the distillation data\n",
    "    distillation_data = torch.load(distillation_data_file)\n",
    "\n",
    "    clients = [Client(resnet18, lambda x : torch.optim.SGD(x, lr=0.1, momentum=0.9), dat, i, distillation_data) \n",
    "               for i, dat in enumerate(client_data)]\n",
    "\n",
    "    def aggregate(cluster_indices_new):\n",
    "        cluster_indices = cluster_indices_new\n",
    "        client_clusters = [[clients[i] for i in idcs] for idcs in cluster_indices]\n",
    "\n",
    "        server.aggregate_clusterwise(client_clusters)\n",
    "\n",
    "        return cluster_indices\n",
    "\n",
    "    cfl_stats = ExperimentLogger()\n",
    "\n",
    "    cluster_indices = [np.arange(len(clients)).astype(\"int\")]\n",
    "    client_clusters = [[clients[i] for i in idcs] for idcs in cluster_indices]\n",
    "\n",
    "\n",
    "    for epoch in range(1, LOCAL_EPOCHS+1):\n",
    "\n",
    "        if epoch == 1:\n",
    "            for client in clients:\n",
    "                client.synchronize_with_server(server)\n",
    "\n",
    "        participating_clients = server.select_clients(clients, frac=1.0)\n",
    "\n",
    "        for client in participating_clients:\n",
    "            if epoch == 1:\n",
    "                client.distill()\n",
    "\n",
    "            train_stats = client.compute_weight_update(epochs=1) #train client\n",
    "\n",
    "            if epoch == 1000:\n",
    "                client.reset()\n",
    "\n",
    "        cluster_indices_new = []\n",
    "\n",
    "        for idc in cluster_indices:\n",
    "            max_norm = server.compute_max_update_norm([clients[i] for i in idc])\n",
    "            mean_norm = server.compute_mean_update_norm([clients[i] for i in idc])\n",
    "\n",
    "            #cluster 나누는 기준\n",
    "            if epoch == LOCAL_EPOCHS: #무조건 한번 나누기\n",
    "                similarities = server.compute_pairwise_similarities(clients)\n",
    "\n",
    "                server.cache_model(idc, clients[idc[0]].W, acc_clients)\n",
    "\n",
    "                c1, c2, c3 = server.cluster_clients_GMM(similarities[idc][:,idc])\n",
    "                cluster_indices_new += [c1, c2, c3]\n",
    "\n",
    "        if epoch == 1000:\n",
    "            cluster_indices = aggregate(cluster_indices_new)\n",
    "\n",
    "        acc_clients = [client.evaluate() for client in clients]\n",
    "\n",
    "        if epoch == LOCAL_EPOCHS: #무조건 한번 나누기\n",
    "            label_accuracies = pd.DataFrame()\n",
    "            label_predicted = pd.DataFrame()\n",
    "            label_soft_sum = pd.DataFrame()\n",
    "            label_diff = pd.DataFrame()\n",
    "\n",
    "            for i, client in enumerate(clients):\n",
    "                acc, pred, sum_, diff = server.evaluate(client.model)\n",
    "                # Convert each dictionary to a DataFrame and append to the respective DataFrame\n",
    "                label_accuracies = label_accuracies.append(pd.DataFrame(acc, index=[i]))\n",
    "                label_predicted = label_predicted.append(pd.DataFrame(pred, index=[i]))\n",
    "                label_soft_sum = label_soft_sum.append(pd.DataFrame(sum_, index=[i]))\n",
    "                label_diff = label_diff.append(pd.DataFrame(diff, index=[i]))\n",
    "\n",
    "            # Reset index for all DataFrames\n",
    "            label_accuracies.reset_index(drop=True, inplace=True)\n",
    "            label_predicted.reset_index(drop=True, inplace=True)\n",
    "            label_soft_sum.reset_index(drop=True, inplace=True)\n",
    "            label_diff.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        if epoch == 1:\n",
    "            first_accuracies = pd.DataFrame()\n",
    "            for i, client in enumerate(clients):\n",
    "                first_acc, pred, sum_, diff = server.evaluate(client.model)\n",
    "                first_accuracies = pd.concat([first_accuracies, pd.DataFrame(first_acc, index=[i])])\n",
    "            first_accuracies = first_accuracies.fillna(0)\n",
    "\n",
    "            client_acc_after_distill = sum(acc_clients)/len(acc_clients)\n",
    "            global_acc_after_distill = np.mean(np.ravel(first_accuracies.values))\n",
    "\n",
    "\n",
    "        elif epoch == LOCAL_EPOCHS:\n",
    "            client_acc_final = sum(acc_clients)/len(acc_clients)\n",
    "            global_acc_final = np.mean(np.ravel(label_accuracies.values))\n",
    "\n",
    "        average_dw = server.get_average_dw(clients)\n",
    "        #print(average_dw)\n",
    "        cfl_stats.log({\"acc_clients\" : acc_clients, \"mean_norm\" : mean_norm, \"max_norm\" : max_norm,\n",
    "                      \"rounds\" : epoch, \"clusters\" : cluster_indices, \"average_dw\": average_dw})\n",
    "\n",
    "\n",
    "        display_train_stats(cfl_stats, EPS_1, EPS_2, LOCAL_EPOCHS)\n",
    "\n",
    "\n",
    "    for idc in cluster_indices:    \n",
    "        server.cache_model(idc, clients[idc[0]].W, acc_clients)\n",
    "    \n",
    "    client_acc_after_distill = round(client_acc_after_distill, 3)\n",
    "    global_acc_after_distill = round(global_acc_after_distill, 3)\n",
    "    client_acc_final = round(client_acc_final, 3)\n",
    "    global_acc_final = round(global_acc_final, 3)\n",
    "    \n",
    "    return client_acc_after_distill, global_acc_after_distill, client_acc_final, global_acc_final\n",
    "\n",
    "    print(client_acc_after_distill, global_acc_after_distill)\n",
    "    print(client_acc_final, global_acc_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_accuracies.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_soft_sum.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_diff.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_predicted.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Instantiate PCA\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Apply PCA to the dataframes\n",
    "label_accuracies_pca = pca.fit_transform(label_accuracies)\n",
    "label_predicted_pca = pca.fit_transform(label_predicted)\n",
    "label_soft_sum_pca = pca.fit_transform(label_soft_sum)\n",
    "label_diff_pca = pca.fit_transform(label_diff)\n",
    "transformed_data = pca.fit_transform(similarities)\n",
    "\n",
    "# Create labels\n",
    "labels = [0, 0, 0, 1, 1, 1, 2, 2, 2]\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "# Scatter plots with larger dots\n",
    "dot_size = 50\n",
    "axs[0, 0].scatter(label_accuracies_pca[:, 0], label_accuracies_pca[:, 1], c=labels, s=dot_size)\n",
    "axs[0, 0].set_title('Label Accuracies')\n",
    "axs[0, 1].scatter(label_predicted_pca[:, 0], label_predicted_pca[:, 1], c=labels, s=dot_size)\n",
    "axs[0, 1].set_title('Label Predicted')\n",
    "axs[1, 0].scatter(label_soft_sum_pca[:, 0], label_soft_sum_pca[:, 1], c=labels, s=dot_size)\n",
    "axs[1, 0].set_title('Label Soft Sum')\n",
    "axs[1, 1].scatter(label_diff_pca[:, 0], label_diff_pca[:, 1], c=labels, s=dot_size)\n",
    "axs[1, 1].set_title('Label Soft Diff')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Calculate Silhouette Scores\n",
    "silhouette_accuracies = silhouette_score(label_accuracies_pca, labels)\n",
    "silhouette_predicted = silhouette_score(label_predicted_pca, labels)\n",
    "silhouette_soft_sum = silhouette_score(label_soft_sum_pca, labels)\n",
    "silhouette_diff = silhouette_score(label_diff_pca, labels)\n",
    "silhouette_transformed_data = silhouette_score(transformed_data, labels)\n",
    "\n",
    "print('Silhouette Score for Accuracies:', silhouette_accuracies)\n",
    "print('Silhouette Score for Predicted:', silhouette_predicted)\n",
    "print('Silhouette Score for Soft Sum:', silhouette_soft_sum)\n",
    "print('Silhouette Score for diff:', silhouette_diff)\n",
    "print('Silhouette Score for Model params:', silhouette_transformed_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 Cluster 별 모델 파라미터 분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Fit and transform your data to 2D\n",
    "pca = PCA(n_components=2)\n",
    "transformed_data = pca.fit_transform(similarities)\n",
    "\n",
    "# Assign labels based on index ranges\n",
    "labels = [0, 0, 0, 1, 1, 1, 2, 2, 2]\n",
    "unique_labels = np.unique(labels)\n",
    "colors = plt.cm.Spectral(np.linspace(0, 0.35, len(unique_labels)))\n",
    "\n",
    "# Plot the transformed data with labels\n",
    "for label, color in zip(unique_labels, colors):\n",
    "    idx = np.where(labels == label)\n",
    "    plt.scatter(transformed_data[idx, 0], transformed_data[idx, 1], color=color, label=f'Cluster {label}')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.12.1-py3.8-cuda11.3",
   "language": "python",
   "name": "torch1.12.1-py3.8-cuda11.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "f79394e62bebc70f4ea6374f6a04753660b8235adfdaf8a6dfe67d7c0f65c745"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-12T12:37:56.017297Z",
     "iopub.status.busy": "2023-07-12T12:37:56.016896Z",
     "iopub.status.idle": "2023-07-12T12:37:57.555528Z",
     "shell.execute_reply": "2023-07-12T12:37:57.554313Z",
     "shell.execute_reply.started": "2023-07-12T12:37:56.017259Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models import ConvNet\n",
    "from helper import ExperimentLogger, display_train_stats\n",
    "from fl_devices import Server, Client\n",
    "from data_utils import split_not_contain_every_class, split_contain_every_class, generate_server_idcs, CustomSubset, split_noniid\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-12T12:37:57.556893Z",
     "iopub.status.busy": "2023-07-12T12:37:57.556597Z",
     "iopub.status.idle": "2023-07-12T12:37:57.559627Z",
     "shell.execute_reply": "2023-07-12T12:37:57.559119Z",
     "shell.execute_reply.started": "2023-07-12T12:37:57.556878Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "COMMUNICATION_ROUNDS = 50\n",
    "EPS_1 = 0.4\n",
    "EPS_2 = 1.6\n",
    "N_CLIENTS = 10\n",
    "\n",
    "total_client_data = 1000\n",
    "data_per_class = 50\n",
    "ALPHA = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-12T12:37:57.560241Z",
     "iopub.status.busy": "2023-07-12T12:37:57.560095Z",
     "iopub.status.idle": "2023-07-12T12:37:57.604366Z",
     "shell.execute_reply": "2023-07-12T12:37:57.603115Z",
     "shell.execute_reply.started": "2023-07-12T12:37:57.560229Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "data = datasets.MNIST(root=\"MNIST/\", download=False)\n",
    "\n",
    "mapp = np.array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype='<U1')\n",
    "\n",
    "idcs = np.random.permutation(len(data))\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-12T12:37:57.607884Z",
     "iopub.status.busy": "2023-07-12T12:37:57.607061Z",
     "iopub.status.idle": "2023-07-12T12:37:57.616050Z",
     "shell.execute_reply": "2023-07-12T12:37:57.615410Z",
     "shell.execute_reply.started": "2023-07-12T12:37:57.607858Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def do_experiments(total_client_data=total_client_data, data_per_class=data_per_class, ALPHA=ALPHA):\n",
    "    train_idcs, test_idcs = idcs[:int(total_client_data*10)], idcs[int(total_client_data*10):]\n",
    "    train_labels = data.targets\n",
    "    test_labels = data.targets[int(total_client_data*10):]\n",
    "\n",
    "\n",
    "    client_idcs = split_noniid(train_idcs, train_labels, alpha=ALPHA, n_clients=N_CLIENTS)\n",
    "    client_data = [CustomSubset(data, idcs) for idcs in client_idcs]\n",
    "    test_data = CustomSubset(data, test_idcs, transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "    for i, client_datum in enumerate(client_data):\n",
    "        client_datum.subset_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    server = Server(ConvNet, lambda x : torch.optim.SGD(x, lr=0.1, momentum=0.9),test_data)\n",
    "     \n",
    "    # save a model for size comparison\n",
    "    torch.save(server.model.state_dict(), 'model.pth')\n",
    "    \n",
    "    distillation_data_file = f'distillation_data_{data_per_class}_per_class.pth'\n",
    "    if not os.path.exists(distillation_data_file):\n",
    "        torch.save(server.make_distillation_data(data_per_class=data_per_class), distillation_data_file)\n",
    "        \n",
    "    distillation_data = torch.load(distillation_data_file)\n",
    "\n",
    "    clients = [Client(ConvNet, lambda x : torch.optim.SGD(x, lr=0.1, momentum=0.9), dat, i, distillation_data) \n",
    "               for i, dat in enumerate(client_data)]\n",
    "\n",
    "    cfl_stats = ExperimentLogger()\n",
    "\n",
    "    for c_round in range(1, COMMUNICATION_ROUNDS+1):\n",
    "        if c_round == 1:\n",
    "            for client in clients:\n",
    "                client.synchronize_with_server(server)\n",
    "                client.distill()\n",
    "                \n",
    "            first_acc_clients = [client.evaluate() for client in clients]\n",
    "            accuracies = [server.evaluate_distil(client.model) for client in clients]\n",
    "\n",
    "        server.select_clients(clients, frac=1.0)\n",
    "\n",
    "        for client in clients:\n",
    "            client.compute_weight_update(epochs=1) #train client\n",
    "\n",
    "        acc_clients = [client.evaluate() for client in clients]\n",
    "\n",
    "    label_accuracies = pd.DataFrame(server.evaluate(client.model)[0] for client in clients)\n",
    "\n",
    "    client_acc_after_distill = round(sum(first_acc_clients) / len(first_acc_clients), 3)\n",
    "    \n",
    "    global_acc_after_distill = round(np.mean(accuracies), 3)\n",
    "    #global_acc_after_distill = round(np.mean(np.ravel(pd.DataFrame(server.evaluate(client.model)[0] for client in clients).values)), 3)\n",
    "\n",
    "    client_acc_final = round(sum(acc_clients) / len(acc_clients), 3)\n",
    "    global_acc_final = round(np.mean(np.ravel(label_accuracies.values)), 3)\n",
    "    print(client_acc_after_distill, global_acc_after_distill, client_acc_final, global_acc_final)\n",
    "    return client_acc_after_distill, global_acc_after_distill, client_acc_final, global_acc_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-12T13:02:14.690020Z",
     "iopub.status.busy": "2023-07-12T13:02:14.689611Z",
     "iopub.status.idle": "2023-07-12T13:02:47.588653Z",
     "shell.execute_reply": "2023-07-12T13:02:47.587719Z",
     "shell.execute_reply.started": "2023-07-12T13:02:14.689985Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n",
      "total_client_data: 100, data_per_class: 100\n",
      "first acc: 0.667, 0.104\n",
      "last acc: 0.667, 0.103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Timer unit: 1e-09 s\n",
       "\n",
       "Total time: 32.8734 s\n",
       "File: /tmp/ipykernel_138368/2863498789.py\n",
       "Function: do_epoch_experiments at line 2\n",
       "\n",
       "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
       "==============================================================\n",
       "     2                                           def do_epoch_experiments(total_client_data=total_client_data, data_per_class=data_per_class, ALPHA=ALPHA):\n",
       "     3         1       7584.0   7584.0      0.0      train_idcs, test_idcs = idcs[:int(total_client_data*10)], idcs[int(total_client_data*10):]\n",
       "     4         1       1042.0   1042.0      0.0      train_labels = data.targets\n",
       "     5         1      46599.0  46599.0      0.0      test_labels = data.targets[int(total_client_data*10):]\n",
       "     6                                           \n",
       "     7                                           \n",
       "     8         1    1575824.0 1575824.0      0.0      client_idcs = split_noniid(train_idcs, train_labels, alpha=ALPHA, n_clients=N_CLIENTS)\n",
       "     9         1      34726.0  34726.0      0.0      client_data = [CustomSubset(data, idcs) for idcs in client_idcs]\n",
       "    10         1      38554.0  38554.0      0.0      test_data = CustomSubset(data, test_idcs, transforms.Compose([transforms.ToTensor()]))\n",
       "    11                                           \n",
       "    12        10       1791.0    179.1      0.0      for i, client_datum in enumerate(client_data):\n",
       "    13        10      29819.0   2981.9      0.0          client_datum.subset_transform = transforms.Compose([transforms.ToTensor()])\n",
       "    14                                           \n",
       "    15         1    1773459.0 1773459.0      0.0      server = Server(ConvNet, lambda x : torch.optim.SGD(x, lr=0.1, momentum=0.9),test_data)\n",
       "    16                                               \n",
       "    17         1        872.0    872.0      0.0      distillation_data_file = f'distill_data/MNIST/{data_per_class*10}.pth'\n",
       "    18         1      20669.0  20669.0      0.0      if not os.path.exists(distillation_data_file):\n",
       "    19                                                   torch.save(server.make_distillation_data(data_per_class=data_per_class), distillation_data_file)\n",
       "    20                                                   \n",
       "    21         1    3608667.0 3608667.0      0.0      distillation_data = torch.load(distillation_data_file)\n",
       "    22                                           \n",
       "    23         1    9222604.0 9222604.0      0.0      clients = [Client(ConvNet, lambda x : torch.optim.SGD(x, lr=0.1, momentum=0.9), dat, i, distillation_data) \n",
       "    24         1        451.0    451.0      0.0                 for i, dat in enumerate(client_data)]\n",
       "    25                                           \n",
       "    26         1        200.0    200.0      0.0      client_accs = []\n",
       "    27         1        110.0    110.0      0.0      global_accs = []\n",
       "    28                                           \n",
       "    29        50      15621.0    312.4      0.0      for c_round in range(1, COMMUNICATION_ROUNDS+1):\n",
       "    30                                           \n",
       "    31       500     235111.0    470.2      0.0          for client in clients:\n",
       "    32       500   32078435.0  64156.9      0.1              client.synchronize_with_server(server)\n",
       "    33       490     195175.0    398.3      0.0              if c_round == 1:\n",
       "    34        10 7224895485.0 722489548.5     22.0                  client.distill()            \n",
       "    35       500 1406733888.0 2813467.8      4.3              client.compute_weight_update(epochs=1) \n",
       "    36                                                   #print(c_round)\n",
       "    37                                                   \n",
       "    38        45      31412.0    698.0      0.0          if c_round % 10 == 0:\n",
       "    39         5   44467328.0 8893465.6      0.1              acc_clients = [client.evaluate() for client in clients]\n",
       "    40         5      58401.0  11680.2      0.0              client_acc = round(sum(acc_clients) / len(acc_clients), 3)\n",
       "    41         5       3557.0    711.4      0.0              client_accs.append(client_acc)\n",
       "    42                                           \n",
       "    43         5 24146932718.0 4829386543.6     73.5              accuracies = [server.evaluate_distil(client.model) for client in clients]\n",
       "    44         5    1150796.0 230159.2      0.0              global_acc = round(np.mean(accuracies), 3)\n",
       "    45         5       3646.0    729.2      0.0              global_accs.append(global_acc)\n",
       "    46                                               \n",
       "    47         1     200671.0 200671.0      0.0      print(f'total_client_data: {total_client_data}, data_per_class: {data_per_class}')\n",
       "    48         1      31940.0  31940.0      0.0      print(f'first acc: {client_accs[0]}, {global_accs[0]}')\n",
       "    49         1       8677.0   8677.0      0.0      print(f'last acc: {client_accs[-1]}, {global_accs[-1]}')\n",
       "    50         1        190.0    190.0      0.0      return client_accs, global_accs"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def do_epoch_experiments(total_client_data=total_client_data, data_per_class=data_per_class, ALPHA=ALPHA):\n",
    "    train_idcs, test_idcs = idcs[:int(total_client_data*10)], idcs[int(total_client_data*10):]\n",
    "    train_labels = data.targets\n",
    "    test_labels = data.targets[int(total_client_data*10):]\n",
    "\n",
    "\n",
    "    client_idcs = split_noniid(train_idcs, train_labels, alpha=ALPHA, n_clients=N_CLIENTS)\n",
    "    client_data = [CustomSubset(data, idcs) for idcs in client_idcs]\n",
    "    test_data = CustomSubset(data, test_idcs, transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "    for i, client_datum in enumerate(client_data):\n",
    "        client_datum.subset_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    server = Server(ConvNet, lambda x : torch.optim.SGD(x, lr=0.1, momentum=0.9),test_data)\n",
    "    \n",
    "    distillation_data_file = f'distill_data/MNIST/{data_per_class*10}.pth'\n",
    "    if not os.path.exists(distillation_data_file):\n",
    "        torch.save(server.make_distillation_data(data_per_class=data_per_class), distillation_data_file)\n",
    "        \n",
    "    distillation_data = torch.load(distillation_data_file)\n",
    "\n",
    "    clients = [Client(ConvNet, lambda x : torch.optim.SGD(x, lr=0.1, momentum=0.9), dat, i, distillation_data) \n",
    "               for i, dat in enumerate(client_data)]\n",
    "\n",
    "    client_accs = []\n",
    "    global_accs = []\n",
    "\n",
    "    for c_round in range(1, COMMUNICATION_ROUNDS+1):\n",
    "\n",
    "        for client in clients:\n",
    "            if c_round == 1:\n",
    "                client.synchronize_with_server(server)\n",
    "                client.distill()            \n",
    "            client.compute_weight_update(epochs=1) \n",
    "        #print(c_round)\n",
    "        \n",
    "        if c_round % 10 == 0:\n",
    "            acc_clients = [client.evaluate() for client in clients]\n",
    "            client_acc = round(sum(acc_clients) / len(acc_clients), 3)\n",
    "            client_accs.append(client_acc)\n",
    "\n",
    "            accuracies = [server.evaluate_distil(client.model) for client in clients]\n",
    "            global_acc = round(np.mean(accuracies), 3)\n",
    "            global_accs.append(global_acc)\n",
    "    \n",
    "    print(f'total_client_data: {total_client_data}, data_per_class: {data_per_class}')\n",
    "    print(f'first acc: {client_accs[0]}, {global_accs[0]}')\n",
    "    print(f'last acc: {client_accs[-1]}, {global_accs[-1]}')\n",
    "    return client_accs, global_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-12T12:37:57.635527Z",
     "iopub.status.idle": "2023-07-12T12:37:57.635907Z",
     "shell.execute_reply": "2023-07-12T12:37:57.635790Z",
     "shell.execute_reply.started": "2023-07-12T12:37:57.635778Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "total_client_data_values = [100, 200]\n",
    "data_per_class_values = [100, 200]\n",
    "\n",
    "# fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "# fig.suptitle('Client vs Global Accuracy over Communication Rounds')\n",
    "\n",
    "for i, total_client_data in enumerate(total_client_data_values):\n",
    "    for j, data_per_class in enumerate(data_per_class_values):\n",
    "        client_accs, global_accs = do_epoch_experiments(total_client_data=total_client_data, data_per_class=data_per_class, ALPHA=0.1)\n",
    "        \n",
    "        # Create x-axis: number of rounds\n",
    "#         rounds = range(1, COMMUNICATION_ROUNDS+1)\n",
    "\n",
    "#         axs[i, j].plot(rounds, client_accs, label='Client Accuracy')\n",
    "#         axs[i, j].plot(rounds, global_accs, label='Global Accuracy')\n",
    "        \n",
    "#         axs[i, j].set_xlabel('Communication Rounds')\n",
    "#         axs[i, j].set_ylabel('Accuracy')\n",
    "#         axs[i, j].legend()\n",
    "#         axs[i, j].set_title(f'total_client_data={total_client_data}, data_per_class={data_per_class}')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.subplots_adjust(top=0.90)  # Adjust the top space so the title doesn't overlap\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-12T12:37:57.636857Z",
     "iopub.status.idle": "2023-07-12T12:37:57.637302Z",
     "shell.execute_reply": "2023-07-12T12:37:57.637177Z",
     "shell.execute_reply.started": "2023-07-12T12:37:57.637160Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#client_acc_after_distill, global_acc_after_distill, client_acc_final, global_acc_final = do_experiments(total_client_data=total_client_data, data_per_class=data_per_class, ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-12T12:37:57.638125Z",
     "iopub.status.idle": "2023-07-12T12:37:57.638306Z",
     "shell.execute_reply": "2023-07-12T12:37:57.638221Z",
     "shell.execute_reply.started": "2023-07-12T12:37:57.638212Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the directory where you want to save the results\n",
    "save_dir = 'results/data_amount'\n",
    "os.makedirs(save_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "ALPHA = 1\n",
    "\n",
    "# Define the different total_client_data, data_per_class, and ALPHA values\n",
    "total_client_data_values = [100]\n",
    "data_per_class_values = [5, 10, 20]\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Loop over the total_client_data and data_per_class values\n",
    "for data_per_class in data_per_class_values:\n",
    "    print(f'data_per_class: {data_per_class}')\n",
    "    for total_client_data in total_client_data_values:\n",
    "            \n",
    "        temp_results = []\n",
    "\n",
    "        # Repeat the experiment twice\n",
    "        for _ in range(1):\n",
    "            # Call the function and save the results\n",
    "            client_acc_after_distill, global_acc_after_distill, client_acc_final, global_acc_final = do_experiments(\n",
    "                total_client_data=total_client_data, \n",
    "                data_per_class=data_per_class, \n",
    "                ALPHA=ALPHA\n",
    "            )\n",
    "\n",
    "            # Store the results in a temporary list\n",
    "            temp_results.append({\n",
    "                'client_acc_after_distill': client_acc_after_distill, \n",
    "                'global_acc_after_distill': global_acc_after_distill, \n",
    "                'client_acc_final': client_acc_final, \n",
    "                'global_acc_final': global_acc_final\n",
    "            })\n",
    "\n",
    "        # Calculate the average of the results\n",
    "        avg_results = {\n",
    "            key: np.mean([res[key] for res in temp_results]) \n",
    "            for key in temp_results[0]\n",
    "        }\n",
    "\n",
    "        # Save the average results in the results dictionary\n",
    "        results[(total_client_data, data_per_class, ALPHA)] = avg_results\n",
    "\n",
    "    # Save the results dictionary to disk after each data_per_class\n",
    "    with open(os.path.join(save_dir, f'0420_epoch.pkl'), 'wb') as f:\n",
    "        pickle.dump(results, f)## 1) test 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-12T12:37:57.639542Z",
     "iopub.status.idle": "2023-07-12T12:37:57.640034Z",
     "shell.execute_reply": "2023-07-12T12:37:57.639913Z",
     "shell.execute_reply.started": "2023-07-12T12:37:57.639898Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pprint\n",
    "\n",
    "# Define the directory where the results are saved\n",
    "save_dir = 'results/data_amount'\n",
    "\n",
    "# Define the different data_per_class values\n",
    "data_per_class_values = [10, 20, 50, 100, 200, 500]\n",
    "\n",
    "# For pretty printing\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "\n",
    "#0323: client 10, 50 결과 저장\n",
    "with open(os.path.join(save_dir, f'0417_ALPHA_0.1.pkl'), 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "pp.pprint(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-12T12:37:57.641077Z",
     "iopub.status.idle": "2023-07-12T12:37:57.641305Z",
     "shell.execute_reply": "2023-07-12T12:37:57.641193Z",
     "shell.execute_reply.started": "2023-07-12T12:37:57.641182Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the different data_per_class values\n",
    "total_client_data_values = [50, 100, 200, 500, 1000, 2000, 5000]\n",
    "data_per_class_values = [10, 20, 50, 100, 200, 500]\n",
    "\n",
    "# Metrics\n",
    "metrics = ['global_acc_after_distill', 'global_acc_final']\n",
    "\n",
    "# Prepare list to store the rows\n",
    "data = []\n",
    "\n",
    "# Fill in the rows\n",
    "for data_per_class in data_per_class_values:\n",
    "    with open(os.path.join(save_dir, f'0412_ALPHA_0.1.pkl'), 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "\n",
    "    for total_client_data in total_client_data_values:\n",
    "        for metric in metrics:\n",
    "            row = {\"total_client_data\": total_client_data, \"metric\": metric, \"data_per_class\": data_per_class, \"value\": round(results[(total_client_data, data_per_class, 0.1)][metric], 2)}\n",
    "            data.append(row)\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Pivot the DataFrame to the desired shape\n",
    "df = df.pivot(index=['total_client_data', 'metric'], columns='data_per_class', values='value')\n",
    "\n",
    "# Filter for 'client_acc_after_distill'\n",
    "df_after_distill = df.xs('global_acc_after_distill', level='metric')\n",
    "df_final = df.xs('global_acc_final', level='metric')\n",
    "\n",
    "# Plot heatmap\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20,8))  # Create a figure and a set of subplots\n",
    "fig.suptitle('Heatmaps for client_acc_after_distill and client_acc_final')\n",
    "\n",
    "sns.heatmap(df_after_distill, annot=True, cmap=\"YlGnBu\", ax=axs[0])\n",
    "axs[0].set_title('global_acc_after_distill Heatmap')\n",
    "\n",
    "sns.heatmap(df_final, annot=True, cmap=\"YlGnBu\", ax=axs[1])\n",
    "axs[1].set_title('global_acc_final Heatmap')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 서버, client data 양 2차 실험"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Clustering 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-12T12:37:57.642119Z",
     "iopub.status.idle": "2023-07-12T12:37:57.642297Z",
     "shell.execute_reply": "2023-07-12T12:37:57.642217Z",
     "shell.execute_reply.started": "2023-07-12T12:37:57.642208Z"
    }
   },
   "outputs": [],
   "source": [
    "def do_clustering_experiments(total_client_data=total_client_data, data_per_class=data_per_class, ALPHA=ALPHA):\n",
    "    train_idcs, test_idcs = idcs[:int(total_client_data*10)], idcs[int(total_client_data*10):]\n",
    "    train_labels = data.train_labels.numpy()\n",
    "    test_labels = data.train_labels.numpy()[int(total_client_data*10):]\n",
    "\n",
    "    client_idcs = split_noniid(train_idcs, train_labels, alpha=ALPHA, n_clients=N_CLIENTS)#, data_per_class=int(total_client_data/10))\n",
    "    # server_idcs = generate_server_idcs(test_idcs, test_labels, int(total_client_data*10))\n",
    "\n",
    "    client_data = [CustomSubset(data, idcs) for idcs in client_idcs]\n",
    "    test_data = CustomSubset(data, test_idcs, transforms.Compose([transforms.ToTensor()]))\n",
    "    \n",
    "    for i, client_datum in enumerate(client_data):\n",
    "        client_datum.subset_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    server = Server(ConvNet, lambda x : torch.optim.SGD(x, lr=0.1, momentum=0.9),test_data)\n",
    "\n",
    "    \n",
    "    distillation_data_file = f'distillation_data_{data_per_class}_per_class.pth'\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if not os.path.exists(distillation_data_file):\n",
    "        # The file does not exist, generate and save the distillation data\n",
    "        distillation_data = server.make_distillation_data(data_per_class=data_per_class)\n",
    "        torch.save(distillation_data, distillation_data_file)\n",
    "\n",
    "    # Load the distillation data\n",
    "    distillation_data = torch.load(distillation_data_file)\n",
    "\n",
    "    clients = [Client(ConvNet, lambda x : torch.optim.SGD(x, lr=0.1, momentum=0.9), dat, i, distillation_data) \n",
    "               for i, dat in enumerate(client_data)]\n",
    "\n",
    "    def aggregate(cluster_indices_new):\n",
    "        cluster_indices = cluster_indices_new\n",
    "        client_clusters = [[clients[i] for i in idcs] for idcs in cluster_indices]\n",
    "\n",
    "        server.aggregate_clusterwise(client_clusters)\n",
    "\n",
    "        return cluster_indices\n",
    "\n",
    "    cfl_stats = ExperimentLogger()\n",
    "\n",
    "    cluster_indices = [np.arange(len(clients)).astype(\"int\")]\n",
    "    client_clusters = [[clients[i] for i in idcs] for idcs in cluster_indices]\n",
    "\n",
    "\n",
    "    for c_round in range(1, COMMUNICATION_ROUNDS+1):\n",
    "\n",
    "        if c_round == 1:\n",
    "            for client in clients:\n",
    "                client.synchronize_with_server(server)\n",
    "\n",
    "        participating_clients = server.select_clients(clients, frac=1.0)\n",
    "\n",
    "        for client in participating_clients:\n",
    "            if c_round == 1:\n",
    "                client.distill()\n",
    "\n",
    "            train_stats = client.compute_weight_update(epochs=1) #train client\n",
    "\n",
    "            if c_round == 1000:\n",
    "                client.reset()\n",
    "\n",
    "        cluster_indices_new = []\n",
    "\n",
    "        for idc in cluster_indices:\n",
    "            max_norm = server.compute_max_update_norm([clients[i] for i in idc])\n",
    "            mean_norm = server.compute_mean_update_norm([clients[i] for i in idc])\n",
    "\n",
    "            #cluster 나누는 기준\n",
    "            if c_round == COMMUNICATION_ROUNDS: #무조건 한번 나누기\n",
    "                similarities = server.compute_pairwise_similarities(clients)\n",
    "\n",
    "                server.cache_model(idc, clients[idc[0]].W, acc_clients)\n",
    "\n",
    "                c1, c2, c3 = server.cluster_clients_GMM(similarities[idc][:,idc])\n",
    "                cluster_indices_new += [c1, c2, c3]\n",
    "\n",
    "        if c_round == 1000:\n",
    "            cluster_indices = aggregate(cluster_indices_new)\n",
    "\n",
    "        acc_clients = [client.evaluate() for client in clients]\n",
    "\n",
    "        if c_round == COMMUNICATION_ROUNDS: #무조건 한번 나누기\n",
    "            label_accuracies = pd.DataFrame()\n",
    "            label_predicted = pd.DataFrame()\n",
    "            label_soft_sum = pd.DataFrame()\n",
    "            label_diff = pd.DataFrame()\n",
    "\n",
    "            for i, client in enumerate(clients):\n",
    "                acc, pred, sum_, diff = server.evaluate(client.model)\n",
    "                # Convert each dictionary to a DataFrame and append to the respective DataFrame\n",
    "                label_accuracies = label_accuracies.append(pd.DataFrame(acc, index=[i]))\n",
    "                label_predicted = label_predicted.append(pd.DataFrame(pred, index=[i]))\n",
    "                label_soft_sum = label_soft_sum.append(pd.DataFrame(sum_, index=[i]))\n",
    "                label_diff = label_diff.append(pd.DataFrame(diff, index=[i]))\n",
    "\n",
    "            # Reset index for all DataFrames\n",
    "            label_accuracies.reset_index(drop=True, inplace=True)\n",
    "            label_predicted.reset_index(drop=True, inplace=True)\n",
    "            label_soft_sum.reset_index(drop=True, inplace=True)\n",
    "            label_diff.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        if c_round == 1:\n",
    "            first_accuracies = pd.DataFrame()\n",
    "            for i, client in enumerate(clients):\n",
    "                first_acc, pred, sum_, diff = server.evaluate(client.model)\n",
    "                first_accuracies = pd.concat([first_accuracies, pd.DataFrame(first_acc, index=[i])])\n",
    "            first_accuracies = first_accuracies.fillna(0)\n",
    "\n",
    "            client_acc_after_distill = sum(acc_clients)/len(acc_clients)\n",
    "            global_acc_after_distill = np.mean(np.ravel(first_accuracies.values))\n",
    "\n",
    "\n",
    "        elif c_round == COMMUNICATION_ROUNDS:\n",
    "            client_acc_final = sum(acc_clients)/len(acc_clients)\n",
    "            global_acc_final = np.mean(np.ravel(label_accuracies.values))\n",
    "\n",
    "        average_dw = server.get_average_dw(clients)\n",
    "        #print(average_dw)\n",
    "        cfl_stats.log({\"acc_clients\" : acc_clients, \"mean_norm\" : mean_norm, \"max_norm\" : max_norm,\n",
    "                      \"rounds\" : c_round, \"clusters\" : cluster_indices, \"average_dw\": average_dw})\n",
    "\n",
    "\n",
    "        display_train_stats(cfl_stats, EPS_1, EPS_2, COMMUNICATION_ROUNDS)\n",
    "\n",
    "\n",
    "    for idc in cluster_indices:    \n",
    "        server.cache_model(idc, clients[idc[0]].W, acc_clients)\n",
    "    \n",
    "    client_acc_after_distill = round(client_acc_after_distill, 3)\n",
    "    global_acc_after_distill = round(global_acc_after_distill, 3)\n",
    "    client_acc_final = round(client_acc_final, 3)\n",
    "    global_acc_final = round(global_acc_final, 3)\n",
    "    \n",
    "    return client_acc_after_distill, global_acc_after_distill, client_acc_final, global_acc_final\n",
    "\n",
    "    print(client_acc_after_distill, global_acc_after_distill)\n",
    "    print(client_acc_final, global_acc_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-12T12:37:57.643551Z",
     "iopub.status.idle": "2023-07-12T12:37:57.644192Z",
     "shell.execute_reply": "2023-07-12T12:37:57.644067Z",
     "shell.execute_reply.started": "2023-07-12T12:37:57.644052Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-12T12:37:57.644891Z",
     "iopub.status.idle": "2023-07-12T12:37:57.645439Z",
     "shell.execute_reply": "2023-07-12T12:37:57.645317Z",
     "shell.execute_reply.started": "2023-07-12T12:37:57.645303Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_accuracies.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-12T12:37:57.646244Z",
     "iopub.status.idle": "2023-07-12T12:37:57.646431Z",
     "shell.execute_reply": "2023-07-12T12:37:57.646342Z",
     "shell.execute_reply.started": "2023-07-12T12:37:57.646333Z"
    }
   },
   "outputs": [],
   "source": [
    "label_soft_sum.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-12T12:37:57.647532Z",
     "iopub.status.idle": "2023-07-12T12:37:57.647728Z",
     "shell.execute_reply": "2023-07-12T12:37:57.647636Z",
     "shell.execute_reply.started": "2023-07-12T12:37:57.647628Z"
    }
   },
   "outputs": [],
   "source": [
    "label_diff.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-12T12:37:57.648600Z",
     "iopub.status.idle": "2023-07-12T12:37:57.648769Z",
     "shell.execute_reply": "2023-07-12T12:37:57.648690Z",
     "shell.execute_reply.started": "2023-07-12T12:37:57.648682Z"
    }
   },
   "outputs": [],
   "source": [
    "label_predicted.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-12T12:37:57.649545Z",
     "iopub.status.idle": "2023-07-12T12:37:57.649713Z",
     "shell.execute_reply": "2023-07-12T12:37:57.649634Z",
     "shell.execute_reply.started": "2023-07-12T12:37:57.649626Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Instantiate PCA\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Apply PCA to the dataframes\n",
    "label_accuracies_pca = pca.fit_transform(label_accuracies)\n",
    "label_predicted_pca = pca.fit_transform(label_predicted)\n",
    "label_soft_sum_pca = pca.fit_transform(label_soft_sum)\n",
    "label_diff_pca = pca.fit_transform(label_diff)\n",
    "transformed_data = pca.fit_transform(similarities)\n",
    "\n",
    "# Create labels\n",
    "labels = [0, 0, 0, 1, 1, 1, 2, 2, 2]\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "# Scatter plots with larger dots\n",
    "dot_size = 50\n",
    "axs[0, 0].scatter(label_accuracies_pca[:, 0], label_accuracies_pca[:, 1], c=labels, s=dot_size)\n",
    "axs[0, 0].set_title('Label Accuracies')\n",
    "axs[0, 1].scatter(label_predicted_pca[:, 0], label_predicted_pca[:, 1], c=labels, s=dot_size)\n",
    "axs[0, 1].set_title('Label Predicted')\n",
    "axs[1, 0].scatter(label_soft_sum_pca[:, 0], label_soft_sum_pca[:, 1], c=labels, s=dot_size)\n",
    "axs[1, 0].set_title('Label Soft Sum')\n",
    "axs[1, 1].scatter(label_diff_pca[:, 0], label_diff_pca[:, 1], c=labels, s=dot_size)\n",
    "axs[1, 1].set_title('Label Soft Diff')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-12T12:37:57.650753Z",
     "iopub.status.idle": "2023-07-12T12:37:57.650938Z",
     "shell.execute_reply": "2023-07-12T12:37:57.650847Z",
     "shell.execute_reply.started": "2023-07-12T12:37:57.650839Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Calculate Silhouette Scores\n",
    "silhouette_accuracies = silhouette_score(label_accuracies_pca, labels)\n",
    "silhouette_predicted = silhouette_score(label_predicted_pca, labels)\n",
    "silhouette_soft_sum = silhouette_score(label_soft_sum_pca, labels)\n",
    "silhouette_diff = silhouette_score(label_diff_pca, labels)\n",
    "silhouette_transformed_data = silhouette_score(transformed_data, labels)\n",
    "\n",
    "print('Silhouette Score for Accuracies:', silhouette_accuracies)\n",
    "print('Silhouette Score for Predicted:', silhouette_predicted)\n",
    "print('Silhouette Score for Soft Sum:', silhouette_soft_sum)\n",
    "print('Silhouette Score for diff:', silhouette_diff)\n",
    "print('Silhouette Score for Model params:', silhouette_transformed_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-12T12:37:57.652593Z",
     "iopub.status.idle": "2023-07-12T12:37:57.652854Z",
     "shell.execute_reply": "2023-07-12T12:37:57.652722Z",
     "shell.execute_reply.started": "2023-07-12T12:37:57.652710Z"
    }
   },
   "outputs": [],
   "source": [
    "#df.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 Cluster 별 모델 파라미터 분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-12T12:37:57.653896Z",
     "iopub.status.idle": "2023-07-12T12:37:57.654227Z",
     "shell.execute_reply": "2023-07-12T12:37:57.654073Z",
     "shell.execute_reply.started": "2023-07-12T12:37:57.654056Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Fit and transform your data to 2D\n",
    "pca = PCA(n_components=2)\n",
    "transformed_data = pca.fit_transform(similarities)\n",
    "\n",
    "# Assign labels based on index ranges\n",
    "labels = [0, 0, 0, 1, 1, 1, 2, 2, 2]\n",
    "unique_labels = np.unique(labels)\n",
    "colors = plt.cm.Spectral(np.linspace(0, 0.35, len(unique_labels)))\n",
    "\n",
    "# Plot the transformed data with labels\n",
    "for label, color in zip(unique_labels, colors):\n",
    "    idx = np.where(labels == label)\n",
    "    plt.scatter(transformed_data[idx, 0], transformed_data[idx, 1], color=color, label=f'Cluster {label}')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.12.1-py3.8-cuda11.3",
   "language": "python",
   "name": "torch1.12.1-py3.8-cuda11.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "f79394e62bebc70f4ea6374f6a04753660b8235adfdaf8a6dfe67d7c0f65c745"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

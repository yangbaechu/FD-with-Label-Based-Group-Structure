{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-13T03:39:48.179340Z",
     "iopub.status.busy": "2023-07-13T03:39:48.179139Z",
     "iopub.status.idle": "2023-07-13T03:39:49.719210Z",
     "shell.execute_reply": "2023-07-13T03:39:49.718131Z",
     "shell.execute_reply.started": "2023-07-13T03:39:48.179317Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models import ConvNet\n",
    "from helper import ExperimentLogger, display_train_stats\n",
    "from fl_devices import Server, Client\n",
    "from data_utils import split_not_contain_every_class, split_contain_every_class, generate_server_idcs, CustomSubset, split_noniid\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-13T03:39:49.720625Z",
     "iopub.status.busy": "2023-07-13T03:39:49.720320Z",
     "iopub.status.idle": "2023-07-13T03:39:49.724452Z",
     "shell.execute_reply": "2023-07-13T03:39:49.723779Z",
     "shell.execute_reply.started": "2023-07-13T03:39:49.720607Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "COMMUNICATION_ROUNDS = 50\n",
    "EPS_1 = 0.4\n",
    "EPS_2 = 1.6\n",
    "N_CLIENTS = 10\n",
    "\n",
    "total_client_data = 1000\n",
    "data_per_class = 50\n",
    "ALPHA = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-13T03:39:49.725418Z",
     "iopub.status.busy": "2023-07-13T03:39:49.725281Z",
     "iopub.status.idle": "2023-07-13T03:39:49.782164Z",
     "shell.execute_reply": "2023-07-13T03:39:49.781722Z",
     "shell.execute_reply.started": "2023-07-13T03:39:49.725405Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "data = datasets.MNIST(root=\"MNIST/\", download=False)\n",
    "\n",
    "mapp = np.array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype='<U1')\n",
    "\n",
    "idcs = np.random.permutation(len(data))\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-13T03:39:49.782921Z",
     "iopub.status.busy": "2023-07-13T03:39:49.782759Z",
     "iopub.status.idle": "2023-07-13T03:39:49.791107Z",
     "shell.execute_reply": "2023-07-13T03:39:49.790656Z",
     "shell.execute_reply.started": "2023-07-13T03:39:49.782907Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def do_experiments(total_client_data=total_client_data, data_per_class=data_per_class, ALPHA=ALPHA):\n",
    "    train_idcs, test_idcs = idcs[:int(total_client_data*10)], idcs[int(total_client_data*10):]\n",
    "    train_labels = data.targets\n",
    "    test_labels = data.targets[int(total_client_data*10):]\n",
    "\n",
    "\n",
    "    client_idcs = split_noniid(train_idcs, train_labels, alpha=ALPHA, n_clients=N_CLIENTS)\n",
    "    client_data = [CustomSubset(data, idcs) for idcs in client_idcs]\n",
    "    test_data = CustomSubset(data, test_idcs, transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "    for i, client_datum in enumerate(client_data):\n",
    "        client_datum.subset_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    server = Server(ConvNet, lambda x : torch.optim.SGD(x, lr=0.1, momentum=0.9),test_data)\n",
    "     \n",
    "    # save a model for size comparison\n",
    "    torch.save(server.model.state_dict(), 'model.pth')\n",
    "    \n",
    "    distillation_data_file = f'distillation_data_{data_per_class}_per_class.pth'\n",
    "    if not os.path.exists(distillation_data_file):\n",
    "        torch.save(server.make_distillation_data(data_per_class=data_per_class), distillation_data_file)\n",
    "        \n",
    "    distillation_data = torch.load(distillation_data_file)\n",
    "\n",
    "    clients = [Client(ConvNet, lambda x : torch.optim.SGD(x, lr=0.1, momentum=0.9), dat, i, distillation_data) \n",
    "               for i, dat in enumerate(client_data)]\n",
    "\n",
    "    cfl_stats = ExperimentLogger()\n",
    "\n",
    "    for c_round in range(1, COMMUNICATION_ROUNDS+1):\n",
    "        if c_round == 1:\n",
    "            for client in clients:\n",
    "                client.synchronize_with_server(server)\n",
    "                client.distill()\n",
    "                \n",
    "            first_acc_clients = [client.evaluate() for client in clients]\n",
    "            accuracies = [server.evaluate_distil(client.model) for client in clients]\n",
    "\n",
    "        server.select_clients(clients, frac=1.0)\n",
    "\n",
    "        for client in clients:\n",
    "            client.compute_weight_update(epochs=1) #train client\n",
    "\n",
    "        acc_clients = [client.evaluate() for client in clients]\n",
    "\n",
    "    label_accuracies = pd.DataFrame(server.evaluate(client.model)[0] for client in clients)\n",
    "\n",
    "    client_acc_after_distill = round(sum(first_acc_clients) / len(first_acc_clients), 3)\n",
    "    \n",
    "    global_acc_after_distill = round(np.mean(accuracies), 3)\n",
    "    #global_acc_after_distill = round(np.mean(np.ravel(pd.DataFrame(server.evaluate(client.model)[0] for client in clients).values)), 3)\n",
    "\n",
    "    client_acc_final = round(sum(acc_clients) / len(acc_clients), 3)\n",
    "    global_acc_final = round(np.mean(np.ravel(label_accuracies.values)), 3)\n",
    "    print(client_acc_after_distill, global_acc_after_distill, client_acc_final, global_acc_final)\n",
    "    return client_acc_after_distill, global_acc_after_distill, client_acc_final, global_acc_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-13T03:39:49.791779Z",
     "iopub.status.busy": "2023-07-13T03:39:49.791627Z",
     "iopub.status.idle": "2023-07-13T03:39:49.797749Z",
     "shell.execute_reply": "2023-07-13T03:39:49.797385Z",
     "shell.execute_reply.started": "2023-07-13T03:39:49.791766Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def do_epoch_experiments(total_client_data=total_client_data, data_per_class=data_per_class, ALPHA=ALPHA):\n",
    "    train_idcs, test_idcs = idcs[:int(total_client_data*10)], idcs[int(total_client_data*10):]\n",
    "    train_labels = data.targets\n",
    "    test_labels = data.targets[int(total_client_data*10):]\n",
    "\n",
    "\n",
    "    client_idcs = split_noniid(train_idcs, train_labels, alpha=ALPHA, n_clients=N_CLIENTS)\n",
    "    client_data = [CustomSubset(data, idcs) for idcs in client_idcs]\n",
    "    test_data = CustomSubset(data, test_idcs, transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "    for i, client_datum in enumerate(client_data):\n",
    "        client_datum.subset_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    server = Server(ConvNet, lambda x : torch.optim.SGD(x, lr=0.1, momentum=0.9),test_data)\n",
    "    \n",
    "    distillation_data_file = f'distill_data/MNIST/{data_per_class*10}.pth'\n",
    "    if not os.path.exists(distillation_data_file):\n",
    "        torch.save(server.make_distillation_data(data_per_class=data_per_class), distillation_data_file)\n",
    "        \n",
    "    distillation_data = torch.load(distillation_data_file)\n",
    "\n",
    "    clients = [Client(ConvNet, lambda x: torch.optim.SGD(x, lr=0.1, momentum=0.9), dat, i, distillation_data) \n",
    "               for i, dat in enumerate(client_data)]\n",
    "\n",
    "    client_accs = []\n",
    "    global_accs = []\n",
    "\n",
    "    for c_round in range(COMMUNICATION_ROUNDS):\n",
    "\n",
    "        for client in clients:\n",
    "            # # if c_round == 0:\n",
    "            # #     client.synchronize_with_server(server)\n",
    "            # #     client.distill()\n",
    "            if c_round != 0:\n",
    "                client.compute_weight_update(epochs=1)\n",
    "\n",
    "        if c_round % 10 == 0:\n",
    "            acc_clients = [client.evaluate() for client in clients]\n",
    "            client_acc = round(sum(acc_clients) / len(acc_clients), 3)\n",
    "            client_accs.append(client_acc)\n",
    "\n",
    "            accuracies = [server.evaluate_distil(client.model) for client in clients]\n",
    "            global_acc = round(np.mean(accuracies), 3)\n",
    "            global_accs.append(global_acc)\n",
    "\n",
    "    print(f'total_client_data: {total_client_data}, data_per_class: {data_per_class}')\n",
    "    print(f'first acc: {client_accs[0]}, {global_accs[0]}')\n",
    "    print(f'last acc: {client_accs[-1]}, {global_accs[-1]}')\n",
    "    return client_accs, global_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-13T03:39:49.798371Z",
     "iopub.status.busy": "2023-07-13T03:39:49.798226Z",
     "iopub.status.idle": "2023-07-13T03:41:47.117116Z",
     "shell.execute_reply": "2023-07-13T03:41:47.115415Z",
     "shell.execute_reply.started": "2023-07-13T03:39:49.798358Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_client_data: 50, data_per_class: 100\n",
      "first acc: 0.05, 0.018\n",
      "last acc: 0.55, 0.124\n",
      "total_client_data: 100, data_per_class: 100\n",
      "first acc: 0.0, 0.032\n",
      "last acc: 0.567, 0.177\n",
      "total_client_data: 200, data_per_class: 100\n",
      "first acc: 0.017, 0.008\n",
      "last acc: 0.733, 0.204\n",
      "total_client_data: 500, data_per_class: 100\n",
      "first acc: 0.047, 0.017\n",
      "last acc: 0.747, 0.217\n",
      "total_client_data: 1000, data_per_class: 100\n",
      "first acc: 0.0, 0.003\n",
      "last acc: 0.687, 0.201\n",
      "total_client_data: 2000, data_per_class: 100\n",
      "first acc: 0.028, 0.019\n",
      "last acc: 0.632, 0.147\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "client_data_values = [5, 10, 20, 50, 100, 200]\n",
    "distill_data_values = [1000]\n",
    "\n",
    "# fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "# fig.suptitle('Client vs Global Accuracy over Communication Rounds')\n",
    "\n",
    "for i, client_data in enumerate(client_data_values):\n",
    "    for j, distill_data in enumerate(distill_data_values):\n",
    "        client_accs, global_accs = do_epoch_experiments(total_client_data=client_data*10, data_per_class=int(distill_data//10), ALPHA=0.1)\n",
    "        \n",
    "        # Create x-axis: number of rounds\n",
    "#         rounds = range(1, COMMUNICATION_ROUNDS+1)\n",
    "\n",
    "#         axs[i, j].plot(rounds, client_accs, label='Client Accuracy')\n",
    "#         axs[i, j].plot(rounds, global_accs, label='Global Accuracy')\n",
    "        \n",
    "#         axs[i, j].set_xlabel('Communication Rounds')\n",
    "#         axs[i, j].set_ylabel('Accuracy')\n",
    "#         axs[i, j].legend()\n",
    "#         axs[i, j].set_title(f'total_client_data={total_client_data}, data_per_class={data_per_class}')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.subplots_adjust(top=0.90)  # Adjust the top space so the title doesn't overlap\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-13T03:41:47.121809Z",
     "iopub.status.busy": "2023-07-13T03:41:47.121456Z",
     "iopub.status.idle": "2023-07-13T03:41:47.127592Z",
     "shell.execute_reply": "2023-07-13T03:41:47.126533Z",
     "shell.execute_reply.started": "2023-07-13T03:41:47.121749Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#client_acc_after_distill, global_acc_after_distill, client_acc_final, global_acc_final = do_experiments(total_client_data=total_client_data, data_per_class=data_per_class, ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-13T03:41:47.128849Z",
     "iopub.status.busy": "2023-07-13T03:41:47.128633Z",
     "iopub.status.idle": "2023-07-13T03:44:16.224039Z",
     "shell.execute_reply": "2023-07-13T03:44:16.222973Z",
     "shell.execute_reply.started": "2023-07-13T03:41:47.128828Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_per_class: 5\n",
      "0.667 0.682 0.5 0.276\n",
      "data_per_class: 10\n",
      "0.933 0.79 0.667 0.256\n",
      "data_per_class: 20\n",
      "0.933 0.874 0.633 0.291\n"
     ]
    }
   ],
   "source": [
    "# Define the directory where you want to save the results\n",
    "save_dir = 'results/data_amount'\n",
    "os.makedirs(save_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "ALPHA = 1\n",
    "\n",
    "# Define the different total_client_data, data_per_class, and ALPHA values\n",
    "total_client_data_values = [100]\n",
    "data_per_class_values = [5, 10, 20]\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Loop over the total_client_data and data_per_class values\n",
    "for data_per_class in data_per_class_values:\n",
    "    print(f'data_per_class: {data_per_class}')\n",
    "    for total_client_data in total_client_data_values:\n",
    "            \n",
    "        temp_results = []\n",
    "\n",
    "        # Repeat the experiment twice\n",
    "        for _ in range(1):\n",
    "            # Call the function and save the results\n",
    "            client_acc_after_distill, global_acc_after_distill, client_acc_final, global_acc_final = do_experiments(\n",
    "                total_client_data=total_client_data, \n",
    "                data_per_class=data_per_class, \n",
    "                ALPHA=ALPHA\n",
    "            )\n",
    "\n",
    "            # Store the results in a temporary list\n",
    "            temp_results.append({\n",
    "                'client_acc_after_distill': client_acc_after_distill, \n",
    "                'global_acc_after_distill': global_acc_after_distill, \n",
    "                'client_acc_final': client_acc_final, \n",
    "                'global_acc_final': global_acc_final\n",
    "            })\n",
    "\n",
    "        # Calculate the average of the results\n",
    "        avg_results = {\n",
    "            key: np.mean([res[key] for res in temp_results]) \n",
    "            for key in temp_results[0]\n",
    "        }\n",
    "\n",
    "        # Save the average results in the results dictionary\n",
    "        results[(total_client_data, data_per_class, ALPHA)] = avg_results\n",
    "\n",
    "    # Save the results dictionary to disk after each data_per_class\n",
    "    with open(os.path.join(save_dir, f'0420_epoch.pkl'), 'wb') as f:\n",
    "        pickle.dump(results, f)## 1) test 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-13T03:44:16.225711Z",
     "iopub.status.busy": "2023-07-13T03:44:16.225340Z",
     "iopub.status.idle": "2023-07-13T03:44:16.508104Z",
     "shell.execute_reply": "2023-07-13T03:44:16.506757Z",
     "shell.execute_reply.started": "2023-07-13T03:44:16.225690Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/data_amount/0417_ALPHA_0.1.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m pp \u001b[38;5;241m=\u001b[39m pprint\u001b[38;5;241m.\u001b[39mPrettyPrinter(indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#0323: client 10, 50 결과 저장\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m0417_ALPHA_0.1.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     17\u001b[0m     results \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     19\u001b[0m pp\u001b[38;5;241m.\u001b[39mpprint(results)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/data_amount/0417_ALPHA_0.1.pkl'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pprint\n",
    "\n",
    "# Define the directory where the results are saved\n",
    "save_dir = 'results/data_amount'\n",
    "\n",
    "# Define the different data_per_class values\n",
    "data_per_class_values = [10, 20, 50, 100, 200, 500]\n",
    "\n",
    "# For pretty printing\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "\n",
    "#0323: client 10, 50 결과 저장\n",
    "with open(os.path.join(save_dir, f'0417_ALPHA_0.1.pkl'), 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "pp.pprint(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-13T03:44:16.508742Z",
     "iopub.status.idle": "2023-07-13T03:44:16.508938Z",
     "shell.execute_reply": "2023-07-13T03:44:16.508847Z",
     "shell.execute_reply.started": "2023-07-13T03:44:16.508837Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the different data_per_class values\n",
    "total_client_data_values = [50, 100, 200, 500, 1000, 2000, 5000]\n",
    "data_per_class_values = [10, 20, 50, 100, 200, 500]\n",
    "\n",
    "# Metrics\n",
    "metrics = ['global_acc_after_distill', 'global_acc_final']\n",
    "\n",
    "# Prepare list to store the rows\n",
    "data = []\n",
    "\n",
    "# Fill in the rows\n",
    "for data_per_class in data_per_class_values:\n",
    "    with open(os.path.join(save_dir, f'0412_ALPHA_0.1.pkl'), 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "\n",
    "    for total_client_data in total_client_data_values:\n",
    "        for metric in metrics:\n",
    "            row = {\"total_client_data\": total_client_data, \"metric\": metric, \"data_per_class\": data_per_class, \"value\": round(results[(total_client_data, data_per_class, 0.1)][metric], 2)}\n",
    "            data.append(row)\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Pivot the DataFrame to the desired shape\n",
    "df = df.pivot(index=['total_client_data', 'metric'], columns='data_per_class', values='value')\n",
    "\n",
    "# Filter for 'client_acc_after_distill'\n",
    "df_after_distill = df.xs('global_acc_after_distill', level='metric')\n",
    "df_final = df.xs('global_acc_final', level='metric')\n",
    "\n",
    "# Plot heatmap\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20,8))  # Create a figure and a set of subplots\n",
    "fig.suptitle('Heatmaps for client_acc_after_distill and client_acc_final')\n",
    "\n",
    "sns.heatmap(df_after_distill, annot=True, cmap=\"YlGnBu\", ax=axs[0])\n",
    "axs[0].set_title('global_acc_after_distill Heatmap')\n",
    "\n",
    "sns.heatmap(df_final, annot=True, cmap=\"YlGnBu\", ax=axs[1])\n",
    "axs[1].set_title('global_acc_final Heatmap')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 서버, client data 양 2차 실험"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Clustering 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-13T03:44:16.509969Z",
     "iopub.status.idle": "2023-07-13T03:44:16.510156Z",
     "shell.execute_reply": "2023-07-13T03:44:16.510066Z",
     "shell.execute_reply.started": "2023-07-13T03:44:16.510057Z"
    }
   },
   "outputs": [],
   "source": [
    "def do_clustering_experiments(total_client_data=total_client_data, data_per_class=data_per_class, ALPHA=ALPHA):\n",
    "    train_idcs, test_idcs = idcs[:int(total_client_data*10)], idcs[int(total_client_data*10):]\n",
    "    train_labels = data.train_labels.numpy()\n",
    "    test_labels = data.train_labels.numpy()[int(total_client_data*10):]\n",
    "\n",
    "    client_idcs = split_noniid(train_idcs, train_labels, alpha=ALPHA, n_clients=N_CLIENTS)#, data_per_class=int(total_client_data/10))\n",
    "    # server_idcs = generate_server_idcs(test_idcs, test_labels, int(total_client_data*10))\n",
    "\n",
    "    client_data = [CustomSubset(data, idcs) for idcs in client_idcs]\n",
    "    test_data = CustomSubset(data, test_idcs, transforms.Compose([transforms.ToTensor()]))\n",
    "    \n",
    "    for i, client_datum in enumerate(client_data):\n",
    "        client_datum.subset_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    server = Server(ConvNet, lambda x : torch.optim.SGD(x, lr=0.1, momentum=0.9),test_data)\n",
    "\n",
    "    \n",
    "    distillation_data_file = f'distillation_data_{data_per_class}_per_class.pth'\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if not os.path.exists(distillation_data_file):\n",
    "        # The file does not exist, generate and save the distillation data\n",
    "        distillation_data = server.make_distillation_data(data_per_class=data_per_class)\n",
    "        torch.save(distillation_data, distillation_data_file)\n",
    "\n",
    "    # Load the distillation data\n",
    "    distillation_data = torch.load(distillation_data_file)\n",
    "\n",
    "    clients = [Client(ConvNet, lambda x : torch.optim.SGD(x, lr=0.1, momentum=0.9), dat, i, distillation_data) \n",
    "               for i, dat in enumerate(client_data)]\n",
    "\n",
    "    def aggregate(cluster_indices_new):\n",
    "        cluster_indices = cluster_indices_new\n",
    "        client_clusters = [[clients[i] for i in idcs] for idcs in cluster_indices]\n",
    "\n",
    "        server.aggregate_clusterwise(client_clusters)\n",
    "\n",
    "        return cluster_indices\n",
    "\n",
    "    cfl_stats = ExperimentLogger()\n",
    "\n",
    "    cluster_indices = [np.arange(len(clients)).astype(\"int\")]\n",
    "    client_clusters = [[clients[i] for i in idcs] for idcs in cluster_indices]\n",
    "\n",
    "\n",
    "    for c_round in range(1, COMMUNICATION_ROUNDS+1):\n",
    "\n",
    "        if c_round == 1:\n",
    "            for client in clients:\n",
    "                client.synchronize_with_server(server)\n",
    "\n",
    "        participating_clients = server.select_clients(clients, frac=1.0)\n",
    "\n",
    "        for client in participating_clients:\n",
    "            if c_round == 1:\n",
    "                client.distill()\n",
    "\n",
    "            train_stats = client.compute_weight_update(epochs=1) #train client\n",
    "\n",
    "            if c_round == 1000:\n",
    "                client.reset()\n",
    "\n",
    "        cluster_indices_new = []\n",
    "\n",
    "        for idc in cluster_indices:\n",
    "            max_norm = server.compute_max_update_norm([clients[i] for i in idc])\n",
    "            mean_norm = server.compute_mean_update_norm([clients[i] for i in idc])\n",
    "\n",
    "            #cluster 나누는 기준\n",
    "            if c_round == COMMUNICATION_ROUNDS: #무조건 한번 나누기\n",
    "                similarities = server.compute_pairwise_similarities(clients)\n",
    "\n",
    "                server.cache_model(idc, clients[idc[0]].W, acc_clients)\n",
    "\n",
    "                c1, c2, c3 = server.cluster_clients_GMM(similarities[idc][:,idc])\n",
    "                cluster_indices_new += [c1, c2, c3]\n",
    "\n",
    "        if c_round == 1000:\n",
    "            cluster_indices = aggregate(cluster_indices_new)\n",
    "\n",
    "        acc_clients = [client.evaluate() for client in clients]\n",
    "\n",
    "        if c_round == COMMUNICATION_ROUNDS: #무조건 한번 나누기\n",
    "            label_accuracies = pd.DataFrame()\n",
    "            label_predicted = pd.DataFrame()\n",
    "            label_soft_sum = pd.DataFrame()\n",
    "            label_diff = pd.DataFrame()\n",
    "\n",
    "            for i, client in enumerate(clients):\n",
    "                acc, pred, sum_, diff = server.evaluate(client.model)\n",
    "                # Convert each dictionary to a DataFrame and append to the respective DataFrame\n",
    "                label_accuracies = label_accuracies.append(pd.DataFrame(acc, index=[i]))\n",
    "                label_predicted = label_predicted.append(pd.DataFrame(pred, index=[i]))\n",
    "                label_soft_sum = label_soft_sum.append(pd.DataFrame(sum_, index=[i]))\n",
    "                label_diff = label_diff.append(pd.DataFrame(diff, index=[i]))\n",
    "\n",
    "            # Reset index for all DataFrames\n",
    "            label_accuracies.reset_index(drop=True, inplace=True)\n",
    "            label_predicted.reset_index(drop=True, inplace=True)\n",
    "            label_soft_sum.reset_index(drop=True, inplace=True)\n",
    "            label_diff.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        if c_round == 1:\n",
    "            first_accuracies = pd.DataFrame()\n",
    "            for i, client in enumerate(clients):\n",
    "                first_acc, pred, sum_, diff = server.evaluate(client.model)\n",
    "                first_accuracies = pd.concat([first_accuracies, pd.DataFrame(first_acc, index=[i])])\n",
    "            first_accuracies = first_accuracies.fillna(0)\n",
    "\n",
    "            client_acc_after_distill = sum(acc_clients)/len(acc_clients)\n",
    "            global_acc_after_distill = np.mean(np.ravel(first_accuracies.values))\n",
    "\n",
    "\n",
    "        elif c_round == COMMUNICATION_ROUNDS:\n",
    "            client_acc_final = sum(acc_clients)/len(acc_clients)\n",
    "            global_acc_final = np.mean(np.ravel(label_accuracies.values))\n",
    "\n",
    "        average_dw = server.get_average_dw(clients)\n",
    "        #print(average_dw)\n",
    "        cfl_stats.log({\"acc_clients\" : acc_clients, \"mean_norm\" : mean_norm, \"max_norm\" : max_norm,\n",
    "                      \"rounds\" : c_round, \"clusters\" : cluster_indices, \"average_dw\": average_dw})\n",
    "\n",
    "\n",
    "        display_train_stats(cfl_stats, EPS_1, EPS_2, COMMUNICATION_ROUNDS)\n",
    "\n",
    "\n",
    "    for idc in cluster_indices:    \n",
    "        server.cache_model(idc, clients[idc[0]].W, acc_clients)\n",
    "    \n",
    "    client_acc_after_distill = round(client_acc_after_distill, 3)\n",
    "    global_acc_after_distill = round(global_acc_after_distill, 3)\n",
    "    client_acc_final = round(client_acc_final, 3)\n",
    "    global_acc_final = round(global_acc_final, 3)\n",
    "    \n",
    "    return client_acc_after_distill, global_acc_after_distill, client_acc_final, global_acc_final\n",
    "\n",
    "    print(client_acc_after_distill, global_acc_after_distill)\n",
    "    print(client_acc_final, global_acc_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-13T03:44:16.511357Z",
     "iopub.status.idle": "2023-07-13T03:44:16.511547Z",
     "shell.execute_reply": "2023-07-13T03:44:16.511458Z",
     "shell.execute_reply.started": "2023-07-13T03:44:16.511448Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-13T03:44:16.512746Z",
     "iopub.status.idle": "2023-07-13T03:44:16.513304Z",
     "shell.execute_reply": "2023-07-13T03:44:16.513126Z",
     "shell.execute_reply.started": "2023-07-13T03:44:16.513104Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_accuracies.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-13T03:44:16.514343Z",
     "iopub.status.idle": "2023-07-13T03:44:16.514919Z",
     "shell.execute_reply": "2023-07-13T03:44:16.514762Z",
     "shell.execute_reply.started": "2023-07-13T03:44:16.514743Z"
    }
   },
   "outputs": [],
   "source": [
    "label_soft_sum.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-13T03:44:16.515915Z",
     "iopub.status.idle": "2023-07-13T03:44:16.516516Z",
     "shell.execute_reply": "2023-07-13T03:44:16.516361Z",
     "shell.execute_reply.started": "2023-07-13T03:44:16.516341Z"
    }
   },
   "outputs": [],
   "source": [
    "label_diff.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-13T03:44:16.517383Z",
     "iopub.status.idle": "2023-07-13T03:44:16.518016Z",
     "shell.execute_reply": "2023-07-13T03:44:16.517858Z",
     "shell.execute_reply.started": "2023-07-13T03:44:16.517840Z"
    }
   },
   "outputs": [],
   "source": [
    "label_predicted.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-13T03:44:16.518899Z",
     "iopub.status.idle": "2023-07-13T03:44:16.519516Z",
     "shell.execute_reply": "2023-07-13T03:44:16.519316Z",
     "shell.execute_reply.started": "2023-07-13T03:44:16.519270Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Instantiate PCA\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Apply PCA to the dataframes\n",
    "label_accuracies_pca = pca.fit_transform(label_accuracies)\n",
    "label_predicted_pca = pca.fit_transform(label_predicted)\n",
    "label_soft_sum_pca = pca.fit_transform(label_soft_sum)\n",
    "label_diff_pca = pca.fit_transform(label_diff)\n",
    "transformed_data = pca.fit_transform(similarities)\n",
    "\n",
    "# Create labels\n",
    "labels = [0, 0, 0, 1, 1, 1, 2, 2, 2]\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "# Scatter plots with larger dots\n",
    "dot_size = 50\n",
    "axs[0, 0].scatter(label_accuracies_pca[:, 0], label_accuracies_pca[:, 1], c=labels, s=dot_size)\n",
    "axs[0, 0].set_title('Label Accuracies')\n",
    "axs[0, 1].scatter(label_predicted_pca[:, 0], label_predicted_pca[:, 1], c=labels, s=dot_size)\n",
    "axs[0, 1].set_title('Label Predicted')\n",
    "axs[1, 0].scatter(label_soft_sum_pca[:, 0], label_soft_sum_pca[:, 1], c=labels, s=dot_size)\n",
    "axs[1, 0].set_title('Label Soft Sum')\n",
    "axs[1, 1].scatter(label_diff_pca[:, 0], label_diff_pca[:, 1], c=labels, s=dot_size)\n",
    "axs[1, 1].set_title('Label Soft Diff')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-13T03:44:16.520484Z",
     "iopub.status.idle": "2023-07-13T03:44:16.521118Z",
     "shell.execute_reply": "2023-07-13T03:44:16.520958Z",
     "shell.execute_reply.started": "2023-07-13T03:44:16.520939Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Calculate Silhouette Scores\n",
    "silhouette_accuracies = silhouette_score(label_accuracies_pca, labels)\n",
    "silhouette_predicted = silhouette_score(label_predicted_pca, labels)\n",
    "silhouette_soft_sum = silhouette_score(label_soft_sum_pca, labels)\n",
    "silhouette_diff = silhouette_score(label_diff_pca, labels)\n",
    "silhouette_transformed_data = silhouette_score(transformed_data, labels)\n",
    "\n",
    "print('Silhouette Score for Accuracies:', silhouette_accuracies)\n",
    "print('Silhouette Score for Predicted:', silhouette_predicted)\n",
    "print('Silhouette Score for Soft Sum:', silhouette_soft_sum)\n",
    "print('Silhouette Score for diff:', silhouette_diff)\n",
    "print('Silhouette Score for Model params:', silhouette_transformed_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-13T03:44:16.521962Z",
     "iopub.status.idle": "2023-07-13T03:44:16.522553Z",
     "shell.execute_reply": "2023-07-13T03:44:16.522398Z",
     "shell.execute_reply.started": "2023-07-13T03:44:16.522379Z"
    }
   },
   "outputs": [],
   "source": [
    "#df.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 Cluster 별 모델 파라미터 분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-13T03:44:16.523368Z",
     "iopub.status.idle": "2023-07-13T03:44:16.523666Z",
     "shell.execute_reply": "2023-07-13T03:44:16.523522Z",
     "shell.execute_reply.started": "2023-07-13T03:44:16.523507Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Fit and transform your data to 2D\n",
    "pca = PCA(n_components=2)\n",
    "transformed_data = pca.fit_transform(similarities)\n",
    "\n",
    "# Assign labels based on index ranges\n",
    "labels = [0, 0, 0, 1, 1, 1, 2, 2, 2]\n",
    "unique_labels = np.unique(labels)\n",
    "colors = plt.cm.Spectral(np.linspace(0, 0.35, len(unique_labels)))\n",
    "\n",
    "# Plot the transformed data with labels\n",
    "for label, color in zip(unique_labels, colors):\n",
    "    idx = np.where(labels == label)\n",
    "    plt.scatter(transformed_data[idx, 0], transformed_data[idx, 1], color=color, label=f'Cluster {label}')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.12.1-py3.8-cuda11.3",
   "language": "python",
   "name": "torch1.12.1-py3.8-cuda11.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "f79394e62bebc70f4ea6374f6a04753660b8235adfdaf8a6dfe67d7c0f65c745"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

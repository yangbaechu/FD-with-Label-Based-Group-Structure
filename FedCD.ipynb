{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-31T13:32:08.359522Z",
     "iopub.status.busy": "2023-07-31T13:32:08.359330Z",
     "iopub.status.idle": "2023-07-31T13:32:11.247717Z",
     "shell.execute_reply": "2023-07-31T13:32:11.244855Z",
     "shell.execute_reply.started": "2023-07-31T13:32:08.359500Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (fl_devices.py, line 214)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m/usr/local/lib/python3.8/dist-packages/IPython/core/interactiveshell.py:3460\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 19\u001b[0;36m\n\u001b[0;31m    from fl_devices import Server, Client\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m~/Desktop/Federate-distillation/fl_devices.py:214\u001b[0;36m\u001b[0m\n\u001b[0;31m    self.dW = {key: torch.zeros_like(value) for key, value in self.model.named_parameters()}\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import torch.nn as nn\n",
    "import pytz\n",
    "\n",
    "from models import ConvNet\n",
    "from helper import ExperimentLogger, display_train_stats\n",
    "from fl_devices import Server, Client\n",
    "from data_utils import generate_server_idcs, CustomSubset, split_noniid, split_contain_3class\n",
    "from torchvision.models import MobileNet_V3_Large\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-31T13:32:11.248967Z",
     "iopub.status.idle": "2023-07-31T13:32:11.249308Z",
     "shell.execute_reply": "2023-07-31T13:32:11.249150Z",
     "shell.execute_reply.started": "2023-07-31T13:32:11.249133Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "LOCAL_EPOCHS = 50\n",
    "EPS_1 = 0.4\n",
    "EPS_2 = 1.6\n",
    "N_CLIENTS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-31T13:32:11.251049Z",
     "iopub.status.idle": "2023-07-31T13:32:11.251368Z",
     "shell.execute_reply": "2023-07-31T13:32:11.251212Z",
     "shell.execute_reply.started": "2023-07-31T13:32:11.251196Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = datasets.CIFAR10(root=\"CIFAR10/\", download=False)\n",
    "idcs = np.random.permutation(len(data))\n",
    "\n",
    "def acc_test(server, clients, global_accs, client_distribution=[0.5, 0.3, 0.2]):\n",
    "    \n",
    "    # Get individual client accuracies\n",
    "    acc_clients = [client.evaluate() for client in clients]\n",
    "    \n",
    "    # Compute cluster accuracies\n",
    "    cluster_accs = []\n",
    "    start_idx = 0\n",
    "    for distribution in client_distribution:\n",
    "        end_idx = start_idx + int(distribution * len(clients))\n",
    "        cluster_acc = round(sum(acc_clients[start_idx:end_idx]) / (end_idx - start_idx), 3)\n",
    "        cluster_accs.append(cluster_acc)\n",
    "        start_idx = end_idx\n",
    "    \n",
    "    accuracies = [server.evaluate_distil(client.model) for client in clients]\n",
    "    global_acc = round(np.mean(accuracies), 3)\n",
    "    global_accs.append(global_acc)\n",
    "    \n",
    "    return cluster_accs, global_accs\n",
    "\n",
    "def cluster(server, clients):\n",
    "    label_predicted = pd.DataFrame()\n",
    "    for i, client in enumerate(clients):\n",
    "        pred = server.check_cluster(client.model)\n",
    "        # print(f'pred: {pred}, acc: {acc}, diff:{diff}')\n",
    "        label_predicted = pd.concat([label_predicted, pd.DataFrame(pred, index=[i])])\n",
    "    label_predicted.reset_index(drop=True, inplace=True)\n",
    "    label_predicted.fillna(0, inplace=True)\n",
    "    \n",
    "    print(f'predicted label')\n",
    "    print(label_predicted)\n",
    "    cluster_index = server.cluster_clients_GMM(label_predicted)\n",
    "    # PCA 후 cluster index를 label로 활용해 시각화\n",
    "    return label_predicted, cluster_index\n",
    "\n",
    "def visualize_clusters(label_predicted, c1, c2, c3):\n",
    "    # Reduce the dimension of the data\n",
    "    pca = PCA(n_components=2)\n",
    "    label_predicted_pca = pca.fit_transform(label_predicted)\n",
    "\n",
    "    # Plot the clusters\n",
    "    plt.figure(figsize=(10,7))\n",
    "\n",
    "    plt.scatter(label_predicted_pca[c1, 0], label_predicted_pca[c1, 1], c='blue', label='Cluster 1')\n",
    "    plt.scatter(label_predicted_pca[c2, 0], label_predicted_pca[c2, 1], c='red', label='Cluster 2')\n",
    "    plt.scatter(label_predicted_pca[c3, 0], label_predicted_pca[c3, 1], c='green', label='Cluster 3')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def get_cluster_logits(self, client_logits, cluster_idcs):\n",
    "    cluster_logits = []\n",
    "    for cluster in cluster_idcs:\n",
    "        cluster_client_logits = [client_logits[i] for i in cluster]\n",
    "        avg_cluster_logits = torch.mean(torch.stack(cluster_client_logits), dim=0)\n",
    "        cluster_logits.append(avg_cluster_logits)\n",
    "    return cluster_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-31T13:32:11.252972Z",
     "iopub.status.idle": "2023-07-31T13:32:11.253610Z",
     "shell.execute_reply": "2023-07-31T13:32:11.253431Z",
     "shell.execute_reply.started": "2023-07-31T13:32:11.253410Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def do_epoch_experiments(total_client_data, distill_data, ALPHA):\n",
    "    data_per_class=int(distill_data//10)\n",
    "    train_idcs, test_idcs = idcs[:total_client_data], idcs[total_client_data:(total_client_data + int(distill_data * 2))]\n",
    "    train_labels = data.targets\n",
    "    test_labels = data.targets\n",
    "    \n",
    "    server_idcs = generate_server_idcs(test_idcs, test_labels, int(distill_data//10))\n",
    "\n",
    "    client_idcs = split_noniid(train_idcs, train_labels, alpha=ALPHA, n_clients=N_CLIENTS)\n",
    "    client_data = [CustomSubset(data, idcs) for idcs in client_idcs]\n",
    "    test_data = CustomSubset(data, server_idcs, transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "    for i, client_datum in enumerate(client_data):\n",
    "        client_datum.subset_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    server = Server(MobileNet_V3_Large, lambda x : torch.optim.Adam(x),test_data)\n",
    "\n",
    "    clients = [Client(MobileNet_V3_Large, lambda x: torch.optim.Adam(x), dat, i) \n",
    "               for i, dat in enumerate(client_data)]\n",
    "\n",
    "    client_accs = []\n",
    "    global_accs = []\n",
    "    client_logits = []\n",
    "    \n",
    "    # 1.Local training\n",
    "    for epoch in range(LOCAL_EPOCHS):\n",
    "        for i, client in enumerate(clients):\n",
    "            client.compute_weight_update(epochs=1)\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            client_accs, global_accs = acc_test(server, clients, client_accs, global_accs)\n",
    "            print(f'client_acc: {client_accs[-1]}, global_acc: {global_accs[-1]}')\n",
    "\n",
    "    # 2.Clustering\n",
    "    label_predicted, cluster_index = cluster(server, clients)\n",
    "    # visualize_clusters(label_predicted, clusters[0], clusters[1], clusters[2])\n",
    "    \n",
    "    # 3.Get cluster, global loigt\n",
    "    for i, client in enumerate(clients):\n",
    "        if i == 0:\n",
    "            distill_data = server.get_clients_logit(client.model,data_per_class=data_per_class)\n",
    "            client_logits.append(distill_data[2])\n",
    "        else:\n",
    "            client_logits.append(server.get_clients_logit(client.model,data_per_class=data_per_class)[2])\n",
    "    global_logits = server.get_global_logits(client_logits)\n",
    "    get_cluster_logits(client_logits, cluster_index)\n",
    "    \n",
    "    # 4.Distillation\n",
    "    for i, client in enumerate(clients):\n",
    "        if i % 10 == 0:\n",
    "            print(f'client {i} distill')\n",
    "        client.distill((distill_data[0], distill_data[1], global_logits))\n",
    "    \n",
    "    client_accs, cluster_accs, global_accs = acc_test(server, clients, client_accs, cluster_accs, global_accs)\n",
    "    \n",
    "    print(f'total_client_data: {total_client_data}, data_per_class: {data_per_class}')\n",
    "    print(f'first acc: {client_accs[0]}, {cluster_accs}, {global_accs[0]}')\n",
    "    print(f'acc before distill: {client_accs[-2]}, {cluster_accs[-2]}, {global_accs[-2]}')\n",
    "    print(f'last acc: {client_accs[-1]}, {cluster_accs[-2]}, {global_accs[-1]}')\n",
    "    return client_accs, global_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-31T13:32:11.254692Z",
     "iopub.status.idle": "2023-07-31T13:32:11.255222Z",
     "shell.execute_reply": "2023-07-31T13:32:11.255095Z",
     "shell.execute_reply.started": "2023-07-31T13:32:11.255081Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_CLIENTS = 150\n",
    "\n",
    "now = datetime.datetime.now(pytz.timezone('Asia/Seoul'))\n",
    "date_time = now.strftime(\"%m%d_%H%M\")\n",
    "\n",
    "columns = pd.MultiIndex.from_product([['client_accs', 'cluster_accs', 'global_accs'], ['before_distill', 'after_distill']],\n",
    "                                     names=['acc_type', 'distill_state'])\n",
    "\n",
    "# The tuples for which we want to run the experiment \n",
    "desired_pairs = [(50000, 5000)]\n",
    "cluster_distribution = [0.5, 0.3, 0.2]\n",
    "# Add additional index 'global_distill' and 'cluster_distill'\n",
    "experiments = ['cluster_distill', 'global_distill']\n",
    "index = pd.MultiIndex.from_product([experiments, desired_pairs], names=['experiment', 'data_pair'])\n",
    "\n",
    "# Initialize an empty DataFrame with the desired index for rows and columns\n",
    "df = pd.DataFrame(index=index, columns=columns)\n",
    "\n",
    "for exp in experiments:\n",
    "    for pair in desired_pairs:\n",
    "        client_data, distill_data = pair\n",
    "\n",
    "        if exp == 'global_distill':\n",
    "            client_accs, cluster_accs, global_accs = global_distill_experiments(client_data, distill_data, ALPHA, NUMBER_OF_CLUSTER, cluster_distribution)\n",
    "        else:\n",
    "            client_accs, cluster_accs, global_accs = cluster_distill_experiments(client_data, distill_data, ALPHA, NUMBER_OF_CLUSTER, cluster_distribution)\n",
    "\n",
    "        # Set the values in the DataFrame\n",
    "        df.loc[(exp, pair), ('client_accs', 'before_distill')] = client_accs[-2]\n",
    "        df.loc[(exp, pair), ('client_accs', 'after_distill')] = client_accs[-1]\n",
    "        df.loc[(exp, pair), ('global_accs', 'before_distill')] = global_accs[-2]\n",
    "        df.loc[(exp, pair), ('global_accs', 'after_distill')] = global_accs[-1]\n",
    "        df.loc[(exp, pair), ('cluster_accs', 'before_distill')] = cluster_accs[-2]\n",
    "        df.loc[(exp, pair), ('cluster_accs', 'after_distill')] = cluster_accs[-1]\n",
    "\n",
    "        directory = f'results/Unbalanced_cluster'\n",
    "\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "\n",
    "        cluster_distribution_str = \"_\".join(map(str, cluster_distribution))  # Convert the cluster distribution to a string\n",
    "        file_name = f'{directory}/client:{N_CLIENTS}_cluster:{NUMBER_OF_CLUSTER}_distribution:{cluster_distribution_str}_{date_time}.csv'\n",
    "        df = df.round(decimals=3)\n",
    "        df.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-31T13:32:11.255954Z",
     "iopub.status.idle": "2023-07-31T13:32:11.256155Z",
     "shell.execute_reply": "2023-07-31T13:32:11.256063Z",
     "shell.execute_reply.started": "2023-07-31T13:32:11.256054Z"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-31T13:32:11.257593Z",
     "iopub.status.idle": "2023-07-31T13:32:11.257815Z",
     "shell.execute_reply": "2023-07-31T13:32:11.257697Z",
     "shell.execute_reply.started": "2023-07-31T13:32:11.257688Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('results/global_distill/CIFAR_0720_0435.csv', index_col=[0,1], header=[0,1])\n",
    "\n",
    "# 그릴 데이터와 제목을 리스트로 저장\n",
    "heatmap_data = [('client_accs', 'change_after_distill', 'Client Accuracy change after Distillation'),\n",
    "                ('global_accs', 'change_after_distill', 'Global Accuracy change after Distillation')]\n",
    "\n",
    "# Compute change in accuracy\n",
    "df[('client_accs', 'change_after_distill')] = df[('client_accs', 'after_distill')] - df[('client_accs', 'before_distill')]\n",
    "df[('global_accs', 'change_after_distill')] = df[('global_accs', 'after_distill')] - df[('global_accs', 'before_distill')]\n",
    "\n",
    "# 전체 데이터의 최솟값, 최댓값 계산\n",
    "vmin = min(df[data1][data2].min() for data1, data2, _ in heatmap_data)\n",
    "vmax = max(df[data1][data2].max() for data1, data2, _ in heatmap_data)\n",
    "\n",
    "for data1, data2, title in heatmap_data:\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    sns.heatmap(df[(data1, data2)].unstack(), annot=True, cmap='coolwarm', center=0, vmin=-0.1, vmax=0.2)\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Clustering 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-31T13:32:11.258736Z",
     "iopub.status.idle": "2023-07-31T13:32:11.259237Z",
     "shell.execute_reply": "2023-07-31T13:32:11.259104Z",
     "shell.execute_reply.started": "2023-07-31T13:32:11.259089Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def do_clustering_experiments(total_client_data=total_client_data, data_per_class=data_per_class, ALPHA=ALPHA):\n",
    "    train_idcs, test_idcs = idcs[:int(total_client_data*10)], idcs[int(total_client_data*10):]\n",
    "    train_labels = data.train_labels.numpy()\n",
    "    test_labels = data.train_labels.numpy()[int(total_client_data*10):]\n",
    "\n",
    "    client_idcs = split_noniid(train_idcs, train_labels, alpha=ALPHA, n_clients=N_CLIENTS)#, data_per_class=int(total_client_data/10))\n",
    "    # server_idcs = generate_server_idcs(test_idcs, test_labels, int(total_client_data*10))\n",
    "\n",
    "    client_data = [CustomSubset(data, idcs) for idcs in client_idcs]\n",
    "    test_data = CustomSubset(data, test_idcs, transforms.Compose([transforms.ToTensor()]))\n",
    "    \n",
    "    for i, client_datum in enumerate(client_data):\n",
    "        client_datum.subset_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    server = Server(resnet18, lambda x : torch.optim.SGD(x, lr=0.1, momentum=0.9),test_data)\n",
    "\n",
    "    \n",
    "    distillation_data_file = f'distillation_data_{data_per_class}_per_class.pth'\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if not os.path.exists(distillation_data_file):\n",
    "        # The file does not exist, generate and save the distillation data\n",
    "        distillation_data = server.make_distillation_data(data_per_class=data_per_class)\n",
    "        torch.save(distillation_data, distillation_data_file)\n",
    "\n",
    "    # Load the distillation data\n",
    "    distillation_data = torch.load(distillation_data_file)\n",
    "\n",
    "    clients = [Client(resnet18, lambda x : torch.optim.SGD(x, lr=0.1, momentum=0.9), dat, i, distillation_data) \n",
    "               for i, dat in enumerate(client_data)]\n",
    "\n",
    "    def aggregate(cluster_indices_new):\n",
    "        cluster_indices = cluster_indices_new\n",
    "        client_clusters = [[clients[i] for i in idcs] for idcs in cluster_indices]\n",
    "\n",
    "        server.aggregate_clusterwise(client_clusters)\n",
    "\n",
    "        return cluster_indices\n",
    "\n",
    "    cfl_stats = ExperimentLogger()\n",
    "\n",
    "    cluster_indices = [np.arange(len(clients)).astype(\"int\")]\n",
    "    client_clusters = [[clients[i] for i in idcs] for idcs in cluster_indices]\n",
    "\n",
    "\n",
    "    for epoch in range(1, LOCAL_EPOCHS+1):\n",
    "\n",
    "        if epoch == 1:\n",
    "            for client in clients:\n",
    "                client.synchronize_with_server(server)\n",
    "\n",
    "        participating_clients = server.select_clients(clients, frac=1.0)\n",
    "\n",
    "        for client in participating_clients:\n",
    "            if epoch == 1:\n",
    "                client.distill()\n",
    "\n",
    "            train_stats = client.compute_weight_update(epochs=1) #train client\n",
    "\n",
    "            if epoch == 1000:\n",
    "                client.reset()\n",
    "\n",
    "        cluster_indices_new = []\n",
    "\n",
    "        for idc in cluster_indices:\n",
    "            max_norm = server.compute_max_update_norm([clients[i] for i in idc])\n",
    "            mean_norm = server.compute_mean_update_norm([clients[i] for i in idc])\n",
    "\n",
    "            #cluster 나누는 기준\n",
    "            if epoch == LOCAL_EPOCHS: #무조건 한번 나누기\n",
    "                similarities = server.compute_pairwise_similarities(clients)\n",
    "\n",
    "                server.cache_model(idc, clients[idc[0]].W, acc_clients)\n",
    "\n",
    "                c1, c2, c3 = server.cluster_clients_GMM(similarities[idc][:,idc])\n",
    "                cluster_indices_new += [c1, c2, c3]\n",
    "\n",
    "        if epoch == 1000:\n",
    "            cluster_indices = aggregate(cluster_indices_new)\n",
    "\n",
    "        acc_clients = [client.evaluate() for client in clients]\n",
    "\n",
    "        if epoch == LOCAL_EPOCHS: #무조건 한번 나누기\n",
    "            label_accuracies = pd.DataFrame()\n",
    "            label_predicted = pd.DataFrame()\n",
    "            label_soft_sum = pd.DataFrame()\n",
    "            label_diff = pd.DataFrame()\n",
    "\n",
    "            for i, client in enumerate(clients):\n",
    "                acc, pred, sum_, diff = server.evaluate(client.model)\n",
    "                # Convert each dictionary to a DataFrame and append to the respective DataFrame\n",
    "                label_accuracies = label_accuracies.append(pd.DataFrame(acc, index=[i]))\n",
    "                label_predicted = label_predicted.append(pd.DataFrame(pred, index=[i]))\n",
    "                label_soft_sum = label_soft_sum.append(pd.DataFrame(sum_, index=[i]))\n",
    "                label_diff = label_diff.append(pd.DataFrame(diff, index=[i]))\n",
    "\n",
    "            # Reset index for all DataFrames\n",
    "            label_accuracies.reset_index(drop=True, inplace=True)\n",
    "            label_predicted.reset_index(drop=True, inplace=True)\n",
    "            label_soft_sum.reset_index(drop=True, inplace=True)\n",
    "            label_diff.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        if epoch == 1:\n",
    "            first_accuracies = pd.DataFrame()\n",
    "            for i, client in enumerate(clients):\n",
    "                first_acc, pred, sum_, diff = server.evaluate(client.model)\n",
    "                first_accuracies = pd.concat([first_accuracies, pd.DataFrame(first_acc, index=[i])])\n",
    "            first_accuracies = first_accuracies.fillna(0)\n",
    "\n",
    "            client_acc_after_distill = sum(acc_clients)/len(acc_clients)\n",
    "            global_acc_after_distill = np.mean(np.ravel(first_accuracies.values))\n",
    "\n",
    "\n",
    "        elif epoch == LOCAL_EPOCHS:\n",
    "            client_acc_final = sum(acc_clients)/len(acc_clients)\n",
    "            global_acc_final = np.mean(np.ravel(label_accuracies.values))\n",
    "\n",
    "        average_dw = server.get_average_dw(clients)\n",
    "        #print(average_dw)\n",
    "        cfl_stats.log({\"acc_clients\" : acc_clients, \"mean_norm\" : mean_norm, \"max_norm\" : max_norm,\n",
    "                      \"rounds\" : epoch, \"clusters\" : cluster_indices, \"average_dw\": average_dw})\n",
    "\n",
    "\n",
    "        display_train_stats(cfl_stats, EPS_1, EPS_2, LOCAL_EPOCHS)\n",
    "\n",
    "\n",
    "    for idc in cluster_indices:    \n",
    "        server.cache_model(idc, clients[idc[0]].W, acc_clients)\n",
    "    \n",
    "    client_acc_after_distill = round(client_acc_after_distill, 3)\n",
    "    global_acc_after_distill = round(global_acc_after_distill, 3)\n",
    "    client_acc_final = round(client_acc_final, 3)\n",
    "    global_acc_final = round(global_acc_final, 3)\n",
    "    \n",
    "    return client_acc_after_distill, global_acc_after_distill, client_acc_final, global_acc_final\n",
    "\n",
    "    print(client_acc_after_distill, global_acc_after_distill)\n",
    "    print(client_acc_final, global_acc_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-31T13:32:11.260046Z",
     "iopub.status.idle": "2023-07-31T13:32:11.260254Z",
     "shell.execute_reply": "2023-07-31T13:32:11.260145Z",
     "shell.execute_reply.started": "2023-07-31T13:32:11.260136Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-31T13:32:11.261009Z",
     "iopub.status.idle": "2023-07-31T13:32:11.261183Z",
     "shell.execute_reply": "2023-07-31T13:32:11.261103Z",
     "shell.execute_reply.started": "2023-07-31T13:32:11.261094Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_accuracies.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-31T13:32:11.262489Z",
     "iopub.status.idle": "2023-07-31T13:32:11.262805Z",
     "shell.execute_reply": "2023-07-31T13:32:11.262652Z",
     "shell.execute_reply.started": "2023-07-31T13:32:11.262637Z"
    }
   },
   "outputs": [],
   "source": [
    "label_soft_sum.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-31T13:32:11.263861Z",
     "iopub.status.idle": "2023-07-31T13:32:11.264470Z",
     "shell.execute_reply": "2023-07-31T13:32:11.264296Z",
     "shell.execute_reply.started": "2023-07-31T13:32:11.264276Z"
    }
   },
   "outputs": [],
   "source": [
    "label_diff.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-31T13:32:11.265437Z",
     "iopub.status.idle": "2023-07-31T13:32:11.266041Z",
     "shell.execute_reply": "2023-07-31T13:32:11.265867Z",
     "shell.execute_reply.started": "2023-07-31T13:32:11.265848Z"
    }
   },
   "outputs": [],
   "source": [
    "label_predicted.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-31T13:32:11.266998Z",
     "iopub.status.idle": "2023-07-31T13:32:11.267480Z",
     "shell.execute_reply": "2023-07-31T13:32:11.267309Z",
     "shell.execute_reply.started": "2023-07-31T13:32:11.267292Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Instantiate PCA\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Apply PCA to the dataframes\n",
    "label_accuracies_pca = pca.fit_transform(label_accuracies)\n",
    "label_predicted_pca = pca.fit_transform(label_predicted)\n",
    "label_soft_sum_pca = pca.fit_transform(label_soft_sum)\n",
    "label_diff_pca = pca.fit_transform(label_diff)\n",
    "transformed_data = pca.fit_transform(similarities)\n",
    "\n",
    "# Create labels\n",
    "labels = [0, 0, 0, 1, 1, 1, 2, 2, 2]\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "# Scatter plots with larger dots\n",
    "dot_size = 50\n",
    "axs[0, 0].scatter(label_accuracies_pca[:, 0], label_accuracies_pca[:, 1], c=labels, s=dot_size)\n",
    "axs[0, 0].set_title('Label Accuracies')\n",
    "axs[0, 1].scatter(label_predicted_pca[:, 0], label_predicted_pca[:, 1], c=labels, s=dot_size)\n",
    "axs[0, 1].set_title('Label Predicted')\n",
    "axs[1, 0].scatter(label_soft_sum_pca[:, 0], label_soft_sum_pca[:, 1], c=labels, s=dot_size)\n",
    "axs[1, 0].set_title('Label Soft Sum')\n",
    "axs[1, 1].scatter(label_diff_pca[:, 0], label_diff_pca[:, 1], c=labels, s=dot_size)\n",
    "axs[1, 1].set_title('Label Soft Diff')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-31T13:32:11.268545Z",
     "iopub.status.idle": "2023-07-31T13:32:11.268844Z",
     "shell.execute_reply": "2023-07-31T13:32:11.268694Z",
     "shell.execute_reply.started": "2023-07-31T13:32:11.268680Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Calculate Silhouette Scores\n",
    "silhouette_accuracies = silhouette_score(label_accuracies_pca, labels)\n",
    "silhouette_predicted = silhouette_score(label_predicted_pca, labels)\n",
    "silhouette_soft_sum = silhouette_score(label_soft_sum_pca, labels)\n",
    "silhouette_diff = silhouette_score(label_diff_pca, labels)\n",
    "silhouette_transformed_data = silhouette_score(transformed_data, labels)\n",
    "\n",
    "print('Silhouette Score for Accuracies:', silhouette_accuracies)\n",
    "print('Silhouette Score for Predicted:', silhouette_predicted)\n",
    "print('Silhouette Score for Soft Sum:', silhouette_soft_sum)\n",
    "print('Silhouette Score for diff:', silhouette_diff)\n",
    "print('Silhouette Score for Model params:', silhouette_transformed_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-31T13:32:11.269781Z",
     "iopub.status.idle": "2023-07-31T13:32:11.270070Z",
     "shell.execute_reply": "2023-07-31T13:32:11.269929Z",
     "shell.execute_reply.started": "2023-07-31T13:32:11.269915Z"
    }
   },
   "outputs": [],
   "source": [
    "#df.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 Cluster 별 모델 파라미터 분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-31T13:32:11.271242Z",
     "iopub.status.idle": "2023-07-31T13:32:11.271569Z",
     "shell.execute_reply": "2023-07-31T13:32:11.271415Z",
     "shell.execute_reply.started": "2023-07-31T13:32:11.271398Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Fit and transform your data to 2D\n",
    "pca = PCA(n_components=2)\n",
    "transformed_data = pca.fit_transform(similarities)\n",
    "\n",
    "# Assign labels based on index ranges\n",
    "labels = [0, 0, 0, 1, 1, 1, 2, 2, 2]\n",
    "unique_labels = np.unique(labels)\n",
    "colors = plt.cm.Spectral(np.linspace(0, 0.35, len(unique_labels)))\n",
    "\n",
    "# Plot the transformed data with labels\n",
    "for label, color in zip(unique_labels, colors):\n",
    "    idx = np.where(labels == label)\n",
    "    plt.scatter(transformed_data[idx, 0], transformed_data[idx, 1], color=color, label=f'Cluster {label}')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.12.1-py3.8-cuda11.3",
   "language": "python",
   "name": "torch1.12.1-py3.8-cuda11.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "f79394e62bebc70f4ea6374f6a04753660b8235adfdaf8a6dfe67d7c0f65c745"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

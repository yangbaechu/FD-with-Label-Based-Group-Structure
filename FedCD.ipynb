{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T02:31:47.323855Z",
     "iopub.status.busy": "2023-07-19T02:31:47.323749Z",
     "iopub.status.idle": "2023-07-19T02:31:48.821256Z",
     "shell.execute_reply": "2023-07-19T02:31:48.820148Z",
     "shell.execute_reply.started": "2023-07-19T02:31:47.323842Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "from models import ConvNet\n",
    "from helper import ExperimentLogger, display_train_stats\n",
    "from fl_devices import Server, Client\n",
    "from data_utils import split_not_contain_every_class, split_contain_every_class, generate_server_idcs, CustomSubset, split_noniid, split_noniid_original\n",
    "from torchvision.models import resnet50\n",
    "import seaborn as sns\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T02:31:48.822523Z",
     "iopub.status.busy": "2023-07-19T02:31:48.822300Z",
     "iopub.status.idle": "2023-07-19T02:31:48.826373Z",
     "shell.execute_reply": "2023-07-19T02:31:48.825576Z",
     "shell.execute_reply.started": "2023-07-19T02:31:48.822505Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "COMMUNICATION_ROUNDS = 50\n",
    "EPS_1 = 0.4\n",
    "EPS_2 = 1.6\n",
    "N_CLIENTS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T02:31:48.827120Z",
     "iopub.status.busy": "2023-07-19T02:31:48.826992Z",
     "iopub.status.idle": "2023-07-19T02:31:48.890643Z",
     "shell.execute_reply": "2023-07-19T02:31:48.889976Z",
     "shell.execute_reply.started": "2023-07-19T02:31:48.827107Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = datasets.MNIST(root=\"MNIST/\", download=False)\n",
    "\n",
    "mapp = np.array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype='<U1')\n",
    "\n",
    "idcs = np.random.permutation(len(data))\n",
    "\n",
    "#print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T02:31:48.892174Z",
     "iopub.status.busy": "2023-07-19T02:31:48.891779Z",
     "iopub.status.idle": "2023-07-19T02:31:48.902283Z",
     "shell.execute_reply": "2023-07-19T02:31:48.901780Z",
     "shell.execute_reply.started": "2023-07-19T02:31:48.892156Z"
    }
   },
   "outputs": [],
   "source": [
    "def do_epoch_experiments(total_client_data, data_per_class, ALPHA):\n",
    "    train_idcs, test_idcs = idcs[:int(total_client_data)], idcs[int(total_client_data):]\n",
    "    train_labels = data.targets\n",
    "    test_labels = data.targets[int(total_client_data):]\n",
    "\n",
    "\n",
    "    client_idcs = split_noniid(train_idcs, train_labels, alpha=ALPHA, n_clients=N_CLIENTS)\n",
    "    client_data = [CustomSubset(data, idcs) for idcs in client_idcs]\n",
    "    test_data = CustomSubset(data, test_idcs, transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "    for i, client_datum in enumerate(client_data):\n",
    "        client_datum.subset_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    server = Server(ConvNet, lambda x : torch.optim.SGD(x, lr=0.1, momentum=0.9),test_data)\n",
    "    \n",
    "    distillation_data_file = f'distill_data/MNIST/{data_per_class*10}.pth'\n",
    "    if not os.path.exists(distillation_data_file):\n",
    "        torch.save(server.make_distillation_data(data_per_class=data_per_class), distillation_data_file)\n",
    "        \n",
    "    distillation_data = torch.load(distillation_data_file)\n",
    "\n",
    "    clients = [Client(ConvNet, lambda x: torch.optim.SGD(x, lr=0.1, momentum=0.9), dat, i, distillation_data) \n",
    "               for i, dat in enumerate(client_data)]\n",
    "\n",
    "    client_accs = []\n",
    "    global_accs = []\n",
    "    client_logits = []\n",
    "\n",
    "    for c_round in range(COMMUNICATION_ROUNDS):\n",
    "\n",
    "        for client in clients:\n",
    "            if c_round == COMMUNICATION_ROUNDS -1:\n",
    "                #client.synchronize_with_server(server)\n",
    "                client.distill((distill_data[0], distill_data[1], averaged_logits))\n",
    "\n",
    "            if c_round != 0:\n",
    "                client.compute_weight_update(epochs=1)\n",
    "\n",
    "        if c_round % 10 == 0 or c_round == COMMUNICATION_ROUNDS -1:\n",
    "            acc_clients = [client.evaluate() for client in clients]\n",
    "            client_acc = round(sum(acc_clients) / len(acc_clients), 3)\n",
    "            client_accs.append(client_acc)\n",
    "\n",
    "            accuracies = [server.evaluate_distil(client.model) for client in clients]\n",
    "            global_acc = round(np.mean(accuracies), 3)\n",
    "            global_accs.append(global_acc)\n",
    "            \n",
    "        if c_round == COMMUNICATION_ROUNDS -2:\n",
    "            for i, client in enumerate(clients):\n",
    "                if i == 0:\n",
    "                    distill_data = server.get_clients_logit(client.model,data_per_class=data_per_class)\n",
    "                    client_logits.append(distill_data[2])\n",
    "                else:\n",
    "                    client_logits.append(server.get_clients_logit(client.model,data_per_class=data_per_class)[2])\n",
    "            averaged_logits = server.make_averaged_logits(client_logits)\n",
    "    \n",
    "    print(f'total_client_data: {total_client_data}, data_per_class: {data_per_class}')\n",
    "    print(f'first acc: {client_accs[0]}, {global_accs[0]}')\n",
    "    print(f'acc before distill: {client_accs[-2]}, {global_accs[-2]}')\n",
    "    print(f'last acc: {client_accs[-1]}, {global_accs[-1]}')\n",
    "    return client_accs, global_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T02:31:48.903008Z",
     "iopub.status.busy": "2023-07-19T02:31:48.902848Z",
     "iopub.status.idle": "2023-07-19T02:32:00.288140Z",
     "shell.execute_reply": "2023-07-19T02:32:00.285744Z",
     "shell.execute_reply.started": "2023-07-19T02:31:48.902994Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 0's distribution: [  0   5   5   5   7   8   8   8   9   9   9   9   9   9  14  14  15 110\n",
      " 110]\n",
      "class 1's distribution: [  1   1   1   1   1  98  98  98  98  99 102 102 102 102 102 109 109 109\n",
      " 109]\n",
      "class 2's distribution: [ 0  0  0  0  0  0 26 28 28 29 33 33 33 33 34 34 34 34 98]\n",
      "class 3's distribution: [ 1  1  1 23 23 23 23 23 48 48 48 48 48 48 74 75 75 75 75]\n",
      "class 4's distribution: [ 0  0  0  0  1  1  1 13 32 32 32 32 35 77 77 77 77 77 77]\n",
      "class 5's distribution: [ 0  0 29 29 29 29 30 30 30 30 30 30 30 30 30 31 77 77 77]\n",
      "class 6's distribution: [ 0  0  3  3  3  3 66 66 66 67 67 67 67 74 74 89 89 89 89]\n",
      "class 7's distribution: [ 0 35 37 69 72 72 72 84 84 84 84 85 85 92 92 92 92 92 92]\n",
      "class 8's distribution: [  0   0   0   0   0   0   0   0  97  97  97 103 105 105 112 112 113 113\n",
      " 113]\n",
      "class 9's distribution: [ 26  26  26  26  89  89  89  98  98  98  98  99  99  99  99  99 100 100\n",
      " 100]\n",
      "class 0's distribution: [  0   0   0   0   0  23  23  26  26  27  27  27  27  29  34  34  73 110\n",
      " 110]\n",
      "class 1's distribution: [  0   3   4   4   4   6  30  30  30  35  53  57  59 108 109 109 109 109\n",
      " 109]\n",
      "class 2's distribution: [ 0  0  0  0 15 15 15 22 22 23 23 23 23 23 23 96 96 96 98]\n",
      "class 3's distribution: [13 13 71 71 71 71 71 72 72 72 82 82 82 82 82 82 82 82 82]\n",
      "class 4's distribution: [36 42 42 44 44 44 44 61 61 61 76 78 79 79 94 94 94 94 94]\n",
      "class 5's distribution: [ 3  3  3  3 37 37 37 37 37 37 37 38 38 38 38 38 43 54 57]\n",
      "class 6's distribution: [ 0  5  5  6 29 30 31 31 37 42 42 60 71 71 73 73 73 89 89]\n",
      "class 7's distribution: [ 0  0  0  0  0  0  0  0  3  3 14 14 62 62 62 62 62 62 62]\n",
      "class 8's distribution: [  0  83  83  83  83  85  93  99 105 105 105 105 105 105 105 105 112 112\n",
      " 113]\n",
      "class 9's distribution: [  0   0   0   2   2   2   2   2   2   3   3   5   5   5  66  66  66  66\n",
      " 100]\n",
      "class 0's distribution: [  0   0   0   1   1   1  12  12  12  33  81  93 101 102 102 102 102 102\n",
      " 102]\n",
      "class 1's distribution: [  0   1   1  16  47  75  75  76  78  78  79  80  92  92 107 108 108 108\n",
      " 108]\n",
      "class 2's distribution: [ 0 80 80 83 83 83 85 85 86 88 97 97 97 97 97 98 98 98 98]\n",
      "class 3's distribution: [ 0  0  0  0  7  7  9  9 10 10 10 10 13 13 13 22 23 26 26]\n",
      "class 4's distribution: [ 0  0 11 30 30 30 34 34 72 72 72 72 72 73 73 94 94 94 94]\n",
      "class 5's distribution: [ 2  2  2  5  5  6 23 23 23 23 23 23 23 26 86 86 92 92 93]\n",
      "class 6's distribution: [ 0  0  1  2  2  2  2 33 36 36 36 36 51 51 51 54 89 89 90]\n",
      "class 7's distribution: [  0   0  39  39  39  71  73  73  75  75  75  75  75  95  95  96  96  96\n",
      " 106]\n",
      "class 8's distribution: [  0   0   0   0   0   0   0  15  18  18  18  18  18 112 112 112 112 112\n",
      " 113]\n",
      "class 9's distribution: [  4   4   4   4   4   4   4   4   4   9   9  11  11  11  11  11  46  46\n",
      " 100]\n",
      "class 0's distribution: [  0   4   4   4   4   5  77  77  77  84  84  84  84  84  84 110 110 110\n",
      " 110]\n",
      "class 1's distribution: [  2  23  23  29  29  31  31  97  97 105 105 105 108 108 108 108 108 108\n",
      " 109]\n",
      "class 2's distribution: [56 56 56 56 57 57 57 57 57 58 69 69 83 83 83 85 92 92 93]\n",
      "class 3's distribution: [ 0  0  0  0  3  3  3  3  3  3  3  5  7  7 12 22 22 22 22]\n",
      "class 4's distribution: [ 0  0 53 53 53 55 55 55 67 68 68 72 72 84 84 84 84 84 95]\n",
      "class 5's distribution: [ 0  0  0  0  0  0  0  0 14 84 84 84 84 84 85 86 86 86 92]\n",
      "class 6's distribution: [ 0 15 15 27 27 30 30 30 30 30 30 84 84 84 84 84 89 89 89]\n",
      "class 7's distribution: [  0   5   5  17  17  19  19  19  19  19 102 102 105 105 105 105 105 105\n",
      " 106]\n",
      "class 8's distribution: [  0  65  65  65  65  65  65  65  65  65  65  65  65  65 112 112 112 112\n",
      " 113]\n",
      "class 9's distribution: [  0   0   0   1   1   1   1   1  99  99  99  99  99  99  99  99  99  99\n",
      " 100]\n",
      "class 0's distribution: [  0   1  11  29  29  29  29  42  49  49  78  78  78  83  87  87  90 110\n",
      " 110]\n",
      "class 1's distribution: [  0   0  20  20  32  32  41  43  43  51  51  51  75  75  77  78  78  78\n",
      " 109]\n",
      "class 2's distribution: [ 0 19 19 19 23 23 23 23 23 23 93 93 93 93 93 93 93 98 98]\n",
      "class 3's distribution: [ 0  0  0  3  9  9  9  9 10 14 14 14 14 14 17 17 72 72 72]\n",
      "class 4's distribution: [ 0  0  0  0 12 13 21 21 39 39 39 90 90 90 90 90 90 90 90]\n",
      "class 5's distribution: [ 1  1  1  1  1  1  1  1  1  5  5  5  5  5 10 10 10 72 88]\n",
      "class 6's distribution: [ 0  0  0  0  1 69 69 69 69 69 69 69 69 69 70 70 70 70 70]\n",
      "class 7's distribution: [  0   2   6   6   6   6  57  57  57  57  57  57  57  64  66  66  66  66\n",
      " 100]\n",
      "class 8's distribution: [  0   0  68 107 107 107 107 107 111 112 112 112 112 112 112 113 113 113\n",
      " 113]\n",
      "class 9's distribution: [ 27  27  27  27  27  27  27  27  27  29  29  29  36  36  74 100 100 100\n",
      " 100]\n",
      "[28, 22, 102, 60, 35, 69, 68, 15, 30, 19, 99, 51, 31, 12, 55, 28, 58, 87, 81, 50]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, client_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(client_data_values):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j, distill_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(distill_data_values):\n\u001b[0;32m---> 18\u001b[0m         client_accs, global_accs \u001b[38;5;241m=\u001b[39m \u001b[43mdo_epoch_experiments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_client_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_data\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_per_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdistill_data\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mALPHA\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;66;03m# Set the values in the DataFrame\u001b[39;00m\n\u001b[1;32m     21\u001b[0m         df\u001b[38;5;241m.\u001b[39mloc[(client_data, distill_data), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclient_accs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_distill\u001b[39m\u001b[38;5;124m'\u001b[39m)] \u001b[38;5;241m=\u001b[39m client_accs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n",
      "Cell \u001b[0;32mIn[4], line 44\u001b[0m, in \u001b[0;36mdo_epoch_experiments\u001b[0;34m(total_client_data, data_per_class, ALPHA)\u001b[0m\n\u001b[1;32m     41\u001b[0m client_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(\u001b[38;5;28msum\u001b[39m(acc_clients) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(acc_clients), \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     42\u001b[0m client_accs\u001b[38;5;241m.\u001b[39mappend(client_acc)\n\u001b[0;32m---> 44\u001b[0m accuracies \u001b[38;5;241m=\u001b[39m [server\u001b[38;5;241m.\u001b[39mevaluate_distil(client\u001b[38;5;241m.\u001b[39mmodel) \u001b[38;5;28;01mfor\u001b[39;00m client \u001b[38;5;129;01min\u001b[39;00m clients]\n\u001b[1;32m     45\u001b[0m global_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(accuracies), \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     46\u001b[0m global_accs\u001b[38;5;241m.\u001b[39mappend(global_acc)\n",
      "Cell \u001b[0;32mIn[4], line 44\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     41\u001b[0m client_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(\u001b[38;5;28msum\u001b[39m(acc_clients) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(acc_clients), \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     42\u001b[0m client_accs\u001b[38;5;241m.\u001b[39mappend(client_acc)\n\u001b[0;32m---> 44\u001b[0m accuracies \u001b[38;5;241m=\u001b[39m [\u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_distil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m client \u001b[38;5;129;01min\u001b[39;00m clients]\n\u001b[1;32m     45\u001b[0m global_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(accuracies), \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     46\u001b[0m global_accs\u001b[38;5;241m.\u001b[39mappend(global_acc)\n",
      "File \u001b[0;32m~/FCL/fl_devices.py:440\u001b[0m, in \u001b[0;36mServer.evaluate_distil\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    437\u001b[0m samples, correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 440\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (x, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader):\n\u001b[1;32m    441\u001b[0m         \u001b[38;5;66;03m# Evaluate only on 20% of the data\u001b[39;00m\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m20\u001b[39m:\n\u001b[1;32m    443\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/torch1.12.1-py3.8-cuda11.3/lib/python3.8/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.venv/torch1.12.1-py3.8-cuda11.3/lib/python3.8/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.venv/torch1.12.1-py3.8-cuda11.3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.venv/torch1.12.1-py3.8-cuda11.3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/FCL/data_utils.py:202\u001b[0m, in \u001b[0;36mCustomSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    199\u001b[0m x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]]\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubset_transform:\n\u001b[0;32m--> 202\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubset_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, y\n",
      "File \u001b[0;32m~/.venv/torch1.12.1-py3.8-cuda11.3/lib/python3.8/site-packages/torchvision/transforms/transforms.py:94\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 94\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.venv/torch1.12.1-py3.8-cuda11.3/lib/python3.8/site-packages/torchvision/transforms/transforms.py:134\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    127\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/torch1.12.1-py3.8-cuda11.3/lib/python3.8/site-packages/torchvision/transforms/functional.py:164\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[1;32m    163\u001b[0m mode_to_nptype \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint32, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint16, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mfloat32}\n\u001b[0;32m--> 164\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_to_nptype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    167\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/PIL/Image.py:701\u001b[0m, in \u001b[0;36mImage.__array_interface__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m         new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtobytes(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 701\u001b[0m         new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, (\u001b[38;5;167;01mMemoryError\u001b[39;00m, \u001b[38;5;167;01mRecursionError\u001b[39;00m)):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/PIL/Image.py:772\u001b[0m, in \u001b[0;36mImage.tobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    771\u001b[0m     l, s, d \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mencode(bufsize)\n\u001b[0;32m--> 772\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m s:\n\u001b[1;32m    774\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a multi-index for the columns\n",
    "now = datetime.datetime.now()\n",
    "date_time = now.strftime(\"%m%d_%H%M\")\n",
    "\n",
    "columns = pd.MultiIndex.from_product([['client_accs', 'global_accs'], ['before_distill', 'after_distill']],\n",
    "                                     names=['acc_type', 'distill_state'])\n",
    "\n",
    "client_data_values = [100, 100, 200, 500]\n",
    "distill_data_values = [200, 500, 1000]\n",
    "\n",
    "index = pd.MultiIndex.from_product([client_data_values, distill_data_values], names=['client_data', 'distill_data'])\n",
    "\n",
    "# Initialize an empty DataFrame with the desired MultiIndex for rows and columns\n",
    "df = pd.DataFrame(np.nan, index=index, columns=columns)\n",
    "\n",
    "for i, client_data in enumerate(client_data_values):\n",
    "    for j, distill_data in enumerate(distill_data_values):\n",
    "        client_accs, global_accs = do_epoch_experiments(total_client_data=client_data*10, data_per_class=int(distill_data//10), ALPHA=0.1)\n",
    "\n",
    "        # Set the values in the DataFrame\n",
    "        df.loc[(client_data, distill_data), ('client_accs', 'before_distill')] = client_accs[-2]\n",
    "        df.loc[(client_data, distill_data), ('client_accs', 'after_distill')] = client_accs[-1]\n",
    "        df.loc[(client_data, distill_data), ('global_accs', 'before_distill')] = global_accs[-2]\n",
    "        df.loc[(client_data, distill_data), ('global_accs', 'after_distill')] = global_accs[-1]\n",
    "\n",
    "df.to_csv(f'results/global_distill/{date_time}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-19T02:32:00.289093Z",
     "iopub.status.idle": "2023-07-19T02:32:00.289309Z",
     "shell.execute_reply": "2023-07-19T02:32:00.289202Z",
     "shell.execute_reply.started": "2023-07-19T02:32:00.289192Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 그릴 데이터와 제목을 리스트로 저장\n",
    "heatmap_data = [('client_accs', 'after_distill', 'Client Accuracies After Distillation'),\n",
    "                ('global_accs', 'after_distill', 'Global Accuracies After Distillation')]\n",
    "\n",
    "# 전체 데이터의 최솟값, 최댓값 계산\n",
    "vmin = min(df[data1][data2].min() for data1, data2, _ in heatmap_data)\n",
    "vmax = max(df[data1][data2].max() for data1, data2, _ in heatmap_data)\n",
    "\n",
    "for data1, data2, title in heatmap_data:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(df[(data1, data2)].unstack(), annot=True, cmap='Spectral', center=0, vmin=vmin, vmax=vmax)\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-19T02:32:00.290458Z",
     "iopub.status.idle": "2023-07-19T02:32:00.290639Z",
     "shell.execute_reply": "2023-07-19T02:32:00.290556Z",
     "shell.execute_reply.started": "2023-07-19T02:32:00.290547Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#client_acc_after_distill, global_acc_after_distill, client_acc_final, global_acc_final = do_experiments(total_client_data=total_client_data, data_per_class=data_per_class, ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-19T02:32:00.294716Z",
     "iopub.status.idle": "2023-07-19T02:32:00.295085Z",
     "shell.execute_reply": "2023-07-19T02:32:00.294891Z",
     "shell.execute_reply.started": "2023-07-19T02:32:00.294874Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pprint\n",
    "\n",
    "# Define the directory where the results are saved\n",
    "save_dir = 'results/data_amount'\n",
    "\n",
    "# Define the different data_per_class values\n",
    "data_per_class_values = [10, 20, 50, 100, 200, 500]\n",
    "\n",
    "# For pretty printing\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "\n",
    "#0323: client 10, 50 결과 저장\n",
    "with open(os.path.join(save_dir, f'0417_ALPHA_0.1.pkl'), 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "pp.pprint(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-19T02:32:00.298114Z",
     "iopub.status.idle": "2023-07-19T02:32:00.298399Z",
     "shell.execute_reply": "2023-07-19T02:32:00.298261Z",
     "shell.execute_reply.started": "2023-07-19T02:32:00.298247Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the different data_per_class values\n",
    "total_client_data_values = [50, 100, 200, 500, 1000, 2000, 5000]\n",
    "data_per_class_values = [10, 20, 50, 100, 200, 500]\n",
    "\n",
    "# Metrics\n",
    "metrics = ['global_acc_after_distill', 'global_acc_final']\n",
    "\n",
    "# Prepare list to store the rows\n",
    "data = []\n",
    "\n",
    "# Fill in the rows\n",
    "for data_per_class in data_per_class_values:\n",
    "    with open(os.path.join(save_dir, f'0412_ALPHA_0.1.pkl'), 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "\n",
    "    for total_client_data in total_client_data_values:\n",
    "        for metric in metrics:\n",
    "            row = {\"total_client_data\": total_client_data, \"metric\": metric, \"data_per_class\": data_per_class, \"value\": round(results[(total_client_data, data_per_class, 0.1)][metric], 2)}\n",
    "            data.append(row)\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Pivot the DataFrame to the desired shape\n",
    "df = df.pivot(index=['total_client_data', 'metric'], columns='data_per_class', values='value')\n",
    "\n",
    "# Filter for 'client_acc_after_distill'\n",
    "df_after_distill = df.xs('global_acc_after_distill', level='metric')\n",
    "df_final = df.xs('global_acc_final', level='metric')\n",
    "\n",
    "# Plot heatmap\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20,8))  # Create a figure and a set of subplots\n",
    "fig.suptitle('Heatmaps for client_acc_after_distill and client_acc_final')\n",
    "\n",
    "sns.heatmap(df_after_distill, annot=True, cmap=\"YlGnBu\", ax=axs[0])\n",
    "axs[0].set_title('global_acc_after_distill Heatmap')\n",
    "\n",
    "sns.heatmap(df_final, annot=True, cmap=\"YlGnBu\", ax=axs[1])\n",
    "axs[1].set_title('global_acc_final Heatmap')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 서버, client data 양 2차 실험"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Clustering 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-19T02:32:00.301764Z",
     "iopub.status.idle": "2023-07-19T02:32:00.302365Z",
     "shell.execute_reply": "2023-07-19T02:32:00.302186Z",
     "shell.execute_reply.started": "2023-07-19T02:32:00.302167Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def do_clustering_experiments(total_client_data=total_client_data, data_per_class=data_per_class, ALPHA=ALPHA):\n",
    "    train_idcs, test_idcs = idcs[:int(total_client_data*10)], idcs[int(total_client_data*10):]\n",
    "    train_labels = data.train_labels.numpy()\n",
    "    test_labels = data.train_labels.numpy()[int(total_client_data*10):]\n",
    "\n",
    "    client_idcs = split_noniid(train_idcs, train_labels, alpha=ALPHA, n_clients=N_CLIENTS)#, data_per_class=int(total_client_data/10))\n",
    "    # server_idcs = generate_server_idcs(test_idcs, test_labels, int(total_client_data*10))\n",
    "\n",
    "    client_data = [CustomSubset(data, idcs) for idcs in client_idcs]\n",
    "    test_data = CustomSubset(data, test_idcs, transforms.Compose([transforms.ToTensor()]))\n",
    "    \n",
    "    for i, client_datum in enumerate(client_data):\n",
    "        client_datum.subset_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    server = Server(ConvNet, lambda x : torch.optim.SGD(x, lr=0.1, momentum=0.9),test_data)\n",
    "\n",
    "    \n",
    "    distillation_data_file = f'distillation_data_{data_per_class}_per_class.pth'\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if not os.path.exists(distillation_data_file):\n",
    "        # The file does not exist, generate and save the distillation data\n",
    "        distillation_data = server.make_distillation_data(data_per_class=data_per_class)\n",
    "        torch.save(distillation_data, distillation_data_file)\n",
    "\n",
    "    # Load the distillation data\n",
    "    distillation_data = torch.load(distillation_data_file)\n",
    "\n",
    "    clients = [Client(ConvNet, lambda x : torch.optim.SGD(x, lr=0.1, momentum=0.9), dat, i, distillation_data) \n",
    "               for i, dat in enumerate(client_data)]\n",
    "\n",
    "    def aggregate(cluster_indices_new):\n",
    "        cluster_indices = cluster_indices_new\n",
    "        client_clusters = [[clients[i] for i in idcs] for idcs in cluster_indices]\n",
    "\n",
    "        server.aggregate_clusterwise(client_clusters)\n",
    "\n",
    "        return cluster_indices\n",
    "\n",
    "    cfl_stats = ExperimentLogger()\n",
    "\n",
    "    cluster_indices = [np.arange(len(clients)).astype(\"int\")]\n",
    "    client_clusters = [[clients[i] for i in idcs] for idcs in cluster_indices]\n",
    "\n",
    "\n",
    "    for c_round in range(1, COMMUNICATION_ROUNDS+1):\n",
    "\n",
    "        if c_round == 1:\n",
    "            for client in clients:\n",
    "                client.synchronize_with_server(server)\n",
    "\n",
    "        participating_clients = server.select_clients(clients, frac=1.0)\n",
    "\n",
    "        for client in participating_clients:\n",
    "            if c_round == 1:\n",
    "                client.distill()\n",
    "\n",
    "            train_stats = client.compute_weight_update(epochs=1) #train client\n",
    "\n",
    "            if c_round == 1000:\n",
    "                client.reset()\n",
    "\n",
    "        cluster_indices_new = []\n",
    "\n",
    "        for idc in cluster_indices:\n",
    "            max_norm = server.compute_max_update_norm([clients[i] for i in idc])\n",
    "            mean_norm = server.compute_mean_update_norm([clients[i] for i in idc])\n",
    "\n",
    "            #cluster 나누는 기준\n",
    "            if c_round == COMMUNICATION_ROUNDS: #무조건 한번 나누기\n",
    "                similarities = server.compute_pairwise_similarities(clients)\n",
    "\n",
    "                server.cache_model(idc, clients[idc[0]].W, acc_clients)\n",
    "\n",
    "                c1, c2, c3 = server.cluster_clients_GMM(similarities[idc][:,idc])\n",
    "                cluster_indices_new += [c1, c2, c3]\n",
    "\n",
    "        if c_round == 1000:\n",
    "            cluster_indices = aggregate(cluster_indices_new)\n",
    "\n",
    "        acc_clients = [client.evaluate() for client in clients]\n",
    "\n",
    "        if c_round == COMMUNICATION_ROUNDS: #무조건 한번 나누기\n",
    "            label_accuracies = pd.DataFrame()\n",
    "            label_predicted = pd.DataFrame()\n",
    "            label_soft_sum = pd.DataFrame()\n",
    "            label_diff = pd.DataFrame()\n",
    "\n",
    "            for i, client in enumerate(clients):\n",
    "                acc, pred, sum_, diff = server.evaluate(client.model)\n",
    "                # Convert each dictionary to a DataFrame and append to the respective DataFrame\n",
    "                label_accuracies = label_accuracies.append(pd.DataFrame(acc, index=[i]))\n",
    "                label_predicted = label_predicted.append(pd.DataFrame(pred, index=[i]))\n",
    "                label_soft_sum = label_soft_sum.append(pd.DataFrame(sum_, index=[i]))\n",
    "                label_diff = label_diff.append(pd.DataFrame(diff, index=[i]))\n",
    "\n",
    "            # Reset index for all DataFrames\n",
    "            label_accuracies.reset_index(drop=True, inplace=True)\n",
    "            label_predicted.reset_index(drop=True, inplace=True)\n",
    "            label_soft_sum.reset_index(drop=True, inplace=True)\n",
    "            label_diff.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        if c_round == 1:\n",
    "            first_accuracies = pd.DataFrame()\n",
    "            for i, client in enumerate(clients):\n",
    "                first_acc, pred, sum_, diff = server.evaluate(client.model)\n",
    "                first_accuracies = pd.concat([first_accuracies, pd.DataFrame(first_acc, index=[i])])\n",
    "            first_accuracies = first_accuracies.fillna(0)\n",
    "\n",
    "            client_acc_after_distill = sum(acc_clients)/len(acc_clients)\n",
    "            global_acc_after_distill = np.mean(np.ravel(first_accuracies.values))\n",
    "\n",
    "\n",
    "        elif c_round == COMMUNICATION_ROUNDS:\n",
    "            client_acc_final = sum(acc_clients)/len(acc_clients)\n",
    "            global_acc_final = np.mean(np.ravel(label_accuracies.values))\n",
    "\n",
    "        average_dw = server.get_average_dw(clients)\n",
    "        #print(average_dw)\n",
    "        cfl_stats.log({\"acc_clients\" : acc_clients, \"mean_norm\" : mean_norm, \"max_norm\" : max_norm,\n",
    "                      \"rounds\" : c_round, \"clusters\" : cluster_indices, \"average_dw\": average_dw})\n",
    "\n",
    "\n",
    "        display_train_stats(cfl_stats, EPS_1, EPS_2, COMMUNICATION_ROUNDS)\n",
    "\n",
    "\n",
    "    for idc in cluster_indices:    \n",
    "        server.cache_model(idc, clients[idc[0]].W, acc_clients)\n",
    "    \n",
    "    client_acc_after_distill = round(client_acc_after_distill, 3)\n",
    "    global_acc_after_distill = round(global_acc_after_distill, 3)\n",
    "    client_acc_final = round(client_acc_final, 3)\n",
    "    global_acc_final = round(global_acc_final, 3)\n",
    "    \n",
    "    return client_acc_after_distill, global_acc_after_distill, client_acc_final, global_acc_final\n",
    "\n",
    "    print(client_acc_after_distill, global_acc_after_distill)\n",
    "    print(client_acc_final, global_acc_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-19T02:32:00.305351Z",
     "iopub.status.idle": "2023-07-19T02:32:00.305667Z",
     "shell.execute_reply": "2023-07-19T02:32:00.305501Z",
     "shell.execute_reply.started": "2023-07-19T02:32:00.305487Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-19T02:32:00.308698Z",
     "iopub.status.idle": "2023-07-19T02:32:00.309389Z",
     "shell.execute_reply": "2023-07-19T02:32:00.309209Z",
     "shell.execute_reply.started": "2023-07-19T02:32:00.309189Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_accuracies.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-19T02:32:00.310322Z",
     "iopub.status.idle": "2023-07-19T02:32:00.310615Z",
     "shell.execute_reply": "2023-07-19T02:32:00.310469Z",
     "shell.execute_reply.started": "2023-07-19T02:32:00.310455Z"
    }
   },
   "outputs": [],
   "source": [
    "label_soft_sum.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-19T02:32:00.311806Z",
     "iopub.status.idle": "2023-07-19T02:32:00.312482Z",
     "shell.execute_reply": "2023-07-19T02:32:00.312287Z",
     "shell.execute_reply.started": "2023-07-19T02:32:00.312264Z"
    }
   },
   "outputs": [],
   "source": [
    "label_diff.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-19T02:32:00.313429Z",
     "iopub.status.idle": "2023-07-19T02:32:00.313728Z",
     "shell.execute_reply": "2023-07-19T02:32:00.313575Z",
     "shell.execute_reply.started": "2023-07-19T02:32:00.313560Z"
    }
   },
   "outputs": [],
   "source": [
    "label_predicted.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-19T02:32:00.315241Z",
     "iopub.status.idle": "2023-07-19T02:32:00.315571Z",
     "shell.execute_reply": "2023-07-19T02:32:00.315414Z",
     "shell.execute_reply.started": "2023-07-19T02:32:00.315400Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Instantiate PCA\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Apply PCA to the dataframes\n",
    "label_accuracies_pca = pca.fit_transform(label_accuracies)\n",
    "label_predicted_pca = pca.fit_transform(label_predicted)\n",
    "label_soft_sum_pca = pca.fit_transform(label_soft_sum)\n",
    "label_diff_pca = pca.fit_transform(label_diff)\n",
    "transformed_data = pca.fit_transform(similarities)\n",
    "\n",
    "# Create labels\n",
    "labels = [0, 0, 0, 1, 1, 1, 2, 2, 2]\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "# Scatter plots with larger dots\n",
    "dot_size = 50\n",
    "axs[0, 0].scatter(label_accuracies_pca[:, 0], label_accuracies_pca[:, 1], c=labels, s=dot_size)\n",
    "axs[0, 0].set_title('Label Accuracies')\n",
    "axs[0, 1].scatter(label_predicted_pca[:, 0], label_predicted_pca[:, 1], c=labels, s=dot_size)\n",
    "axs[0, 1].set_title('Label Predicted')\n",
    "axs[1, 0].scatter(label_soft_sum_pca[:, 0], label_soft_sum_pca[:, 1], c=labels, s=dot_size)\n",
    "axs[1, 0].set_title('Label Soft Sum')\n",
    "axs[1, 1].scatter(label_diff_pca[:, 0], label_diff_pca[:, 1], c=labels, s=dot_size)\n",
    "axs[1, 1].set_title('Label Soft Diff')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-19T02:32:00.316851Z",
     "iopub.status.idle": "2023-07-19T02:32:00.317194Z",
     "shell.execute_reply": "2023-07-19T02:32:00.317025Z",
     "shell.execute_reply.started": "2023-07-19T02:32:00.317009Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Calculate Silhouette Scores\n",
    "silhouette_accuracies = silhouette_score(label_accuracies_pca, labels)\n",
    "silhouette_predicted = silhouette_score(label_predicted_pca, labels)\n",
    "silhouette_soft_sum = silhouette_score(label_soft_sum_pca, labels)\n",
    "silhouette_diff = silhouette_score(label_diff_pca, labels)\n",
    "silhouette_transformed_data = silhouette_score(transformed_data, labels)\n",
    "\n",
    "print('Silhouette Score for Accuracies:', silhouette_accuracies)\n",
    "print('Silhouette Score for Predicted:', silhouette_predicted)\n",
    "print('Silhouette Score for Soft Sum:', silhouette_soft_sum)\n",
    "print('Silhouette Score for diff:', silhouette_diff)\n",
    "print('Silhouette Score for Model params:', silhouette_transformed_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-19T02:32:00.318583Z",
     "iopub.status.idle": "2023-07-19T02:32:00.318885Z",
     "shell.execute_reply": "2023-07-19T02:32:00.318730Z",
     "shell.execute_reply.started": "2023-07-19T02:32:00.318716Z"
    }
   },
   "outputs": [],
   "source": [
    "#df.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 Cluster 별 모델 파라미터 분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-19T02:32:00.320159Z",
     "iopub.status.idle": "2023-07-19T02:32:00.320759Z",
     "shell.execute_reply": "2023-07-19T02:32:00.320584Z",
     "shell.execute_reply.started": "2023-07-19T02:32:00.320565Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Fit and transform your data to 2D\n",
    "pca = PCA(n_components=2)\n",
    "transformed_data = pca.fit_transform(similarities)\n",
    "\n",
    "# Assign labels based on index ranges\n",
    "labels = [0, 0, 0, 1, 1, 1, 2, 2, 2]\n",
    "unique_labels = np.unique(labels)\n",
    "colors = plt.cm.Spectral(np.linspace(0, 0.35, len(unique_labels)))\n",
    "\n",
    "# Plot the transformed data with labels\n",
    "for label, color in zip(unique_labels, colors):\n",
    "    idx = np.where(labels == label)\n",
    "    plt.scatter(transformed_data[idx, 0], transformed_data[idx, 1], color=color, label=f'Cluster {label}')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.12.1-py3.8-cuda11.3",
   "language": "python",
   "name": "torch1.12.1-py3.8-cuda11.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "f79394e62bebc70f4ea6374f6a04753660b8235adfdaf8a6dfe67d7c0f65c745"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

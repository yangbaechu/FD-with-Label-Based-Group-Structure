{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-04T14:25:29.898498Z",
     "iopub.status.busy": "2023-07-04T14:25:29.898306Z",
     "iopub.status.idle": "2023-07-04T14:25:31.435509Z",
     "shell.execute_reply": "2023-07-04T14:25:31.434309Z",
     "shell.execute_reply.started": "2023-07-04T14:25:29.898476Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models import ConvNet\n",
    "from helper import ExperimentLogger, display_train_stats\n",
    "from fl_devices import Server, Client\n",
    "from data_utils import split_not_contain_every_class, split_contain_every_class, generate_server_idcs, CustomSubset, split_noniid\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-04T14:25:31.437070Z",
     "iopub.status.busy": "2023-07-04T14:25:31.436744Z",
     "iopub.status.idle": "2023-07-04T14:25:31.441075Z",
     "shell.execute_reply": "2023-07-04T14:25:31.440377Z",
     "shell.execute_reply.started": "2023-07-04T14:25:31.437050Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "COMMUNICATION_ROUNDS = 50\n",
    "EPS_1 = 0.4\n",
    "EPS_2 = 1.6\n",
    "N_CLIENTS = 10\n",
    "\n",
    "total_client_data = 1000\n",
    "data_per_class = 50\n",
    "ALPHA = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-04T14:25:31.442083Z",
     "iopub.status.busy": "2023-07-04T14:25:31.441948Z",
     "iopub.status.idle": "2023-07-04T14:25:46.858936Z",
     "shell.execute_reply": "2023-07-04T14:25:46.857495Z",
     "shell.execute_reply.started": "2023-07-04T14:25:31.442070Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to CIFAR10/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c8f3fe852b4a2b873ceb36dc6c86e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting CIFAR10/cifar-10-python.tar.gz to CIFAR10/\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "data = datasets.CIFAR10(root=\"CIFAR10/\", download=False)\n",
    "\n",
    "mapp = np.array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype='<U1')\n",
    "\n",
    "idcs = np.random.permutation(len(data))\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-04T14:25:46.861154Z",
     "iopub.status.busy": "2023-07-04T14:25:46.860711Z",
     "iopub.status.idle": "2023-07-04T14:25:46.872510Z",
     "shell.execute_reply": "2023-07-04T14:25:46.871514Z",
     "shell.execute_reply.started": "2023-07-04T14:25:46.861131Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def do_experiments(total_client_data=total_client_data, data_per_class=data_per_class, ALPHA=ALPHA):\n",
    "    train_idcs, test_idcs = idcs[:int(total_client_data*10)], idcs[int(total_client_data*10):]\n",
    "    train_labels = train_data.targets\n",
    "    test_labels = test_data.targets[int(total_client_data*10):]\n",
    "\n",
    "\n",
    "    client_idcs = split_noniid(train_idcs, train_labels, alpha=ALPHA, n_clients=N_CLIENTS)\n",
    "    client_data = [CustomSubset(data, idcs) for idcs in client_idcs]\n",
    "    test_data = CustomSubset(data, test_idcs, transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "    for i, client_datum in enumerate(client_data):\n",
    "        client_datum.subset_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    server = Server(ConvNet, lambda x : torch.optim.SGD(x, lr=0.1, momentum=0.9),test_data)\n",
    "\n",
    "    distillation_data_file = f'distillation_data_{data_per_class}_per_class.pth'\n",
    "\n",
    "    if not os.path.exists(distillation_data_file):\n",
    "        torch.save(server.make_distillation_data(data_per_class=data_per_class), distillation_data_file)\n",
    "     \n",
    "    # save a model for size comparison\n",
    "    torch.save(server.model.state_dict(), 'model.pth')\n",
    "    \n",
    "    distillation_data_file = f'distillation_data_{data_per_class}_per_class.pth'\n",
    "    if not os.path.exists(distillation_data_file):\n",
    "        torch.save(server.make_distillation_data(data_per_class=data_per_class), distillation_data_file)\n",
    "        \n",
    "    distillation_data = torch.load(distillation_data_file)\n",
    "\n",
    "    clients = [Client(ConvNet, lambda x : torch.optim.SGD(x, lr=0.1, momentum=0.9), dat, i, distillation_data) \n",
    "               for i, dat in enumerate(client_data)]\n",
    "\n",
    "    cfl_stats = ExperimentLogger()\n",
    "\n",
    "    for c_round in range(1, COMMUNICATION_ROUNDS+1):\n",
    "        if c_round == 1:\n",
    "            for client in clients:\n",
    "                client.synchronize_with_server(server)\n",
    "                client.distill()\n",
    "                \n",
    "            first_acc_clients = [client.evaluate() for client in clients]\n",
    "            accuracies = [server.evaluate_distil(client.model) for client in clients]\n",
    "\n",
    "        server.select_clients(clients, frac=1.0)\n",
    "\n",
    "        for client in clients:\n",
    "            client.compute_weight_update(epochs=1) #train client\n",
    "\n",
    "        acc_clients = [client.evaluate() for client in clients]\n",
    "\n",
    "    label_accuracies = pd.DataFrame(server.evaluate(client.model)[0] for client in clients)\n",
    "\n",
    "    client_acc_after_distill = round(sum(first_acc_clients) / len(first_acc_clients), 3)\n",
    "    \n",
    "    global_acc_after_distill = round(np.mean(accuracies), 3)\n",
    "    #global_acc_after_distill = round(np.mean(np.ravel(pd.DataFrame(server.evaluate(client.model)[0] for client in clients).values)), 3)\n",
    "\n",
    "    client_acc_final = round(sum(acc_clients) / len(acc_clients), 3)\n",
    "    global_acc_final = round(np.mean(np.ravel(label_accuracies.values)), 3)\n",
    "    print(client_acc_after_distill, global_acc_after_distill, client_acc_final, global_acc_final)\n",
    "    return client_acc_after_distill, global_acc_after_distill, client_acc_final, global_acc_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-04T14:25:46.875922Z",
     "iopub.status.busy": "2023-07-04T14:25:46.875561Z",
     "iopub.status.idle": "2023-07-04T14:25:46.879804Z",
     "shell.execute_reply": "2023-07-04T14:25:46.878955Z",
     "shell.execute_reply.started": "2023-07-04T14:25:46.875898Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#client_acc_after_distill, global_acc_after_distill, client_acc_final, global_acc_final = do_experiments(total_client_data=total_client_data, data_per_class=data_per_class, ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-04T14:25:46.881541Z",
     "iopub.status.busy": "2023-07-04T14:25:46.880896Z",
     "iopub.status.idle": "2023-07-04T14:25:47.002333Z",
     "shell.execute_reply": "2023-07-04T14:25:47.000662Z",
     "shell.execute_reply.started": "2023-07-04T14:25:46.881517Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_per_class: 5\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CIFAR10' object has no attribute 'train_labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Repeat the experiment twice\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Call the function and save the results\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     client_acc_after_distill, global_acc_after_distill, client_acc_final, global_acc_final \u001b[38;5;241m=\u001b[39m \u001b[43mdo_experiments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_client_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_client_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_per_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_per_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mALPHA\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mALPHA\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# Store the results in a temporary list\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     temp_results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclient_acc_after_distill\u001b[39m\u001b[38;5;124m'\u001b[39m: client_acc_after_distill, \n\u001b[1;32m     32\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglobal_acc_after_distill\u001b[39m\u001b[38;5;124m'\u001b[39m: global_acc_after_distill, \n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclient_acc_final\u001b[39m\u001b[38;5;124m'\u001b[39m: client_acc_final, \n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglobal_acc_final\u001b[39m\u001b[38;5;124m'\u001b[39m: global_acc_final\n\u001b[1;32m     35\u001b[0m     })\n",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m, in \u001b[0;36mdo_experiments\u001b[0;34m(total_client_data, data_per_class, ALPHA)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_experiments\u001b[39m(total_client_data\u001b[38;5;241m=\u001b[39mtotal_client_data, data_per_class\u001b[38;5;241m=\u001b[39mdata_per_class, ALPHA\u001b[38;5;241m=\u001b[39mALPHA):\n\u001b[1;32m      2\u001b[0m     train_idcs, test_idcs \u001b[38;5;241m=\u001b[39m idcs[:\u001b[38;5;28mint\u001b[39m(total_client_data\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m)], idcs[\u001b[38;5;28mint\u001b[39m(total_client_data\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m):]\n\u001b[0;32m----> 3\u001b[0m     train_labels \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_labels\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      4\u001b[0m     test_labels \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mtrain_labels\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;28mint\u001b[39m(total_client_data\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m):]\n\u001b[1;32m      6\u001b[0m     client_idcs \u001b[38;5;241m=\u001b[39m split_noniid(train_idcs, train_labels, alpha\u001b[38;5;241m=\u001b[39mALPHA, n_clients\u001b[38;5;241m=\u001b[39mN_CLIENTS)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CIFAR10' object has no attribute 'train_labels'"
     ]
    }
   ],
   "source": [
    "# Define the directory where you want to save the results\n",
    "save_dir = 'results/data_amount'\n",
    "os.makedirs(save_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "ALPHA = 1\n",
    "\n",
    "# Define the different total_client_data, data_per_class, and ALPHA values\n",
    "total_client_data_values = [100]\n",
    "data_per_class_values = [5, 10, 20]\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Loop over the total_client_data and data_per_class values\n",
    "for data_per_class in data_per_class_values:\n",
    "    print(f'data_per_class: {data_per_class}')\n",
    "    for total_client_data in total_client_data_values:\n",
    "            \n",
    "        temp_results = []\n",
    "\n",
    "        # Repeat the experiment twice\n",
    "        for _ in range(1):\n",
    "            # Call the function and save the results\n",
    "            client_acc_after_distill, global_acc_after_distill, client_acc_final, global_acc_final = do_experiments(\n",
    "                total_client_data=total_client_data, \n",
    "                data_per_class=data_per_class, \n",
    "                ALPHA=ALPHA\n",
    "            )\n",
    "\n",
    "            # Store the results in a temporary list\n",
    "            temp_results.append({\n",
    "                'client_acc_after_distill': client_acc_after_distill, \n",
    "                'global_acc_after_distill': global_acc_after_distill, \n",
    "                'client_acc_final': client_acc_final, \n",
    "                'global_acc_final': global_acc_final\n",
    "            })\n",
    "\n",
    "        # Calculate the average of the results\n",
    "        avg_results = {\n",
    "            key: np.mean([res[key] for res in temp_results]) \n",
    "            for key in temp_results[0]\n",
    "        }\n",
    "\n",
    "        # Save the average results in the results dictionary\n",
    "        results[(total_client_data, data_per_class, ALPHA)] = avg_results\n",
    "\n",
    "    # Save the results dictionary to disk after each data_per_class\n",
    "    with open(os.path.join(save_dir, f'0420_ALPHA_{ALPHA}.pkl'), 'wb') as f:\n",
    "        pickle.dump(results, f)## 1) test 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-04T14:25:47.003026Z",
     "iopub.status.idle": "2023-07-04T14:25:47.003245Z",
     "shell.execute_reply": "2023-07-04T14:25:47.003143Z",
     "shell.execute_reply.started": "2023-07-04T14:25:47.003132Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pprint\n",
    "\n",
    "# Define the directory where the results are saved\n",
    "save_dir = 'results/data_amount'\n",
    "\n",
    "# Define the different data_per_class values\n",
    "data_per_class_values = [10, 20, 50, 100, 200, 500]\n",
    "\n",
    "# For pretty printing\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "\n",
    "#0323: client 10, 50 결과 저장\n",
    "with open(os.path.join(save_dir, f'0417_ALPHA_0.1.pkl'), 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "pp.pprint(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-04T14:25:47.005098Z",
     "iopub.status.idle": "2023-07-04T14:25:47.005405Z",
     "shell.execute_reply": "2023-07-04T14:25:47.005260Z",
     "shell.execute_reply.started": "2023-07-04T14:25:47.005244Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the different data_per_class values\n",
    "total_client_data_values = [50, 100, 200, 500, 1000, 2000, 5000]\n",
    "data_per_class_values = [10, 20, 50, 100, 200, 500]\n",
    "\n",
    "# Metrics\n",
    "metrics = ['global_acc_after_distill', 'global_acc_final']\n",
    "\n",
    "# Prepare list to store the rows\n",
    "data = []\n",
    "\n",
    "# Fill in the rows\n",
    "for data_per_class in data_per_class_values:\n",
    "    with open(os.path.join(save_dir, f'0412_ALPHA_0.1.pkl'), 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "\n",
    "    for total_client_data in total_client_data_values:\n",
    "        for metric in metrics:\n",
    "            row = {\"total_client_data\": total_client_data, \"metric\": metric, \"data_per_class\": data_per_class, \"value\": round(results[(total_client_data, data_per_class, 0.1)][metric], 2)}\n",
    "            data.append(row)\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Pivot the DataFrame to the desired shape\n",
    "df = df.pivot(index=['total_client_data', 'metric'], columns='data_per_class', values='value')\n",
    "\n",
    "# Filter for 'client_acc_after_distill'\n",
    "df_after_distill = df.xs('global_acc_after_distill', level='metric')\n",
    "df_final = df.xs('global_acc_final', level='metric')\n",
    "\n",
    "# Plot heatmap\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20,8))  # Create a figure and a set of subplots\n",
    "fig.suptitle('Heatmaps for client_acc_after_distill and client_acc_final')\n",
    "\n",
    "sns.heatmap(df_after_distill, annot=True, cmap=\"YlGnBu\", ax=axs[0])\n",
    "axs[0].set_title('global_acc_after_distill Heatmap')\n",
    "\n",
    "sns.heatmap(df_final, annot=True, cmap=\"YlGnBu\", ax=axs[1])\n",
    "axs[1].set_title('global_acc_final Heatmap')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 서버, client data 양 2차 실험"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Clustering 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-04T14:25:47.006950Z",
     "iopub.status.idle": "2023-07-04T14:25:47.007250Z",
     "shell.execute_reply": "2023-07-04T14:25:47.007107Z",
     "shell.execute_reply.started": "2023-07-04T14:25:47.007092Z"
    }
   },
   "outputs": [],
   "source": [
    "def do_clustering_experiments(total_client_data=total_client_data, data_per_class=data_per_class, ALPHA=ALPHA):\n",
    "    train_idcs, test_idcs = idcs[:int(total_client_data*10)], idcs[int(total_client_data*10):]\n",
    "    train_labels = data.train_labels.numpy()\n",
    "    test_labels = data.train_labels.numpy()[int(total_client_data*10):]\n",
    "\n",
    "    client_idcs = split_noniid(train_idcs, train_labels, alpha=ALPHA, n_clients=N_CLIENTS)#, data_per_class=int(total_client_data/10))\n",
    "    # server_idcs = generate_server_idcs(test_idcs, test_labels, int(total_client_data*10))\n",
    "\n",
    "    client_data = [CustomSubset(data, idcs) for idcs in client_idcs]\n",
    "    test_data = CustomSubset(data, test_idcs, transforms.Compose([transforms.ToTensor()]))\n",
    "    \n",
    "    for i, client_datum in enumerate(client_data):\n",
    "        client_datum.subset_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    server = Server(ConvNet, lambda x : torch.optim.SGD(x, lr=0.1, momentum=0.9),test_data)\n",
    "\n",
    "    \n",
    "    distillation_data_file = f'distillation_data_{data_per_class}_per_class.pth'\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if not os.path.exists(distillation_data_file):\n",
    "        # The file does not exist, generate and save the distillation data\n",
    "        distillation_data = server.make_distillation_data(data_per_class=data_per_class)\n",
    "        torch.save(distillation_data, distillation_data_file)\n",
    "\n",
    "    # Load the distillation data\n",
    "    distillation_data = torch.load(distillation_data_file)\n",
    "\n",
    "    clients = [Client(ConvNet, lambda x : torch.optim.SGD(x, lr=0.1, momentum=0.9), dat, i, distillation_data) \n",
    "               for i, dat in enumerate(client_data)]\n",
    "\n",
    "    def aggregate(cluster_indices_new):\n",
    "        cluster_indices = cluster_indices_new\n",
    "        client_clusters = [[clients[i] for i in idcs] for idcs in cluster_indices]\n",
    "\n",
    "        server.aggregate_clusterwise(client_clusters)\n",
    "\n",
    "        return cluster_indices\n",
    "\n",
    "    cfl_stats = ExperimentLogger()\n",
    "\n",
    "    cluster_indices = [np.arange(len(clients)).astype(\"int\")]\n",
    "    client_clusters = [[clients[i] for i in idcs] for idcs in cluster_indices]\n",
    "\n",
    "\n",
    "    for c_round in range(1, COMMUNICATION_ROUNDS+1):\n",
    "\n",
    "        if c_round == 1:\n",
    "            for client in clients:\n",
    "                client.synchronize_with_server(server)\n",
    "\n",
    "        participating_clients = server.select_clients(clients, frac=1.0)\n",
    "\n",
    "        for client in participating_clients:\n",
    "            if c_round == 1:\n",
    "                client.distill()\n",
    "\n",
    "            train_stats = client.compute_weight_update(epochs=1) #train client\n",
    "\n",
    "            if c_round == 1000:\n",
    "                client.reset()\n",
    "\n",
    "        cluster_indices_new = []\n",
    "\n",
    "        for idc in cluster_indices:\n",
    "            max_norm = server.compute_max_update_norm([clients[i] for i in idc])\n",
    "            mean_norm = server.compute_mean_update_norm([clients[i] for i in idc])\n",
    "\n",
    "            #cluster 나누는 기준\n",
    "            if c_round == COMMUNICATION_ROUNDS: #무조건 한번 나누기\n",
    "                similarities = server.compute_pairwise_similarities(clients)\n",
    "\n",
    "                server.cache_model(idc, clients[idc[0]].W, acc_clients)\n",
    "\n",
    "                c1, c2, c3 = server.cluster_clients_GMM(similarities[idc][:,idc])\n",
    "                cluster_indices_new += [c1, c2, c3]\n",
    "\n",
    "        if c_round == 1000:\n",
    "            cluster_indices = aggregate(cluster_indices_new)\n",
    "\n",
    "        acc_clients = [client.evaluate() for client in clients]\n",
    "\n",
    "        if c_round == COMMUNICATION_ROUNDS: #무조건 한번 나누기\n",
    "            label_accuracies = pd.DataFrame()\n",
    "            label_predicted = pd.DataFrame()\n",
    "            label_soft_sum = pd.DataFrame()\n",
    "            label_diff = pd.DataFrame()\n",
    "\n",
    "            for i, client in enumerate(clients):\n",
    "                acc, pred, sum_, diff = server.evaluate(client.model)\n",
    "                # Convert each dictionary to a DataFrame and append to the respective DataFrame\n",
    "                label_accuracies = label_accuracies.append(pd.DataFrame(acc, index=[i]))\n",
    "                label_predicted = label_predicted.append(pd.DataFrame(pred, index=[i]))\n",
    "                label_soft_sum = label_soft_sum.append(pd.DataFrame(sum_, index=[i]))\n",
    "                label_diff = label_diff.append(pd.DataFrame(diff, index=[i]))\n",
    "\n",
    "            # Reset index for all DataFrames\n",
    "            label_accuracies.reset_index(drop=True, inplace=True)\n",
    "            label_predicted.reset_index(drop=True, inplace=True)\n",
    "            label_soft_sum.reset_index(drop=True, inplace=True)\n",
    "            label_diff.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        if c_round == 1:\n",
    "            first_accuracies = pd.DataFrame()\n",
    "            for i, client in enumerate(clients):\n",
    "                first_acc, pred, sum_, diff = server.evaluate(client.model)\n",
    "                first_accuracies = pd.concat([first_accuracies, pd.DataFrame(first_acc, index=[i])])\n",
    "            first_accuracies = first_accuracies.fillna(0)\n",
    "\n",
    "            client_acc_after_distill = sum(acc_clients)/len(acc_clients)\n",
    "            global_acc_after_distill = np.mean(np.ravel(first_accuracies.values))\n",
    "\n",
    "\n",
    "        elif c_round == COMMUNICATION_ROUNDS:\n",
    "            client_acc_final = sum(acc_clients)/len(acc_clients)\n",
    "            global_acc_final = np.mean(np.ravel(label_accuracies.values))\n",
    "\n",
    "        average_dw = server.get_average_dw(clients)\n",
    "        #print(average_dw)\n",
    "        cfl_stats.log({\"acc_clients\" : acc_clients, \"mean_norm\" : mean_norm, \"max_norm\" : max_norm,\n",
    "                      \"rounds\" : c_round, \"clusters\" : cluster_indices, \"average_dw\": average_dw})\n",
    "\n",
    "\n",
    "        display_train_stats(cfl_stats, EPS_1, EPS_2, COMMUNICATION_ROUNDS)\n",
    "\n",
    "\n",
    "    for idc in cluster_indices:    \n",
    "        server.cache_model(idc, clients[idc[0]].W, acc_clients)\n",
    "    \n",
    "    client_acc_after_distill = round(client_acc_after_distill, 3)\n",
    "    global_acc_after_distill = round(global_acc_after_distill, 3)\n",
    "    client_acc_final = round(client_acc_final, 3)\n",
    "    global_acc_final = round(global_acc_final, 3)\n",
    "    \n",
    "    return client_acc_after_distill, global_acc_after_distill, client_acc_final, global_acc_final\n",
    "\n",
    "    print(client_acc_after_distill, global_acc_after_distill)\n",
    "    print(client_acc_final, global_acc_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-04T14:25:47.008798Z",
     "iopub.status.idle": "2023-07-04T14:25:47.009117Z",
     "shell.execute_reply": "2023-07-04T14:25:47.008956Z",
     "shell.execute_reply.started": "2023-07-04T14:25:47.008941Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-04T14:25:47.010468Z",
     "iopub.status.idle": "2023-07-04T14:25:47.011249Z",
     "shell.execute_reply": "2023-07-04T14:25:47.011069Z",
     "shell.execute_reply.started": "2023-07-04T14:25:47.011049Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_accuracies.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-04T14:25:47.012279Z",
     "iopub.status.idle": "2023-07-04T14:25:47.012574Z",
     "shell.execute_reply": "2023-07-04T14:25:47.012439Z",
     "shell.execute_reply.started": "2023-07-04T14:25:47.012423Z"
    }
   },
   "outputs": [],
   "source": [
    "label_soft_sum.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-04T14:25:47.014103Z",
     "iopub.status.idle": "2023-07-04T14:25:47.014421Z",
     "shell.execute_reply": "2023-07-04T14:25:47.014261Z",
     "shell.execute_reply.started": "2023-07-04T14:25:47.014246Z"
    }
   },
   "outputs": [],
   "source": [
    "label_diff.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-04T14:25:47.015533Z",
     "iopub.status.idle": "2023-07-04T14:25:47.015820Z",
     "shell.execute_reply": "2023-07-04T14:25:47.015687Z",
     "shell.execute_reply.started": "2023-07-04T14:25:47.015673Z"
    }
   },
   "outputs": [],
   "source": [
    "label_predicted.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-04T14:25:47.017375Z",
     "iopub.status.idle": "2023-07-04T14:25:47.017700Z",
     "shell.execute_reply": "2023-07-04T14:25:47.017536Z",
     "shell.execute_reply.started": "2023-07-04T14:25:47.017521Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Instantiate PCA\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Apply PCA to the dataframes\n",
    "label_accuracies_pca = pca.fit_transform(label_accuracies)\n",
    "label_predicted_pca = pca.fit_transform(label_predicted)\n",
    "label_soft_sum_pca = pca.fit_transform(label_soft_sum)\n",
    "label_diff_pca = pca.fit_transform(label_diff)\n",
    "transformed_data = pca.fit_transform(similarities)\n",
    "\n",
    "# Create labels\n",
    "labels = [0, 0, 0, 1, 1, 1, 2, 2, 2]\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "# Scatter plots with larger dots\n",
    "dot_size = 50\n",
    "axs[0, 0].scatter(label_accuracies_pca[:, 0], label_accuracies_pca[:, 1], c=labels, s=dot_size)\n",
    "axs[0, 0].set_title('Label Accuracies')\n",
    "axs[0, 1].scatter(label_predicted_pca[:, 0], label_predicted_pca[:, 1], c=labels, s=dot_size)\n",
    "axs[0, 1].set_title('Label Predicted')\n",
    "axs[1, 0].scatter(label_soft_sum_pca[:, 0], label_soft_sum_pca[:, 1], c=labels, s=dot_size)\n",
    "axs[1, 0].set_title('Label Soft Sum')\n",
    "axs[1, 1].scatter(label_diff_pca[:, 0], label_diff_pca[:, 1], c=labels, s=dot_size)\n",
    "axs[1, 1].set_title('Label Soft Diff')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-04T14:25:47.018487Z",
     "iopub.status.idle": "2023-07-04T14:25:47.018761Z",
     "shell.execute_reply": "2023-07-04T14:25:47.018633Z",
     "shell.execute_reply.started": "2023-07-04T14:25:47.018619Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Calculate Silhouette Scores\n",
    "silhouette_accuracies = silhouette_score(label_accuracies_pca, labels)\n",
    "silhouette_predicted = silhouette_score(label_predicted_pca, labels)\n",
    "silhouette_soft_sum = silhouette_score(label_soft_sum_pca, labels)\n",
    "silhouette_diff = silhouette_score(label_diff_pca, labels)\n",
    "silhouette_transformed_data = silhouette_score(transformed_data, labels)\n",
    "\n",
    "print('Silhouette Score for Accuracies:', silhouette_accuracies)\n",
    "print('Silhouette Score for Predicted:', silhouette_predicted)\n",
    "print('Silhouette Score for Soft Sum:', silhouette_soft_sum)\n",
    "print('Silhouette Score for diff:', silhouette_diff)\n",
    "print('Silhouette Score for Model params:', silhouette_transformed_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-04T14:25:47.020119Z",
     "iopub.status.idle": "2023-07-04T14:25:47.020424Z",
     "shell.execute_reply": "2023-07-04T14:25:47.020299Z",
     "shell.execute_reply.started": "2023-07-04T14:25:47.020281Z"
    }
   },
   "outputs": [],
   "source": [
    "#df.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 Cluster 별 모델 파라미터 분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-07-04T14:25:47.021395Z",
     "iopub.status.idle": "2023-07-04T14:25:47.021578Z",
     "shell.execute_reply": "2023-07-04T14:25:47.021492Z",
     "shell.execute_reply.started": "2023-07-04T14:25:47.021483Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Fit and transform your data to 2D\n",
    "pca = PCA(n_components=2)\n",
    "transformed_data = pca.fit_transform(similarities)\n",
    "\n",
    "# Assign labels based on index ranges\n",
    "labels = [0, 0, 0, 1, 1, 1, 2, 2, 2]\n",
    "unique_labels = np.unique(labels)\n",
    "colors = plt.cm.Spectral(np.linspace(0, 0.35, len(unique_labels)))\n",
    "\n",
    "# Plot the transformed data with labels\n",
    "for label, color in zip(unique_labels, colors):\n",
    "    idx = np.where(labels == label)\n",
    "    plt.scatter(transformed_data[idx, 0], transformed_data[idx, 1], color=color, label=f'Cluster {label}')\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.12.1-py3.8-cuda11.3",
   "language": "python",
   "name": "torch1.12.1-py3.8-cuda11.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "f79394e62bebc70f4ea6374f6a04753660b8235adfdaf8a6dfe67d7c0f65c745"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
